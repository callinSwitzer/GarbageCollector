{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame( (np.random.randn(1000,2)), columns=[\"A\", \"B\"] )\n",
    "X[\"B\"] = (X[\"A\"]) *6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X.A, X.B, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return(x * (x > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. Keras initializes some negative and some positive weights, randomly by default -- you can also specify how the weights are initialized [link] (https://keras.io/initializers/). I think that you're right that some nodes could be dead at the start, if the incoming weights sum up to a value less than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# CUSTOM LOSS FUNCTION ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from keras.backend import clear_session\n",
    "clear_session()\n",
    "\n",
    "from keras import losses\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    final_loss = (losses.binary_crossentropy(y_true[:, 0], y_pred[:, 0]) + \n",
    "                    y_true[:, 0] * \n",
    "                  losses.categorical_crossentropy(y_true[:, 1:], y_pred[:,1:]))\n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate\n",
    "XX = pd.DataFrame( (np.random.randn(1000,2)), columns=[\"A\", \"B\"] )\n",
    "XX[\"B\"] = (XX[\"A\"]) *6 + XX[\"B\"]\n",
    "yy = np.random.randn(1000, 2)\n",
    "\n",
    "n = 2\n",
    "\n",
    "input_layer = Input(shape=(n, ))\n",
    "shared = Dense(32)(input_layer)\n",
    "sub1 = Dense(16)(shared)\n",
    "sub2 = Dense(16)(shared)\n",
    "y1 = Dense(1, activation='sigmoid')(sub1)\n",
    "y2 = Dense(4, activation='softmax')(sub2)\n",
    "mergedOutput = Concatenate()([y1, y2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_layer, mergedOutput)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss=my_loss)\n",
    "yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainmod = model.fit(x=XX, y=yy, epochs=10, batch_size=2**6)\n",
    "# plt.plot(XX.A, X.B, \"o\", label = \"actual\")\n",
    "# plt.plot(AE.predict(X)[:,0], AE.predict(X)[:,1], \"o\", label = \"preds\")\n",
    "\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.array([[13, .12]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def disemvowel(string):\n",
    "    message = []\n",
    "    vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "    for letter in string:\n",
    "         if letter not in vowels:\n",
    "                message.append(letter)\n",
    "    return (message)\n",
    "\n",
    "disemvowel(\"Leol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeInput = Input((2,))\n",
    "encode = Dense(2, activation='relu')(aeInput)\n",
    "aeOutput = Dense(2, activation='relu')(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "# examples of custom loss functions\n",
    "def my_loss(y_true, y_pred):\n",
    "    # this example is mean squared error\n",
    "    # works if if y_pred and y_true are greater than 1D\n",
    "    return K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    # calculate mean(abs(y_pred1*y_pred2 - y_true1*ytrue2)) \n",
    "    return K.mean(K.abs(K.prod(y_pred, axis = 1) - K.prod(y_true, axis = 1)))\n",
    "\n",
    "AE = Model(aeInput, aeOutput, name=\"autoencoder\")\n",
    "AE.compile(optimizer='adam', loss=my_loss, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y_true = np.random.rand(100, 2) + 1\n",
    "y_pred = np.random.rand(100, 2) + 1\n",
    "\n",
    "my_loss(y_true, y_pred) # tensor\n",
    "print(tf.Session().run(K.mean(my_loss(y_true, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = [np.array([[4,  5 ],\n",
    "        [2, 6]], dtype=\"float32\"),\n",
    " np.array([ 0, 0], dtype=\"float32\"),\n",
    " np.array([[1, 1],\n",
    "        [1, 1 ]], dtype=\"float32\"),\n",
    " np.array([0, 0  ], dtype=\"float32\")]\n",
    "\n",
    "AE.set_weights(wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AE.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainAE = AE.fit( x=X, y=X, epochs=10, batch_size=2**6)\n",
    "plt.plot(X.A, X.B, \"o\", label = \"actual\")\n",
    "plt.plot(AE.predict(X)[:,0], AE.predict(X)[:,1], \"o\", label = \"preds\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.evaluate(X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y_true = X.astype(\"float32\")\n",
    "y_pred = AE.predict(X).astype(\"float32\")\n",
    "\n",
    "print(tf.Session().run((my_loss(y_true, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = X\n",
    "y_pred = AE.predict(X)\n",
    "\n",
    "my_loss(y_true, y_pred) # tensor\n",
    "print(tf.Session().run(K.mean(my_loss(y_true, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.predict(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = AE.get_weights()\n",
    "wts = [np.array([[-0,  -0 ],\n",
    "        [ -0, -0]], dtype=\"float32\"),\n",
    " np.array([ 0, 0], dtype=\"float32\"),\n",
    " np.array([[ 0,  0 ],\n",
    "        [ 0 , 0 ]], dtype=\"float32\"),\n",
    " np.array([0, 0  ], dtype=\"float32\")]\n",
    "\n",
    "AE.set_weights(wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtsConcatenated = [np.vstack([wts[ii + 1], wts[ii]]) for ii in np.arange(0, len(wts), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.predict(np.array([[-0.9, -0.33]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.predict(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LayerValues = []\n",
    "    \n",
    "inputData= np.array([[1, -0.9, -0.33]])\n",
    "LayerValues.append(inputData)\n",
    "\n",
    "jj = 0\n",
    "\n",
    "wtsConcatenated = [np.vstack([wts[ii + 1], wts[ii]]) for ii in np.arange(0, len(wts), 2)]\n",
    "\n",
    "nextLayer = np.dot(LayerValues[jj], wtsConcatenated[jj]).astype(\"float32\")\n",
    "nextLayer = np.hstack([np.array([1], dtype = \"float32\").reshape(-1,1), nextLayer] )\n",
    "nextLayer = relu(nextLayer) # apply relu\n",
    "print(nextLayer)\n",
    "jj = 1\n",
    "nextLayer = np.dot(np.array([nextLayer]), wtsConcatenated[jj]).astype(\"float32\")\n",
    "relu(nextLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X.A, X.B, \"o\", label = \"actual\")\n",
    "plt.plot(AE.predict(X)[:,0], AE.predict(X)[:,1], \"o\", label = \"preds\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.kdeplot( X.A, X.B, shade=False, axis=ax)\n",
    "sns.kdeplot(AE.predict(X)[:,0], AE.predict(X)[:,1], shade=False, axis=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(AE.predict(X)[:,0], AE.predict(X)[:,1], \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pylab import imshow, show, get_cmap\n",
    "from numpy import random\n",
    "\n",
    "Z = random.random((50,50))   # Test data\n",
    "\n",
    "imshow(Z, cmap=get_cmap(\"Spectral\"), interpolation='nearest')\n",
    "plt.savefig('your_file.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "BLACK_MIN = np.array([0, 20, 20], np.uint8)\n",
    "BLACK_MAX = np.array([120, 255, 255], np.uint8)\n",
    "imgg = cv2.imread('your_file.tif', 1)\n",
    "dst = cv2.inRange(imgg, BLACK_MIN, BLACK_MAX)\n",
    "\n",
    "no_black = cv2.countNonZero(dst)\n",
    "\n",
    "print('The number of black pixels is: ' + str(no_black))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(imgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### More custom losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Losses:\n",
    "    def IoULoss(targets, inputs, smooth=1e-6):\n",
    "        #targets = K.flatten(targets)\n",
    "#         inputs = K.reshape(inputs, [1, -1]) # 1 row, as many columns as needed\n",
    "#         targets = K.reshape(targets, [-1, 1]) # 1 column, as many rows as needed\n",
    "        inputs = K.reshape(inputs, [1, -1]) # 1 row, as many columns as needed\n",
    "        targets = K.reshape(targets, [-1, 1]) # 1 column, as many rows as needed\n",
    "        print(inputs.shape, targets.shape)\n",
    "        print(K.sum(K.dot(targets, inputs)).shape)\n",
    "        \n",
    "        intersection = K.sum(K.dot(targets, inputs))\n",
    "        total = K.sum(targets) + K.sum(inputs)\n",
    "        union = total - intersection\n",
    "\n",
    "        IoU = (intersection + smooth) / (union + smooth)\n",
    "        return 1 - IoU\n",
    "\n",
    "aeInput = Input((2,))\n",
    "encode = Dense(2, activation='relu')(aeInput)\n",
    "aeOutput = Dense(2, activation='relu')(encode)\n",
    "model = Model(aeInput, aeOutput)\n",
    "\n",
    "model.compile(loss=Losses.IoULoss, optimizer=\"adam\",\n",
    "                      metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = pd.DataFrame( (np.random.randn(1000,2)), columns=[\"A\", \"B\"] )\n",
    "XX[\"B\"] = (XX[\"A\"]) *6 + XX[\"B\"]\n",
    "yy = np.random.randn(1000, 2)\n",
    "\n",
    "model.fit(XX, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xx = np.random.randn(100)\n",
    "xx = np.asarray(xx, np.float32)\n",
    "\n",
    "xx_tf = tf.convert_to_tensor(xx, np.float32)\n",
    "xx_tf = K.reshape(xx_tf, [-1, 1])\n",
    "print(xx_tf.shape)\n",
    "\n",
    "yy = np.random.randn(100)\n",
    "yy = np.asarray(yy, np.float32)\n",
    "\n",
    "yy_tf = tf.convert_to_tensor(xx, np.float32)\n",
    "yy_tf = K.reshape(yy_tf, [1, -1])\n",
    "K.dot(xx_tf, yy_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_tf = K.reshape(xx_tf, [1, -1])\n",
    "yy_tf = K.reshape(yy_tf, [-1, 1])\n",
    "\n",
    "\n",
    "print(tf.Session().run(Losses.IoULoss(xx_tf, yy_tf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.random.rand(100), np.random.rand(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = K.random_uniform_variable(shape=(100, 1), low=0, high=1)\n",
    "y = K.random_uniform_variable(shape=(1, 100), low=0, high=1)\n",
    "xy = K.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = K.flatten(y)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X, y = make_blobs(n_samples = 100, n_features = 2, centers = 2, random_state = 123)\n",
    "\n",
    "# fit supervised KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X, y) \n",
    "\n",
    "# create 50 new data points\n",
    "# with the same number of features as the training set\n",
    "new_data = np.random.randn(50, 2)\n",
    "\n",
    "# predict new labels\n",
    "new_labels = knn.predict(new_data)\n",
    "\n",
    "# plot training clusters\n",
    "plt.plot(X[y== 1, 0], \n",
    "         X[y==1,1], \n",
    "         \"C1o\", label = \"training cluster 1\")\n",
    "plt.plot(X[y== 0, 0], \n",
    "         X[y==0,1], \n",
    "         \"C0o\", label = \"training custer 2\")\n",
    "\n",
    "# plot predictions on new data\n",
    "plt.plot(new_data[new_labels== 1, 0], \n",
    "         new_data[new_labels==1,1], \n",
    "         \"ro\", label = \"new data assigned to cluster 1\")\n",
    "plt.plot(new_data[new_labels== 0, 0], \n",
    "         new_data[new_labels==0,1], \n",
    "         \"bo\", label = \"new data assigned to cluster 2\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_X_train = iris_X[indices[:-10]]\n",
    "iris_y_train = iris_y[indices[:-10]]\n",
    "iris_X_test = iris_X[indices[-10:]]\n",
    "iris_y_test = iris_y[indices[-10:]]\n",
    "# Create and fit a nearest-neighbor classifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(iris_X_train, iris_y_train) \n",
    "\n",
    "\n",
    "\n",
    "knn.predict(iris_X_test)\n",
    "\n",
    "iris_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import time\n",
    "\n",
    "players = input(\"Let's play Five's! How many are you?:\" )\n",
    "#print(\"you are\", players, \"players?\") #test number of players\n",
    "players = int(players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your choices are [0, 5, 10, 15, 20]\n",
      "Computer has chosen 10\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Guess: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're Out.\n",
      "Your choices are [0, 5, 10, 15]\n",
      "Computer has chosen 15\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Guess: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong\n",
      "Your choices are [0, 5, 10]\n",
      "Computer has chosen 5\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Guess: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong\n",
      "Your choices are [0, 5]\n",
      "Computer has chosen 5\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Guess: 1\n",
      "Not allowed, choose again: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your choices are [0]\n",
      "Computer has chosen 0\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Guess: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're Out.\n"
     ]
    }
   ],
   "source": [
    "if players <=1:\n",
    "    print(\"Game Over\")\n",
    "\n",
    "else:\n",
    "    while players >= 1:\n",
    "        #players = players+1\n",
    "\n",
    "#Decide possible values than can be chosen\n",
    "        options = [] #Possible options\n",
    "        for i in range(0,players):\n",
    "            x = i * 5\n",
    "            options.append(x)\n",
    "        print(\"Your choices are\", options)\n",
    "\n",
    "#Playing the game\n",
    "#Each turn\n",
    "        guess = random.choice(options)\n",
    "        print(\"Computer has chosen\", int(guess))\n",
    "        count_down = 3\n",
    "        while (count_down):\n",
    "            print(count_down)\n",
    "            time.sleep(1)\n",
    "            count_down -=  1\n",
    "        choice = input(\"Guess:\")\n",
    "        choice = int(choice)\n",
    "        if choice not in options: #If choice isn't a multiple of 5\n",
    "            input(\"Not allowed, choose again:\")\n",
    "        elif choice in options and choice != guess: #Valid choice but wrong\n",
    "                print(\"Wrong\")                      #so player is still in the\n",
    "        else:                                       #game\n",
    "            choice = int(choice)\n",
    "            if choice == guess: #Correct choice so player leaves game\n",
    "                print(\"You're Out.\") # this should reduce the player count\n",
    "        players -=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def next_batch(X,y,batchsize):\n",
    "    for i in np.arange(0,X.shape[0],batchsize):\n",
    "        yield(X[i:i+batchsize],y[i:i+batchsize])\n",
    "\n",
    "def des(X,y,learning_rate,epoches, batchsize):\n",
    "    X=np.c_[np.ones((X.shape[0])),X]\n",
    "    W=np.random.uniform(size=(X.shape[1],))\n",
    "    lossHistory=[]\n",
    "    for epoch in np.arange(0,epoches):\n",
    "        epochLoss=[]\n",
    "        for (batchX,batchY) in next_batch(X,y,batchsize):\n",
    "            #batchY = batchY.reshape(-1)\n",
    "            preds=batchX.dot(W)\n",
    "            print(preds.shape, batchY.shape)\n",
    "            error=preds-batchY\n",
    "            loss=np.sum(error**2)\n",
    "            epochLoss.append(loss)\n",
    "            gradient=batchX.T.dot(error)/batchX.shape[0]\n",
    "            W+=-learning_rate*gradient\n",
    "    lossHistory.append(np.average(epochLoss))\n",
    "    return W,lossHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 14), (120, 1))"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = np.random.randn(150,13)\n",
    "target = np.random.randn(150)\n",
    "\n",
    "train_data_intercept = np.insert(data, 0, 1, axis=1) \n",
    "train_data,test_data,train_target,test_target = train_test_split(train_data_intercept,(target[:, np.newaxis]), test_size=0.2, random_state=42)\n",
    "#train_data,test_data,train_target,test_target = train_test_split(data,target, test_size=0.2, random_state=42)\n",
    "train_data.shape, train_target.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,) (32, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (15,) (15,32) (15,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-260-3fe6d125dbf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-256-915778a83ade>\u001b[0m in \u001b[0;36mdes\u001b[1;34m(X, y, learning_rate, epoches, batchsize)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mepochLoss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mgradient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatchX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatchX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mW\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mlossHistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochLoss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlossHistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (15,) (15,32) (15,) "
     ]
    }
   ],
   "source": [
    "w, loss = des(train_data,train_target,0.01,10,32)\n",
    "w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.324155</td>\n",
       "      <td>0.280023</td>\n",
       "      <td>0.906174</td>\n",
       "      <td>-0.546467</td>\n",
       "      <td>0.450499</td>\n",
       "      <td>1.544092</td>\n",
       "      <td>-1.187309</td>\n",
       "      <td>-1.809241</td>\n",
       "      <td>0.921990</td>\n",
       "      <td>-0.048623</td>\n",
       "      <td>1.400153</td>\n",
       "      <td>0.295839</td>\n",
       "      <td>-0.649771</td>\n",
       "      <td>0.018622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.296955</td>\n",
       "      <td>-0.014474</td>\n",
       "      <td>-0.373151</td>\n",
       "      <td>-0.983438</td>\n",
       "      <td>1.030189</td>\n",
       "      <td>0.302328</td>\n",
       "      <td>-1.094334</td>\n",
       "      <td>0.981874</td>\n",
       "      <td>0.326451</td>\n",
       "      <td>0.370504</td>\n",
       "      <td>-0.752608</td>\n",
       "      <td>1.196286</td>\n",
       "      <td>0.608868</td>\n",
       "      <td>0.964627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.765843</td>\n",
       "      <td>-0.230580</td>\n",
       "      <td>-0.567077</td>\n",
       "      <td>0.141241</td>\n",
       "      <td>-0.407424</td>\n",
       "      <td>1.817883</td>\n",
       "      <td>0.058964</td>\n",
       "      <td>-1.989771</td>\n",
       "      <td>1.158538</td>\n",
       "      <td>-0.676569</td>\n",
       "      <td>-0.647796</td>\n",
       "      <td>-1.570263</td>\n",
       "      <td>-0.385619</td>\n",
       "      <td>-2.602803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.415270</td>\n",
       "      <td>-0.233568</td>\n",
       "      <td>-0.485154</td>\n",
       "      <td>0.100138</td>\n",
       "      <td>0.777029</td>\n",
       "      <td>0.415959</td>\n",
       "      <td>-0.671644</td>\n",
       "      <td>-0.936163</td>\n",
       "      <td>0.598917</td>\n",
       "      <td>-0.674595</td>\n",
       "      <td>0.593295</td>\n",
       "      <td>2.321953</td>\n",
       "      <td>-0.565495</td>\n",
       "      <td>0.145577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.056278</td>\n",
       "      <td>-0.538365</td>\n",
       "      <td>-2.035299</td>\n",
       "      <td>-0.786087</td>\n",
       "      <td>-0.196908</td>\n",
       "      <td>-0.520349</td>\n",
       "      <td>1.173315</td>\n",
       "      <td>0.244964</td>\n",
       "      <td>0.302927</td>\n",
       "      <td>0.170116</td>\n",
       "      <td>0.121828</td>\n",
       "      <td>-0.434741</td>\n",
       "      <td>-0.864817</td>\n",
       "      <td>-0.144488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.943468</td>\n",
       "      <td>-0.262951</td>\n",
       "      <td>-0.741601</td>\n",
       "      <td>0.582080</td>\n",
       "      <td>-0.472952</td>\n",
       "      <td>-0.234918</td>\n",
       "      <td>0.185775</td>\n",
       "      <td>0.460341</td>\n",
       "      <td>-0.298004</td>\n",
       "      <td>-0.089468</td>\n",
       "      <td>-0.796131</td>\n",
       "      <td>-1.114263</td>\n",
       "      <td>0.050091</td>\n",
       "      <td>-0.312687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.939779</td>\n",
       "      <td>-0.987735</td>\n",
       "      <td>-0.251745</td>\n",
       "      <td>0.133066</td>\n",
       "      <td>1.094890</td>\n",
       "      <td>-0.297343</td>\n",
       "      <td>-2.700651</td>\n",
       "      <td>0.585107</td>\n",
       "      <td>0.982556</td>\n",
       "      <td>0.520110</td>\n",
       "      <td>0.314605</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>-1.499442</td>\n",
       "      <td>-0.845574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.129639</td>\n",
       "      <td>-2.024403</td>\n",
       "      <td>0.810383</td>\n",
       "      <td>0.127532</td>\n",
       "      <td>-0.297367</td>\n",
       "      <td>-0.652425</td>\n",
       "      <td>0.504963</td>\n",
       "      <td>1.832211</td>\n",
       "      <td>0.762544</td>\n",
       "      <td>-1.250540</td>\n",
       "      <td>-0.098536</td>\n",
       "      <td>0.431886</td>\n",
       "      <td>0.465799</td>\n",
       "      <td>-2.451167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.215829</td>\n",
       "      <td>-1.654899</td>\n",
       "      <td>-0.837927</td>\n",
       "      <td>2.861428</td>\n",
       "      <td>0.384361</td>\n",
       "      <td>-0.891486</td>\n",
       "      <td>0.568802</td>\n",
       "      <td>-0.438358</td>\n",
       "      <td>1.031554</td>\n",
       "      <td>0.168371</td>\n",
       "      <td>0.361630</td>\n",
       "      <td>1.105988</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>0.480013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.136869</td>\n",
       "      <td>-1.083203</td>\n",
       "      <td>1.357632</td>\n",
       "      <td>-0.348590</td>\n",
       "      <td>0.383872</td>\n",
       "      <td>-0.341981</td>\n",
       "      <td>1.071189</td>\n",
       "      <td>-0.712925</td>\n",
       "      <td>-0.286234</td>\n",
       "      <td>0.613680</td>\n",
       "      <td>0.206219</td>\n",
       "      <td>0.074014</td>\n",
       "      <td>-1.082022</td>\n",
       "      <td>0.449712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.228964</td>\n",
       "      <td>-0.430872</td>\n",
       "      <td>-1.327280</td>\n",
       "      <td>-0.377273</td>\n",
       "      <td>1.389761</td>\n",
       "      <td>-0.265707</td>\n",
       "      <td>-0.246922</td>\n",
       "      <td>-0.330596</td>\n",
       "      <td>-0.257201</td>\n",
       "      <td>0.502421</td>\n",
       "      <td>1.269257</td>\n",
       "      <td>1.143528</td>\n",
       "      <td>-0.435369</td>\n",
       "      <td>0.711527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.695146</td>\n",
       "      <td>0.457604</td>\n",
       "      <td>-0.769346</td>\n",
       "      <td>-0.035712</td>\n",
       "      <td>0.742386</td>\n",
       "      <td>1.183854</td>\n",
       "      <td>0.108753</td>\n",
       "      <td>1.698550</td>\n",
       "      <td>0.096840</td>\n",
       "      <td>0.787736</td>\n",
       "      <td>-0.649492</td>\n",
       "      <td>-0.960009</td>\n",
       "      <td>-0.405656</td>\n",
       "      <td>0.798463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.673463</td>\n",
       "      <td>-1.902440</td>\n",
       "      <td>-0.240004</td>\n",
       "      <td>2.327057</td>\n",
       "      <td>1.017179</td>\n",
       "      <td>0.351166</td>\n",
       "      <td>0.430511</td>\n",
       "      <td>1.312027</td>\n",
       "      <td>1.153205</td>\n",
       "      <td>-0.048670</td>\n",
       "      <td>-0.281967</td>\n",
       "      <td>0.628821</td>\n",
       "      <td>0.073823</td>\n",
       "      <td>0.143403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.722166</td>\n",
       "      <td>0.021358</td>\n",
       "      <td>1.621775</td>\n",
       "      <td>-0.098504</td>\n",
       "      <td>-0.920280</td>\n",
       "      <td>1.593947</td>\n",
       "      <td>-0.308180</td>\n",
       "      <td>-1.133219</td>\n",
       "      <td>-0.669685</td>\n",
       "      <td>0.995385</td>\n",
       "      <td>-0.709320</td>\n",
       "      <td>1.033608</td>\n",
       "      <td>0.178217</td>\n",
       "      <td>-1.897041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.757093</td>\n",
       "      <td>-1.539339</td>\n",
       "      <td>-0.013525</td>\n",
       "      <td>-0.352009</td>\n",
       "      <td>-0.388951</td>\n",
       "      <td>0.801254</td>\n",
       "      <td>0.037179</td>\n",
       "      <td>0.030929</td>\n",
       "      <td>-2.609712</td>\n",
       "      <td>1.223902</td>\n",
       "      <td>-0.715013</td>\n",
       "      <td>-1.418049</td>\n",
       "      <td>0.882566</td>\n",
       "      <td>0.255738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.628678</td>\n",
       "      <td>-2.632518</td>\n",
       "      <td>1.082110</td>\n",
       "      <td>1.458691</td>\n",
       "      <td>0.686207</td>\n",
       "      <td>0.054818</td>\n",
       "      <td>-2.489097</td>\n",
       "      <td>-0.555055</td>\n",
       "      <td>2.109427</td>\n",
       "      <td>1.538382</td>\n",
       "      <td>1.652889</td>\n",
       "      <td>-0.838494</td>\n",
       "      <td>-1.643691</td>\n",
       "      <td>-1.101963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.057391</td>\n",
       "      <td>-0.654887</td>\n",
       "      <td>0.317156</td>\n",
       "      <td>-0.452213</td>\n",
       "      <td>-0.178781</td>\n",
       "      <td>1.489966</td>\n",
       "      <td>-0.517980</td>\n",
       "      <td>-1.838721</td>\n",
       "      <td>1.322395</td>\n",
       "      <td>1.060383</td>\n",
       "      <td>-0.084937</td>\n",
       "      <td>-0.858862</td>\n",
       "      <td>0.231761</td>\n",
       "      <td>0.675018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.423524</td>\n",
       "      <td>-0.176870</td>\n",
       "      <td>0.318705</td>\n",
       "      <td>0.217995</td>\n",
       "      <td>1.029183</td>\n",
       "      <td>0.429475</td>\n",
       "      <td>0.965592</td>\n",
       "      <td>0.191336</td>\n",
       "      <td>0.499996</td>\n",
       "      <td>0.118169</td>\n",
       "      <td>0.131777</td>\n",
       "      <td>-1.090417</td>\n",
       "      <td>-0.921065</td>\n",
       "      <td>0.914476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.226127</td>\n",
       "      <td>0.813840</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>-0.462188</td>\n",
       "      <td>0.492496</td>\n",
       "      <td>-0.940211</td>\n",
       "      <td>2.060731</td>\n",
       "      <td>0.120592</td>\n",
       "      <td>-0.252443</td>\n",
       "      <td>-0.670390</td>\n",
       "      <td>0.224753</td>\n",
       "      <td>-0.015617</td>\n",
       "      <td>0.105302</td>\n",
       "      <td>-0.876590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.911455</td>\n",
       "      <td>0.199485</td>\n",
       "      <td>-0.638089</td>\n",
       "      <td>-0.416854</td>\n",
       "      <td>2.351695</td>\n",
       "      <td>0.430302</td>\n",
       "      <td>-0.370356</td>\n",
       "      <td>-0.617115</td>\n",
       "      <td>2.006875</td>\n",
       "      <td>0.078716</td>\n",
       "      <td>-0.220873</td>\n",
       "      <td>0.944354</td>\n",
       "      <td>-0.862587</td>\n",
       "      <td>-1.201874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.290633</td>\n",
       "      <td>-0.433765</td>\n",
       "      <td>-0.443114</td>\n",
       "      <td>0.432201</td>\n",
       "      <td>0.221660</td>\n",
       "      <td>0.040998</td>\n",
       "      <td>-0.145046</td>\n",
       "      <td>0.358298</td>\n",
       "      <td>0.991688</td>\n",
       "      <td>0.998200</td>\n",
       "      <td>-1.618268</td>\n",
       "      <td>0.481274</td>\n",
       "      <td>-1.426305</td>\n",
       "      <td>2.043953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.312291</td>\n",
       "      <td>-0.735255</td>\n",
       "      <td>-0.366935</td>\n",
       "      <td>0.451333</td>\n",
       "      <td>-0.023566</td>\n",
       "      <td>-1.705366</td>\n",
       "      <td>-0.872043</td>\n",
       "      <td>-0.054721</td>\n",
       "      <td>-1.663138</td>\n",
       "      <td>-0.530753</td>\n",
       "      <td>-1.629585</td>\n",
       "      <td>-0.295695</td>\n",
       "      <td>0.862091</td>\n",
       "      <td>1.189902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.104877</td>\n",
       "      <td>1.272477</td>\n",
       "      <td>-0.808313</td>\n",
       "      <td>0.934850</td>\n",
       "      <td>-0.294110</td>\n",
       "      <td>-0.907465</td>\n",
       "      <td>1.420540</td>\n",
       "      <td>0.549287</td>\n",
       "      <td>-0.155727</td>\n",
       "      <td>-0.228040</td>\n",
       "      <td>-0.128765</td>\n",
       "      <td>0.021928</td>\n",
       "      <td>-0.704515</td>\n",
       "      <td>0.109991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.440780</td>\n",
       "      <td>-2.958484</td>\n",
       "      <td>-0.038005</td>\n",
       "      <td>-0.155518</td>\n",
       "      <td>-2.008415</td>\n",
       "      <td>1.928285</td>\n",
       "      <td>0.822374</td>\n",
       "      <td>-0.511711</td>\n",
       "      <td>-0.488496</td>\n",
       "      <td>1.044512</td>\n",
       "      <td>0.989719</td>\n",
       "      <td>-0.124980</td>\n",
       "      <td>-0.966005</td>\n",
       "      <td>-0.285723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.073053</td>\n",
       "      <td>1.296603</td>\n",
       "      <td>1.389923</td>\n",
       "      <td>-0.504007</td>\n",
       "      <td>1.258409</td>\n",
       "      <td>0.214323</td>\n",
       "      <td>-1.921479</td>\n",
       "      <td>-1.389260</td>\n",
       "      <td>0.298724</td>\n",
       "      <td>-0.958645</td>\n",
       "      <td>1.123535</td>\n",
       "      <td>1.784467</td>\n",
       "      <td>0.428863</td>\n",
       "      <td>-1.133294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.590173</td>\n",
       "      <td>-0.048059</td>\n",
       "      <td>0.878789</td>\n",
       "      <td>-0.290391</td>\n",
       "      <td>0.171735</td>\n",
       "      <td>-1.404475</td>\n",
       "      <td>-2.391928</td>\n",
       "      <td>0.418166</td>\n",
       "      <td>-0.918952</td>\n",
       "      <td>-0.533108</td>\n",
       "      <td>0.265932</td>\n",
       "      <td>-0.412331</td>\n",
       "      <td>-0.317886</td>\n",
       "      <td>-0.848116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.399167</td>\n",
       "      <td>0.440825</td>\n",
       "      <td>-1.283827</td>\n",
       "      <td>2.227096</td>\n",
       "      <td>-1.177989</td>\n",
       "      <td>-1.156166</td>\n",
       "      <td>1.639631</td>\n",
       "      <td>0.632697</td>\n",
       "      <td>-0.216161</td>\n",
       "      <td>0.669059</td>\n",
       "      <td>-0.438771</td>\n",
       "      <td>-0.507973</td>\n",
       "      <td>0.306449</td>\n",
       "      <td>-0.184829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.339862</td>\n",
       "      <td>-1.439921</td>\n",
       "      <td>0.217743</td>\n",
       "      <td>0.732350</td>\n",
       "      <td>0.989012</td>\n",
       "      <td>-0.622258</td>\n",
       "      <td>-0.484682</td>\n",
       "      <td>0.608675</td>\n",
       "      <td>-0.490622</td>\n",
       "      <td>-1.060110</td>\n",
       "      <td>-0.184689</td>\n",
       "      <td>-1.656397</td>\n",
       "      <td>-0.055470</td>\n",
       "      <td>-1.360479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.658979</td>\n",
       "      <td>-0.270936</td>\n",
       "      <td>-0.033896</td>\n",
       "      <td>0.496706</td>\n",
       "      <td>-0.278124</td>\n",
       "      <td>0.762373</td>\n",
       "      <td>-1.579996</td>\n",
       "      <td>0.876143</td>\n",
       "      <td>1.411948</td>\n",
       "      <td>0.770937</td>\n",
       "      <td>-0.842644</td>\n",
       "      <td>-0.121870</td>\n",
       "      <td>0.738804</td>\n",
       "      <td>-0.520021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.680889</td>\n",
       "      <td>-2.080416</td>\n",
       "      <td>0.582247</td>\n",
       "      <td>0.224400</td>\n",
       "      <td>-0.407728</td>\n",
       "      <td>0.644810</td>\n",
       "      <td>2.126441</td>\n",
       "      <td>1.596192</td>\n",
       "      <td>1.302601</td>\n",
       "      <td>-0.625267</td>\n",
       "      <td>0.728203</td>\n",
       "      <td>0.181350</td>\n",
       "      <td>0.539067</td>\n",
       "      <td>-1.590740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.013833</td>\n",
       "      <td>-0.324368</td>\n",
       "      <td>-1.163318</td>\n",
       "      <td>-0.827445</td>\n",
       "      <td>1.198508</td>\n",
       "      <td>0.916244</td>\n",
       "      <td>-0.793833</td>\n",
       "      <td>1.762520</td>\n",
       "      <td>0.912498</td>\n",
       "      <td>1.182887</td>\n",
       "      <td>-0.581198</td>\n",
       "      <td>-0.121828</td>\n",
       "      <td>-0.891564</td>\n",
       "      <td>0.650155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.395118</td>\n",
       "      <td>-0.404135</td>\n",
       "      <td>0.796659</td>\n",
       "      <td>-1.231262</td>\n",
       "      <td>-0.455600</td>\n",
       "      <td>-0.847728</td>\n",
       "      <td>0.106599</td>\n",
       "      <td>-0.966910</td>\n",
       "      <td>-0.576079</td>\n",
       "      <td>-1.678606</td>\n",
       "      <td>-0.366704</td>\n",
       "      <td>-0.755679</td>\n",
       "      <td>-2.214221</td>\n",
       "      <td>1.023971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.037205</td>\n",
       "      <td>-2.196969</td>\n",
       "      <td>-0.052552</td>\n",
       "      <td>0.242629</td>\n",
       "      <td>0.827549</td>\n",
       "      <td>0.490336</td>\n",
       "      <td>0.088045</td>\n",
       "      <td>-1.556536</td>\n",
       "      <td>-1.535694</td>\n",
       "      <td>0.786301</td>\n",
       "      <td>-0.114562</td>\n",
       "      <td>1.583300</td>\n",
       "      <td>-0.090033</td>\n",
       "      <td>1.568755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962337</td>\n",
       "      <td>-1.483698</td>\n",
       "      <td>0.473625</td>\n",
       "      <td>-1.269572</td>\n",
       "      <td>0.718894</td>\n",
       "      <td>-1.375272</td>\n",
       "      <td>0.308170</td>\n",
       "      <td>-2.695540</td>\n",
       "      <td>0.240131</td>\n",
       "      <td>0.908059</td>\n",
       "      <td>-0.443687</td>\n",
       "      <td>0.743719</td>\n",
       "      <td>0.436824</td>\n",
       "      <td>-0.018611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.133296</td>\n",
       "      <td>-1.313003</td>\n",
       "      <td>-0.344768</td>\n",
       "      <td>0.835567</td>\n",
       "      <td>1.303471</td>\n",
       "      <td>-1.383373</td>\n",
       "      <td>-1.023127</td>\n",
       "      <td>1.049151</td>\n",
       "      <td>-0.617696</td>\n",
       "      <td>-1.974539</td>\n",
       "      <td>0.847049</td>\n",
       "      <td>0.580214</td>\n",
       "      <td>-0.783629</td>\n",
       "      <td>-1.576371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.781138</td>\n",
       "      <td>0.307556</td>\n",
       "      <td>-0.663133</td>\n",
       "      <td>-0.263828</td>\n",
       "      <td>0.786838</td>\n",
       "      <td>-0.133194</td>\n",
       "      <td>-1.838633</td>\n",
       "      <td>-0.079153</td>\n",
       "      <td>-0.677238</td>\n",
       "      <td>-1.100844</td>\n",
       "      <td>-1.377004</td>\n",
       "      <td>-0.521673</td>\n",
       "      <td>-0.213441</td>\n",
       "      <td>0.097560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.284012</td>\n",
       "      <td>-0.185986</td>\n",
       "      <td>0.460376</td>\n",
       "      <td>-0.155886</td>\n",
       "      <td>0.501357</td>\n",
       "      <td>0.615855</td>\n",
       "      <td>-0.496964</td>\n",
       "      <td>-1.228920</td>\n",
       "      <td>-0.043244</td>\n",
       "      <td>0.254221</td>\n",
       "      <td>-0.082780</td>\n",
       "      <td>1.184575</td>\n",
       "      <td>0.475846</td>\n",
       "      <td>-0.593911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.494802</td>\n",
       "      <td>0.172424</td>\n",
       "      <td>0.445790</td>\n",
       "      <td>1.080547</td>\n",
       "      <td>0.966801</td>\n",
       "      <td>2.392395</td>\n",
       "      <td>-1.806336</td>\n",
       "      <td>1.643205</td>\n",
       "      <td>-1.633132</td>\n",
       "      <td>0.496792</td>\n",
       "      <td>-0.742969</td>\n",
       "      <td>-0.489626</td>\n",
       "      <td>0.027503</td>\n",
       "      <td>0.600103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.452051</td>\n",
       "      <td>-0.904870</td>\n",
       "      <td>0.772610</td>\n",
       "      <td>0.927047</td>\n",
       "      <td>-0.058490</td>\n",
       "      <td>1.123663</td>\n",
       "      <td>-0.610201</td>\n",
       "      <td>1.766335</td>\n",
       "      <td>0.915223</td>\n",
       "      <td>-2.323803</td>\n",
       "      <td>1.551051</td>\n",
       "      <td>1.316492</td>\n",
       "      <td>-1.429129</td>\n",
       "      <td>-1.152140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.340752</td>\n",
       "      <td>0.826739</td>\n",
       "      <td>0.147502</td>\n",
       "      <td>0.228408</td>\n",
       "      <td>-1.268531</td>\n",
       "      <td>-1.339232</td>\n",
       "      <td>1.032885</td>\n",
       "      <td>0.064164</td>\n",
       "      <td>-0.769165</td>\n",
       "      <td>0.078407</td>\n",
       "      <td>-0.909857</td>\n",
       "      <td>-0.635943</td>\n",
       "      <td>0.573373</td>\n",
       "      <td>-0.222647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.156193</td>\n",
       "      <td>-1.333003</td>\n",
       "      <td>0.138932</td>\n",
       "      <td>-0.533458</td>\n",
       "      <td>-0.892250</td>\n",
       "      <td>0.372546</td>\n",
       "      <td>-0.433719</td>\n",
       "      <td>1.087625</td>\n",
       "      <td>-0.359697</td>\n",
       "      <td>-0.121089</td>\n",
       "      <td>-1.684378</td>\n",
       "      <td>-0.296507</td>\n",
       "      <td>-0.035591</td>\n",
       "      <td>0.845945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.566892</td>\n",
       "      <td>-0.843184</td>\n",
       "      <td>0.967841</td>\n",
       "      <td>-0.865240</td>\n",
       "      <td>0.541239</td>\n",
       "      <td>0.316399</td>\n",
       "      <td>1.567672</td>\n",
       "      <td>0.185875</td>\n",
       "      <td>-0.811463</td>\n",
       "      <td>1.102997</td>\n",
       "      <td>-0.229851</td>\n",
       "      <td>-0.266047</td>\n",
       "      <td>2.367364</td>\n",
       "      <td>-0.610867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.890357</td>\n",
       "      <td>0.478736</td>\n",
       "      <td>-0.661198</td>\n",
       "      <td>-1.149995</td>\n",
       "      <td>-1.081964</td>\n",
       "      <td>-1.286690</td>\n",
       "      <td>-0.978894</td>\n",
       "      <td>0.691046</td>\n",
       "      <td>0.271246</td>\n",
       "      <td>0.844581</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>0.808210</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.882274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.234831</td>\n",
       "      <td>1.344638</td>\n",
       "      <td>0.543106</td>\n",
       "      <td>-1.535960</td>\n",
       "      <td>0.747636</td>\n",
       "      <td>-0.632584</td>\n",
       "      <td>-0.266455</td>\n",
       "      <td>0.463741</td>\n",
       "      <td>1.341787</td>\n",
       "      <td>-1.005388</td>\n",
       "      <td>-0.526252</td>\n",
       "      <td>-0.099187</td>\n",
       "      <td>0.944251</td>\n",
       "      <td>0.476258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.399513</td>\n",
       "      <td>-0.500048</td>\n",
       "      <td>-0.092196</td>\n",
       "      <td>-0.216849</td>\n",
       "      <td>1.167259</td>\n",
       "      <td>-0.098799</td>\n",
       "      <td>-1.439551</td>\n",
       "      <td>1.233285</td>\n",
       "      <td>2.879099</td>\n",
       "      <td>-0.476272</td>\n",
       "      <td>0.422538</td>\n",
       "      <td>-0.259557</td>\n",
       "      <td>-0.554024</td>\n",
       "      <td>-0.359096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.379593</td>\n",
       "      <td>1.374420</td>\n",
       "      <td>0.137976</td>\n",
       "      <td>-0.705007</td>\n",
       "      <td>0.311377</td>\n",
       "      <td>-0.933001</td>\n",
       "      <td>-2.452414</td>\n",
       "      <td>-2.291273</td>\n",
       "      <td>-1.149213</td>\n",
       "      <td>-1.033316</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.556236</td>\n",
       "      <td>0.145493</td>\n",
       "      <td>-1.853645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.982943</td>\n",
       "      <td>-0.132700</td>\n",
       "      <td>0.669776</td>\n",
       "      <td>-0.535850</td>\n",
       "      <td>0.458167</td>\n",
       "      <td>-0.046628</td>\n",
       "      <td>-0.727509</td>\n",
       "      <td>1.598275</td>\n",
       "      <td>0.174711</td>\n",
       "      <td>-0.127460</td>\n",
       "      <td>-1.648830</td>\n",
       "      <td>-0.906735</td>\n",
       "      <td>1.318613</td>\n",
       "      <td>0.039692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.324451</td>\n",
       "      <td>-0.966093</td>\n",
       "      <td>1.537763</td>\n",
       "      <td>0.768892</td>\n",
       "      <td>0.834857</td>\n",
       "      <td>-0.245928</td>\n",
       "      <td>-0.630132</td>\n",
       "      <td>0.337157</td>\n",
       "      <td>-1.620996</td>\n",
       "      <td>-1.340387</td>\n",
       "      <td>0.350453</td>\n",
       "      <td>-1.799553</td>\n",
       "      <td>-1.164455</td>\n",
       "      <td>0.596010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.257375</td>\n",
       "      <td>0.346621</td>\n",
       "      <td>-0.483461</td>\n",
       "      <td>-1.039528</td>\n",
       "      <td>0.093341</td>\n",
       "      <td>-1.874148</td>\n",
       "      <td>-0.580189</td>\n",
       "      <td>-0.102385</td>\n",
       "      <td>0.115753</td>\n",
       "      <td>-0.978703</td>\n",
       "      <td>-0.282042</td>\n",
       "      <td>-0.068302</td>\n",
       "      <td>-0.817777</td>\n",
       "      <td>1.165414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.301735</td>\n",
       "      <td>-0.266397</td>\n",
       "      <td>-0.227812</td>\n",
       "      <td>0.572541</td>\n",
       "      <td>0.942559</td>\n",
       "      <td>-0.445910</td>\n",
       "      <td>0.765039</td>\n",
       "      <td>-0.110520</td>\n",
       "      <td>0.921936</td>\n",
       "      <td>-1.180993</td>\n",
       "      <td>0.909055</td>\n",
       "      <td>-0.339337</td>\n",
       "      <td>-0.969875</td>\n",
       "      <td>0.285163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016591</td>\n",
       "      <td>-0.155176</td>\n",
       "      <td>1.458151</td>\n",
       "      <td>-0.781127</td>\n",
       "      <td>-0.752588</td>\n",
       "      <td>2.092981</td>\n",
       "      <td>0.231840</td>\n",
       "      <td>0.836649</td>\n",
       "      <td>-3.242005</td>\n",
       "      <td>2.554540</td>\n",
       "      <td>0.251309</td>\n",
       "      <td>0.945601</td>\n",
       "      <td>-0.239059</td>\n",
       "      <td>-1.729303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.236326</td>\n",
       "      <td>0.238113</td>\n",
       "      <td>-0.816594</td>\n",
       "      <td>-0.543328</td>\n",
       "      <td>-1.189269</td>\n",
       "      <td>-0.291341</td>\n",
       "      <td>1.123865</td>\n",
       "      <td>-2.151016</td>\n",
       "      <td>-0.896312</td>\n",
       "      <td>-2.013886</td>\n",
       "      <td>-0.458833</td>\n",
       "      <td>1.053839</td>\n",
       "      <td>-0.003071</td>\n",
       "      <td>0.233951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.898821</td>\n",
       "      <td>-0.515543</td>\n",
       "      <td>0.786032</td>\n",
       "      <td>0.249295</td>\n",
       "      <td>0.557928</td>\n",
       "      <td>0.183303</td>\n",
       "      <td>1.370312</td>\n",
       "      <td>2.106222</td>\n",
       "      <td>0.201604</td>\n",
       "      <td>1.026928</td>\n",
       "      <td>0.369033</td>\n",
       "      <td>-1.596023</td>\n",
       "      <td>0.327179</td>\n",
       "      <td>-0.631026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.638414</td>\n",
       "      <td>0.990666</td>\n",
       "      <td>1.031411</td>\n",
       "      <td>2.031722</td>\n",
       "      <td>0.910487</td>\n",
       "      <td>0.038755</td>\n",
       "      <td>-1.664829</td>\n",
       "      <td>-1.154991</td>\n",
       "      <td>0.876789</td>\n",
       "      <td>0.437520</td>\n",
       "      <td>-1.914616</td>\n",
       "      <td>1.265751</td>\n",
       "      <td>0.650888</td>\n",
       "      <td>-1.572120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>1.992691</td>\n",
       "      <td>-0.304611</td>\n",
       "      <td>1.722787</td>\n",
       "      <td>1.086006</td>\n",
       "      <td>0.842089</td>\n",
       "      <td>1.008068</td>\n",
       "      <td>1.303940</td>\n",
       "      <td>0.015752</td>\n",
       "      <td>0.759068</td>\n",
       "      <td>0.688316</td>\n",
       "      <td>0.249673</td>\n",
       "      <td>-0.436027</td>\n",
       "      <td>-1.259726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.354690</td>\n",
       "      <td>0.725696</td>\n",
       "      <td>0.243493</td>\n",
       "      <td>0.829609</td>\n",
       "      <td>0.815333</td>\n",
       "      <td>0.619227</td>\n",
       "      <td>0.269490</td>\n",
       "      <td>0.793482</td>\n",
       "      <td>-0.862932</td>\n",
       "      <td>-1.850052</td>\n",
       "      <td>0.314250</td>\n",
       "      <td>0.010627</td>\n",
       "      <td>-1.400407</td>\n",
       "      <td>-1.662923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.680959</td>\n",
       "      <td>1.593497</td>\n",
       "      <td>-0.945763</td>\n",
       "      <td>-0.103994</td>\n",
       "      <td>1.319037</td>\n",
       "      <td>1.877450</td>\n",
       "      <td>0.049087</td>\n",
       "      <td>-0.555811</td>\n",
       "      <td>0.443928</td>\n",
       "      <td>-0.320651</td>\n",
       "      <td>-0.350842</td>\n",
       "      <td>0.309964</td>\n",
       "      <td>0.335206</td>\n",
       "      <td>-0.798133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.095581</td>\n",
       "      <td>-0.545238</td>\n",
       "      <td>-0.792189</td>\n",
       "      <td>-1.397642</td>\n",
       "      <td>0.267877</td>\n",
       "      <td>1.850542</td>\n",
       "      <td>-2.150406</td>\n",
       "      <td>1.238607</td>\n",
       "      <td>0.153936</td>\n",
       "      <td>-0.783455</td>\n",
       "      <td>2.296221</td>\n",
       "      <td>0.868531</td>\n",
       "      <td>-0.011128</td>\n",
       "      <td>1.783243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.314175</td>\n",
       "      <td>0.753043</td>\n",
       "      <td>-0.341610</td>\n",
       "      <td>-0.459202</td>\n",
       "      <td>-0.405305</td>\n",
       "      <td>1.118053</td>\n",
       "      <td>0.439409</td>\n",
       "      <td>-0.114551</td>\n",
       "      <td>0.395257</td>\n",
       "      <td>-0.685737</td>\n",
       "      <td>1.974084</td>\n",
       "      <td>0.619691</td>\n",
       "      <td>1.859962</td>\n",
       "      <td>1.896146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.065394</td>\n",
       "      <td>0.958108</td>\n",
       "      <td>-0.939434</td>\n",
       "      <td>-0.047829</td>\n",
       "      <td>1.746429</td>\n",
       "      <td>-0.559338</td>\n",
       "      <td>1.032144</td>\n",
       "      <td>-0.682549</td>\n",
       "      <td>0.575425</td>\n",
       "      <td>0.562728</td>\n",
       "      <td>-0.599750</td>\n",
       "      <td>0.751304</td>\n",
       "      <td>-0.318354</td>\n",
       "      <td>0.804555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6   \\\n",
       "0    1.0 -1.324155  0.280023  0.906174 -0.546467  0.450499  1.544092   \n",
       "1    1.0  0.296955 -0.014474 -0.373151 -0.983438  1.030189  0.302328   \n",
       "2    1.0  0.765843 -0.230580 -0.567077  0.141241 -0.407424  1.817883   \n",
       "3    1.0  1.415270 -0.233568 -0.485154  0.100138  0.777029  0.415959   \n",
       "4    1.0  0.056278 -0.538365 -2.035299 -0.786087 -0.196908 -0.520349   \n",
       "5    1.0  0.943468 -0.262951 -0.741601  0.582080 -0.472952 -0.234918   \n",
       "6    1.0  0.939779 -0.987735 -0.251745  0.133066  1.094890 -0.297343   \n",
       "7    1.0 -1.129639 -2.024403  0.810383  0.127532 -0.297367 -0.652425   \n",
       "8    1.0 -0.215829 -1.654899 -0.837927  2.861428  0.384361 -0.891486   \n",
       "9    1.0  2.136869 -1.083203  1.357632 -0.348590  0.383872 -0.341981   \n",
       "10   1.0  0.228964 -0.430872 -1.327280 -0.377273  1.389761 -0.265707   \n",
       "11   1.0 -0.695146  0.457604 -0.769346 -0.035712  0.742386  1.183854   \n",
       "12   1.0  0.673463 -1.902440 -0.240004  2.327057  1.017179  0.351166   \n",
       "13   1.0  0.722166  0.021358  1.621775 -0.098504 -0.920280  1.593947   \n",
       "14   1.0 -0.757093 -1.539339 -0.013525 -0.352009 -0.388951  0.801254   \n",
       "15   1.0  0.628678 -2.632518  1.082110  1.458691  0.686207  0.054818   \n",
       "16   1.0  1.057391 -0.654887  0.317156 -0.452213 -0.178781  1.489966   \n",
       "17   1.0  0.423524 -0.176870  0.318705  0.217995  1.029183  0.429475   \n",
       "18   1.0  0.226127  0.813840  0.004886 -0.462188  0.492496 -0.940211   \n",
       "19   1.0 -0.911455  0.199485 -0.638089 -0.416854  2.351695  0.430302   \n",
       "20   1.0 -0.290633 -0.433765 -0.443114  0.432201  0.221660  0.040998   \n",
       "21   1.0  0.312291 -0.735255 -0.366935  0.451333 -0.023566 -1.705366   \n",
       "22   1.0 -0.104877  1.272477 -0.808313  0.934850 -0.294110 -0.907465   \n",
       "23   1.0 -0.440780 -2.958484 -0.038005 -0.155518 -2.008415  1.928285   \n",
       "24   1.0 -0.073053  1.296603  1.389923 -0.504007  1.258409  0.214323   \n",
       "25   1.0  0.590173 -0.048059  0.878789 -0.290391  0.171735 -1.404475   \n",
       "26   1.0 -0.399167  0.440825 -1.283827  2.227096 -1.177989 -1.156166   \n",
       "27   1.0 -0.339862 -1.439921  0.217743  0.732350  0.989012 -0.622258   \n",
       "28   1.0  1.658979 -0.270936 -0.033896  0.496706 -0.278124  0.762373   \n",
       "29   1.0 -0.680889 -2.080416  0.582247  0.224400 -0.407728  0.644810   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "90   1.0 -0.013833 -0.324368 -1.163318 -0.827445  1.198508  0.916244   \n",
       "91   1.0  0.395118 -0.404135  0.796659 -1.231262 -0.455600 -0.847728   \n",
       "92   1.0 -1.037205 -2.196969 -0.052552  0.242629  0.827549  0.490336   \n",
       "93   1.0  0.962337 -1.483698  0.473625 -1.269572  0.718894 -1.375272   \n",
       "94   1.0 -0.133296 -1.313003 -0.344768  0.835567  1.303471 -1.383373   \n",
       "95   1.0 -0.781138  0.307556 -0.663133 -0.263828  0.786838 -0.133194   \n",
       "96   1.0 -0.284012 -0.185986  0.460376 -0.155886  0.501357  0.615855   \n",
       "97   1.0  0.494802  0.172424  0.445790  1.080547  0.966801  2.392395   \n",
       "98   1.0  1.452051 -0.904870  0.772610  0.927047 -0.058490  1.123663   \n",
       "99   1.0  0.340752  0.826739  0.147502  0.228408 -1.268531 -1.339232   \n",
       "100  1.0 -1.156193 -1.333003  0.138932 -0.533458 -0.892250  0.372546   \n",
       "101  1.0  2.566892 -0.843184  0.967841 -0.865240  0.541239  0.316399   \n",
       "102  1.0 -0.890357  0.478736 -0.661198 -1.149995 -1.081964 -1.286690   \n",
       "103  1.0  0.234831  1.344638  0.543106 -1.535960  0.747636 -0.632584   \n",
       "104  1.0  0.399513 -0.500048 -0.092196 -0.216849  1.167259 -0.098799   \n",
       "105  1.0 -0.379593  1.374420  0.137976 -0.705007  0.311377 -0.933001   \n",
       "106  1.0 -0.982943 -0.132700  0.669776 -0.535850  0.458167 -0.046628   \n",
       "107  1.0  1.324451 -0.966093  1.537763  0.768892  0.834857 -0.245928   \n",
       "108  1.0 -2.257375  0.346621 -0.483461 -1.039528  0.093341 -1.874148   \n",
       "109  1.0  0.301735 -0.266397 -0.227812  0.572541  0.942559 -0.445910   \n",
       "110  1.0  0.016591 -0.155176  1.458151 -0.781127 -0.752588  2.092981   \n",
       "111  1.0  0.236326  0.238113 -0.816594 -0.543328 -1.189269 -0.291341   \n",
       "112  1.0 -0.898821 -0.515543  0.786032  0.249295  0.557928  0.183303   \n",
       "113  1.0  0.638414  0.990666  1.031411  2.031722  0.910487  0.038755   \n",
       "114  1.0  0.027211  1.992691 -0.304611  1.722787  1.086006  0.842089   \n",
       "115  1.0  0.354690  0.725696  0.243493  0.829609  0.815333  0.619227   \n",
       "116  1.0 -1.680959  1.593497 -0.945763 -0.103994  1.319037  1.877450   \n",
       "117  1.0  1.095581 -0.545238 -0.792189 -1.397642  0.267877  1.850542   \n",
       "118  1.0 -0.314175  0.753043 -0.341610 -0.459202 -0.405305  1.118053   \n",
       "119  1.0 -1.065394  0.958108 -0.939434 -0.047829  1.746429 -0.559338   \n",
       "\n",
       "           7         8         9         10        11        12        13  \\\n",
       "0   -1.187309 -1.809241  0.921990 -0.048623  1.400153  0.295839 -0.649771   \n",
       "1   -1.094334  0.981874  0.326451  0.370504 -0.752608  1.196286  0.608868   \n",
       "2    0.058964 -1.989771  1.158538 -0.676569 -0.647796 -1.570263 -0.385619   \n",
       "3   -0.671644 -0.936163  0.598917 -0.674595  0.593295  2.321953 -0.565495   \n",
       "4    1.173315  0.244964  0.302927  0.170116  0.121828 -0.434741 -0.864817   \n",
       "5    0.185775  0.460341 -0.298004 -0.089468 -0.796131 -1.114263  0.050091   \n",
       "6   -2.700651  0.585107  0.982556  0.520110  0.314605  0.003622 -1.499442   \n",
       "7    0.504963  1.832211  0.762544 -1.250540 -0.098536  0.431886  0.465799   \n",
       "8    0.568802 -0.438358  1.031554  0.168371  0.361630  1.105988  0.555555   \n",
       "9    1.071189 -0.712925 -0.286234  0.613680  0.206219  0.074014 -1.082022   \n",
       "10  -0.246922 -0.330596 -0.257201  0.502421  1.269257  1.143528 -0.435369   \n",
       "11   0.108753  1.698550  0.096840  0.787736 -0.649492 -0.960009 -0.405656   \n",
       "12   0.430511  1.312027  1.153205 -0.048670 -0.281967  0.628821  0.073823   \n",
       "13  -0.308180 -1.133219 -0.669685  0.995385 -0.709320  1.033608  0.178217   \n",
       "14   0.037179  0.030929 -2.609712  1.223902 -0.715013 -1.418049  0.882566   \n",
       "15  -2.489097 -0.555055  2.109427  1.538382  1.652889 -0.838494 -1.643691   \n",
       "16  -0.517980 -1.838721  1.322395  1.060383 -0.084937 -0.858862  0.231761   \n",
       "17   0.965592  0.191336  0.499996  0.118169  0.131777 -1.090417 -0.921065   \n",
       "18   2.060731  0.120592 -0.252443 -0.670390  0.224753 -0.015617  0.105302   \n",
       "19  -0.370356 -0.617115  2.006875  0.078716 -0.220873  0.944354 -0.862587   \n",
       "20  -0.145046  0.358298  0.991688  0.998200 -1.618268  0.481274 -1.426305   \n",
       "21  -0.872043 -0.054721 -1.663138 -0.530753 -1.629585 -0.295695  0.862091   \n",
       "22   1.420540  0.549287 -0.155727 -0.228040 -0.128765  0.021928 -0.704515   \n",
       "23   0.822374 -0.511711 -0.488496  1.044512  0.989719 -0.124980 -0.966005   \n",
       "24  -1.921479 -1.389260  0.298724 -0.958645  1.123535  1.784467  0.428863   \n",
       "25  -2.391928  0.418166 -0.918952 -0.533108  0.265932 -0.412331 -0.317886   \n",
       "26   1.639631  0.632697 -0.216161  0.669059 -0.438771 -0.507973  0.306449   \n",
       "27  -0.484682  0.608675 -0.490622 -1.060110 -0.184689 -1.656397 -0.055470   \n",
       "28  -1.579996  0.876143  1.411948  0.770937 -0.842644 -0.121870  0.738804   \n",
       "29   2.126441  1.596192  1.302601 -0.625267  0.728203  0.181350  0.539067   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "90  -0.793833  1.762520  0.912498  1.182887 -0.581198 -0.121828 -0.891564   \n",
       "91   0.106599 -0.966910 -0.576079 -1.678606 -0.366704 -0.755679 -2.214221   \n",
       "92   0.088045 -1.556536 -1.535694  0.786301 -0.114562  1.583300 -0.090033   \n",
       "93   0.308170 -2.695540  0.240131  0.908059 -0.443687  0.743719  0.436824   \n",
       "94  -1.023127  1.049151 -0.617696 -1.974539  0.847049  0.580214 -0.783629   \n",
       "95  -1.838633 -0.079153 -0.677238 -1.100844 -1.377004 -0.521673 -0.213441   \n",
       "96  -0.496964 -1.228920 -0.043244  0.254221 -0.082780  1.184575  0.475846   \n",
       "97  -1.806336  1.643205 -1.633132  0.496792 -0.742969 -0.489626  0.027503   \n",
       "98  -0.610201  1.766335  0.915223 -2.323803  1.551051  1.316492 -1.429129   \n",
       "99   1.032885  0.064164 -0.769165  0.078407 -0.909857 -0.635943  0.573373   \n",
       "100 -0.433719  1.087625 -0.359697 -0.121089 -1.684378 -0.296507 -0.035591   \n",
       "101  1.567672  0.185875 -0.811463  1.102997 -0.229851 -0.266047  2.367364   \n",
       "102 -0.978894  0.691046  0.271246  0.844581  0.003153  0.808210 -0.071268   \n",
       "103 -0.266455  0.463741  1.341787 -1.005388 -0.526252 -0.099187  0.944251   \n",
       "104 -1.439551  1.233285  2.879099 -0.476272  0.422538 -0.259557 -0.554024   \n",
       "105 -2.452414 -2.291273 -1.149213 -1.033316  0.002143  0.556236  0.145493   \n",
       "106 -0.727509  1.598275  0.174711 -0.127460 -1.648830 -0.906735  1.318613   \n",
       "107 -0.630132  0.337157 -1.620996 -1.340387  0.350453 -1.799553 -1.164455   \n",
       "108 -0.580189 -0.102385  0.115753 -0.978703 -0.282042 -0.068302 -0.817777   \n",
       "109  0.765039 -0.110520  0.921936 -1.180993  0.909055 -0.339337 -0.969875   \n",
       "110  0.231840  0.836649 -3.242005  2.554540  0.251309  0.945601 -0.239059   \n",
       "111  1.123865 -2.151016 -0.896312 -2.013886 -0.458833  1.053839 -0.003071   \n",
       "112  1.370312  2.106222  0.201604  1.026928  0.369033 -1.596023  0.327179   \n",
       "113 -1.664829 -1.154991  0.876789  0.437520 -1.914616  1.265751  0.650888   \n",
       "114  1.008068  1.303940  0.015752  0.759068  0.688316  0.249673 -0.436027   \n",
       "115  0.269490  0.793482 -0.862932 -1.850052  0.314250  0.010627 -1.400407   \n",
       "116  0.049087 -0.555811  0.443928 -0.320651 -0.350842  0.309964  0.335206   \n",
       "117 -2.150406  1.238607  0.153936 -0.783455  2.296221  0.868531 -0.011128   \n",
       "118  0.439409 -0.114551  0.395257 -0.685737  1.974084  0.619691  1.859962   \n",
       "119  1.032144 -0.682549  0.575425  0.562728 -0.599750  0.751304 -0.318354   \n",
       "\n",
       "           14  \n",
       "0    0.018622  \n",
       "1    0.964627  \n",
       "2   -2.602803  \n",
       "3    0.145577  \n",
       "4   -0.144488  \n",
       "5   -0.312687  \n",
       "6   -0.845574  \n",
       "7   -2.451167  \n",
       "8    0.480013  \n",
       "9    0.449712  \n",
       "10   0.711527  \n",
       "11   0.798463  \n",
       "12   0.143403  \n",
       "13  -1.897041  \n",
       "14   0.255738  \n",
       "15  -1.101963  \n",
       "16   0.675018  \n",
       "17   0.914476  \n",
       "18  -0.876590  \n",
       "19  -1.201874  \n",
       "20   2.043953  \n",
       "21   1.189902  \n",
       "22   0.109991  \n",
       "23  -0.285723  \n",
       "24  -1.133294  \n",
       "25  -0.848116  \n",
       "26  -0.184829  \n",
       "27  -1.360479  \n",
       "28  -0.520021  \n",
       "29  -1.590740  \n",
       "..        ...  \n",
       "90   0.650155  \n",
       "91   1.023971  \n",
       "92   1.568755  \n",
       "93  -0.018611  \n",
       "94  -1.576371  \n",
       "95   0.097560  \n",
       "96  -0.593911  \n",
       "97   0.600103  \n",
       "98  -1.152140  \n",
       "99  -0.222647  \n",
       "100  0.845945  \n",
       "101 -0.610867  \n",
       "102 -0.882274  \n",
       "103  0.476258  \n",
       "104 -0.359096  \n",
       "105 -1.853645  \n",
       "106  0.039692  \n",
       "107  0.596010  \n",
       "108  1.165414  \n",
       "109  0.285163  \n",
       "110 -1.729303  \n",
       "111  0.233951  \n",
       "112 -0.631026  \n",
       "113 -1.572120  \n",
       "114 -1.259726  \n",
       "115 -1.662923  \n",
       "116 -0.798133  \n",
       "117  1.783243  \n",
       "118  1.896146  \n",
       "119  0.804555  \n",
       "\n",
       "[120 rows x 15 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense, LSTM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "%matplotlib inline\n",
    "\n",
    "# Model.add(LSTM(32, input_shape=(5,2), return_sequences=True, activation='tanh'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5000)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx = np.random.randn(5, 5000)\n",
    "trainx.shape[:-1]\n",
    "\n",
    "input_layer = Input(shape = (5, 1))\n",
    "l1 = LSTM(32, activation='tanh')(input_layer)\n",
    "y1 = Dense(1, activation='softmax')(l1)\n",
    "\n",
    "model = Model(input_layer, y1)\n",
    "trainx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_55 (InputLayer)        (None, 5, 1)              0         \n",
      "_________________________________________________________________\n",
      "lstm_55 (LSTM)               (None, 32)                4352      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=\"mean_squared_error\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_55 to have 3 dimensions, but got array with shape (5, 5000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-03e5ecd85064>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTrainmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1635\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1636\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1637\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1638\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1481\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1483\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1484\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1485\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    111\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    114\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_55 to have 3 dimensions, but got array with shape (5, 5000)"
     ]
    }
   ],
   "source": [
    "Trainmod = model.fit(x=trainx, y=trainx, epochs=2, batch_size=2**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_57 (InputLayer)        (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "lstm_57 (LSTM)               (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 601\n",
      "Trainable params: 601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "visible = Input(shape=(100,1))\n",
    "hidden1 = LSTM(10)(visible)\n",
    "hidden2 = Dense(10, activation='relu')(hidden1)\n",
    "output = Dense(1, activation='sigmoid')(hidden2)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_60 (InputLayer)        (None, 23, 1)             0         \n",
      "_________________________________________________________________\n",
      "lstm_60 (LSTM)               (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 601\n",
      "Trainable params: 601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 1s 213ms/step - loss: 0.9757\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.9738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21ec14862b0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# refref here\n",
    "trainx = np.random.randn(5, 23, 1)\n",
    "trainy = np.random.randn(5, 1, )\n",
    "\n",
    "visible = Input(shape=(23,1))\n",
    "hidden1 = LSTM(10)(visible)\n",
    "hidden2 = Dense(10, activation='relu')(hidden1)\n",
    "output = Dense(1, activation='sigmoid')(hidden2)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "print(model.summary())\n",
    "model.compile(optimizer='adam', loss=\"mean_squared_error\")\n",
    "\n",
    "model.fit(x=trainx, y=trainy, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
