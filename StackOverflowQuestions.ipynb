{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame( (np.random.randn(1000,2)), columns=[\"A\", \"B\"] )\n",
    "X[\"B\"] = (X[\"A\"]) *6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X.A, X.B, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return(x * (x > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. Keras initializes some negative and some positive weights, randomly by default -- you can also specify how the weights are initialized [link] (https://keras.io/initializers/). I think that you're right that some nodes could be dead at the start, if the incoming weights sum up to a value less than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# CUSTOM LOSS FUNCTION ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from keras.backend import clear_session\n",
    "clear_session()\n",
    "\n",
    "from keras import losses\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    final_loss = (losses.binary_crossentropy(y_true[:, 0], y_pred[:, 0]) + \n",
    "                    y_true[:, 0] * \n",
    "                  losses.categorical_crossentropy(y_true[:, 1:], y_pred[:,1:]))\n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate\n",
    "XX = pd.DataFrame( (np.random.randn(1000,2)), columns=[\"A\", \"B\"] )\n",
    "XX[\"B\"] = (XX[\"A\"]) *6 + XX[\"B\"]\n",
    "yy = np.random.randn(1000, 2)\n",
    "\n",
    "n = 2\n",
    "\n",
    "input_layer = Input(shape=(n, ))\n",
    "shared = Dense(32)(input_layer)\n",
    "sub1 = Dense(16)(shared)\n",
    "sub2 = Dense(16)(shared)\n",
    "y1 = Dense(1, activation='sigmoid')(sub1)\n",
    "y2 = Dense(4, activation='softmax')(sub2)\n",
    "mergedOutput = Concatenate()([y1, y2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_layer, mergedOutput)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss=my_loss)\n",
    "yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainmod = model.fit(x=XX, y=yy, epochs=10, batch_size=2**6)\n",
    "# plt.plot(XX.A, X.B, \"o\", label = \"actual\")\n",
    "# plt.plot(AE.predict(X)[:,0], AE.predict(X)[:,1], \"o\", label = \"preds\")\n",
    "\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.array([[13, .12]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def disemvowel(string):\n",
    "    message = []\n",
    "    vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "    for letter in string:\n",
    "         if letter not in vowels:\n",
    "                message.append(letter)\n",
    "    return (message)\n",
    "\n",
    "disemvowel(\"Leol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeInput = Input((2,))\n",
    "encode = Dense(2, activation='relu')(aeInput)\n",
    "aeOutput = Dense(2, activation='relu')(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "# examples of custom loss functions\n",
    "def my_loss(y_true, y_pred):\n",
    "    # this example is mean squared error\n",
    "    # works if if y_pred and y_true are greater than 1D\n",
    "    return K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    # calculate mean(abs(y_pred1*y_pred2 - y_true1*ytrue2)) \n",
    "    return K.mean(K.abs(K.prod(y_pred, axis = 1) - K.prod(y_true, axis = 1)))\n",
    "\n",
    "AE = Model(aeInput, aeOutput, name=\"autoencoder\")\n",
    "AE.compile(optimizer='adam', loss=my_loss, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y_true = np.random.rand(100, 2) + 1\n",
    "y_pred = np.random.rand(100, 2) + 1\n",
    "\n",
    "my_loss(y_true, y_pred) # tensor\n",
    "print(tf.Session().run(K.mean(my_loss(y_true, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = [np.array([[4,  5 ],\n",
    "        [2, 6]], dtype=\"float32\"),\n",
    " np.array([ 0, 0], dtype=\"float32\"),\n",
    " np.array([[1, 1],\n",
    "        [1, 1 ]], dtype=\"float32\"),\n",
    " np.array([0, 0  ], dtype=\"float32\")]\n",
    "\n",
    "AE.set_weights(wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AE.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainAE = AE.fit( x=X, y=X, epochs=10, batch_size=2**6)\n",
    "plt.plot(X.A, X.B, \"o\", label = \"actual\")\n",
    "plt.plot(AE.predict(X)[:,0], AE.predict(X)[:,1], \"o\", label = \"preds\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.evaluate(X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y_true = X.astype(\"float32\")\n",
    "y_pred = AE.predict(X).astype(\"float32\")\n",
    "\n",
    "print(tf.Session().run((my_loss(y_true, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = X\n",
    "y_pred = AE.predict(X)\n",
    "\n",
    "my_loss(y_true, y_pred) # tensor\n",
    "print(tf.Session().run(K.mean(my_loss(y_true, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.predict(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = AE.get_weights()\n",
    "wts = [np.array([[-0,  -0 ],\n",
    "        [ -0, -0]], dtype=\"float32\"),\n",
    " np.array([ 0, 0], dtype=\"float32\"),\n",
    " np.array([[ 0,  0 ],\n",
    "        [ 0 , 0 ]], dtype=\"float32\"),\n",
    " np.array([0, 0  ], dtype=\"float32\")]\n",
    "\n",
    "AE.set_weights(wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtsConcatenated = [np.vstack([wts[ii + 1], wts[ii]]) for ii in np.arange(0, len(wts), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.predict(np.array([[-0.9, -0.33]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.predict(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LayerValues = []\n",
    "    \n",
    "inputData= np.array([[1, -0.9, -0.33]])\n",
    "LayerValues.append(inputData)\n",
    "\n",
    "jj = 0\n",
    "\n",
    "wtsConcatenated = [np.vstack([wts[ii + 1], wts[ii]]) for ii in np.arange(0, len(wts), 2)]\n",
    "\n",
    "nextLayer = np.dot(LayerValues[jj], wtsConcatenated[jj]).astype(\"float32\")\n",
    "nextLayer = np.hstack([np.array([1], dtype = \"float32\").reshape(-1,1), nextLayer] )\n",
    "nextLayer = relu(nextLayer) # apply relu\n",
    "print(nextLayer)\n",
    "jj = 1\n",
    "nextLayer = np.dot(np.array([nextLayer]), wtsConcatenated[jj]).astype(\"float32\")\n",
    "relu(nextLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X.A, X.B, \"o\", label = \"actual\")\n",
    "plt.plot(AE.predict(X)[:,0], AE.predict(X)[:,1], \"o\", label = \"preds\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.kdeplot( X.A, X.B, shade=False, axis=ax)\n",
    "sns.kdeplot(AE.predict(X)[:,0], AE.predict(X)[:,1], shade=False, axis=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(AE.predict(X)[:,0], AE.predict(X)[:,1], \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pylab import imshow, show, get_cmap\n",
    "from numpy import random\n",
    "\n",
    "Z = random.random((50,50))   # Test data\n",
    "\n",
    "imshow(Z, cmap=get_cmap(\"Spectral\"), interpolation='nearest')\n",
    "plt.savefig('your_file.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "BLACK_MIN = np.array([0, 20, 20], np.uint8)\n",
    "BLACK_MAX = np.array([120, 255, 255], np.uint8)\n",
    "imgg = cv2.imread('your_file.tif', 1)\n",
    "dst = cv2.inRange(imgg, BLACK_MIN, BLACK_MAX)\n",
    "\n",
    "no_black = cv2.countNonZero(dst)\n",
    "\n",
    "print('The number of black pixels is: ' + str(no_black))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(imgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### More custom losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Losses:\n",
    "    def IoULoss(targets, inputs, smooth=1e-6):\n",
    "        #targets = K.flatten(targets)\n",
    "#         inputs = K.reshape(inputs, [1, -1]) # 1 row, as many columns as needed\n",
    "#         targets = K.reshape(targets, [-1, 1]) # 1 column, as many rows as needed\n",
    "        inputs = K.reshape(inputs, [1, -1]) # 1 row, as many columns as needed\n",
    "        targets = K.reshape(targets, [-1, 1]) # 1 column, as many rows as needed\n",
    "        print(inputs.shape, targets.shape)\n",
    "        print(K.sum(K.dot(targets, inputs)).shape)\n",
    "        \n",
    "        intersection = K.sum(K.dot(targets, inputs))\n",
    "        total = K.sum(targets) + K.sum(inputs)\n",
    "        union = total - intersection\n",
    "\n",
    "        IoU = (intersection + smooth) / (union + smooth)\n",
    "        return 1 - IoU\n",
    "\n",
    "aeInput = Input((2,))\n",
    "encode = Dense(2, activation='relu')(aeInput)\n",
    "aeOutput = Dense(2, activation='relu')(encode)\n",
    "model = Model(aeInput, aeOutput)\n",
    "\n",
    "model.compile(loss=Losses.IoULoss, optimizer=\"adam\",\n",
    "                      metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = pd.DataFrame( (np.random.randn(1000,2)), columns=[\"A\", \"B\"] )\n",
    "XX[\"B\"] = (XX[\"A\"]) *6 + XX[\"B\"]\n",
    "yy = np.random.randn(1000, 2)\n",
    "\n",
    "model.fit(XX, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xx = np.random.randn(100)\n",
    "xx = np.asarray(xx, np.float32)\n",
    "\n",
    "xx_tf = tf.convert_to_tensor(xx, np.float32)\n",
    "xx_tf = K.reshape(xx_tf, [-1, 1])\n",
    "print(xx_tf.shape)\n",
    "\n",
    "yy = np.random.randn(100)\n",
    "yy = np.asarray(yy, np.float32)\n",
    "\n",
    "yy_tf = tf.convert_to_tensor(xx, np.float32)\n",
    "yy_tf = K.reshape(yy_tf, [1, -1])\n",
    "K.dot(xx_tf, yy_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_tf = K.reshape(xx_tf, [1, -1])\n",
    "yy_tf = K.reshape(yy_tf, [-1, 1])\n",
    "\n",
    "\n",
    "print(tf.Session().run(Losses.IoULoss(xx_tf, yy_tf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.random.rand(100), np.random.rand(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = K.random_uniform_variable(shape=(100, 1), low=0, high=1)\n",
    "y = K.random_uniform_variable(shape=(1, 100), low=0, high=1)\n",
    "xy = K.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = K.flatten(y)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X, y = make_blobs(n_samples = 100, n_features = 2, centers = 2, random_state = 123)\n",
    "\n",
    "# fit supervised KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X, y) \n",
    "\n",
    "# create 50 new data points\n",
    "# with the same number of features as the training set\n",
    "new_data = np.random.randn(50, 2)\n",
    "\n",
    "# predict new labels\n",
    "new_labels = knn.predict(new_data)\n",
    "\n",
    "# plot training clusters\n",
    "plt.plot(X[y== 1, 0], \n",
    "         X[y==1,1], \n",
    "         \"C1o\", label = \"training cluster 1\")\n",
    "plt.plot(X[y== 0, 0], \n",
    "         X[y==0,1], \n",
    "         \"C0o\", label = \"training custer 2\")\n",
    "\n",
    "# plot predictions on new data\n",
    "plt.plot(new_data[new_labels== 1, 0], \n",
    "         new_data[new_labels==1,1], \n",
    "         \"ro\", label = \"new data assigned to cluster 1\")\n",
    "plt.plot(new_data[new_labels== 0, 0], \n",
    "         new_data[new_labels==0,1], \n",
    "         \"bo\", label = \"new data assigned to cluster 2\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_X_train = iris_X[indices[:-10]]\n",
    "iris_y_train = iris_y[indices[:-10]]\n",
    "iris_X_test = iris_X[indices[-10:]]\n",
    "iris_y_test = iris_y[indices[-10:]]\n",
    "# Create and fit a nearest-neighbor classifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(iris_X_train, iris_y_train) \n",
    "\n",
    "\n",
    "\n",
    "knn.predict(iris_X_test)\n",
    "\n",
    "iris_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import time\n",
    "\n",
    "players = input(\"Let's play Five's! How many are you?:\" )\n",
    "#print(\"you are\", players, \"players?\") #test number of players\n",
    "players = int(players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your choices are [0, 5, 10, 15, 20]\n",
      "Computer has chosen 10\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Guess: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're Out.\n",
      "Your choices are [0, 5, 10, 15]\n",
      "Computer has chosen 15\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Guess: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong\n",
      "Your choices are [0, 5, 10]\n",
      "Computer has chosen 5\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Guess: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong\n",
      "Your choices are [0, 5]\n",
      "Computer has chosen 5\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Guess: 1\n",
      "Not allowed, choose again: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your choices are [0]\n",
      "Computer has chosen 0\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Guess: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're Out.\n"
     ]
    }
   ],
   "source": [
    "if players <=1:\n",
    "    print(\"Game Over\")\n",
    "\n",
    "else:\n",
    "    while players >= 1:\n",
    "        #players = players+1\n",
    "\n",
    "#Decide possible values than can be chosen\n",
    "        options = [] #Possible options\n",
    "        for i in range(0,players):\n",
    "            x = i * 5\n",
    "            options.append(x)\n",
    "        print(\"Your choices are\", options)\n",
    "\n",
    "#Playing the game\n",
    "#Each turn\n",
    "        guess = random.choice(options)\n",
    "        print(\"Computer has chosen\", int(guess))\n",
    "        count_down = 3\n",
    "        while (count_down):\n",
    "            print(count_down)\n",
    "            time.sleep(1)\n",
    "            count_down -=  1\n",
    "        choice = input(\"Guess:\")\n",
    "        choice = int(choice)\n",
    "        if choice not in options: #If choice isn't a multiple of 5\n",
    "            input(\"Not allowed, choose again:\")\n",
    "        elif choice in options and choice != guess: #Valid choice but wrong\n",
    "                print(\"Wrong\")                      #so player is still in the\n",
    "        else:                                       #game\n",
    "            choice = int(choice)\n",
    "            if choice == guess: #Correct choice so player leaves game\n",
    "                print(\"You're Out.\") # this should reduce the player count\n",
    "        players -=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def next_batch(X,y,batchsize):\n",
    "    for i in np.arange(0,X.shape[0],batchsize):\n",
    "        yield(X[i:i+batchsize],y[i:i+batchsize])\n",
    "\n",
    "def des(X,y,learning_rate,epoches, batchsize):\n",
    "    X=np.c_[np.ones((X.shape[0])),X]\n",
    "    W=np.random.uniform(size=(X.shape[1],))\n",
    "    lossHistory=[]\n",
    "    for epoch in np.arange(0,epoches):\n",
    "        epochLoss=[]\n",
    "        for (batchX,batchY) in next_batch(X,y,batchsize):\n",
    "            #batchY = batchY.reshape(-1)\n",
    "            preds=batchX.dot(W)\n",
    "            print(preds.shape, batchY.shape)\n",
    "            error=preds-batchY\n",
    "            loss=np.sum(error**2)\n",
    "            epochLoss.append(loss)\n",
    "            gradient=batchX.T.dot(error)/batchX.shape[0]\n",
    "            W+=-learning_rate*gradient\n",
    "    lossHistory.append(np.average(epochLoss))\n",
    "    return W,lossHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 14), (120, 1))"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = np.random.randn(150,13)\n",
    "target = np.random.randn(150)\n",
    "\n",
    "train_data_intercept = np.insert(data, 0, 1, axis=1) \n",
    "train_data,test_data,train_target,test_target = train_test_split(train_data_intercept,(target[:, np.newaxis]), test_size=0.2, random_state=42)\n",
    "#train_data,test_data,train_target,test_target = train_test_split(data,target, test_size=0.2, random_state=42)\n",
    "train_data.shape, train_target.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,) (32, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (15,) (15,32) (15,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-260-3fe6d125dbf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-256-915778a83ade>\u001b[0m in \u001b[0;36mdes\u001b[1;34m(X, y, learning_rate, epoches, batchsize)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mepochLoss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mgradient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatchX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatchX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mW\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mlossHistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochLoss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlossHistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (15,) (15,32) (15,) "
     ]
    }
   ],
   "source": [
    "w, loss = des(train_data,train_target,0.01,10,32)\n",
    "w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.324155</td>\n",
       "      <td>0.280023</td>\n",
       "      <td>0.906174</td>\n",
       "      <td>-0.546467</td>\n",
       "      <td>0.450499</td>\n",
       "      <td>1.544092</td>\n",
       "      <td>-1.187309</td>\n",
       "      <td>-1.809241</td>\n",
       "      <td>0.921990</td>\n",
       "      <td>-0.048623</td>\n",
       "      <td>1.400153</td>\n",
       "      <td>0.295839</td>\n",
       "      <td>-0.649771</td>\n",
       "      <td>0.018622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.296955</td>\n",
       "      <td>-0.014474</td>\n",
       "      <td>-0.373151</td>\n",
       "      <td>-0.983438</td>\n",
       "      <td>1.030189</td>\n",
       "      <td>0.302328</td>\n",
       "      <td>-1.094334</td>\n",
       "      <td>0.981874</td>\n",
       "      <td>0.326451</td>\n",
       "      <td>0.370504</td>\n",
       "      <td>-0.752608</td>\n",
       "      <td>1.196286</td>\n",
       "      <td>0.608868</td>\n",
       "      <td>0.964627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.765843</td>\n",
       "      <td>-0.230580</td>\n",
       "      <td>-0.567077</td>\n",
       "      <td>0.141241</td>\n",
       "      <td>-0.407424</td>\n",
       "      <td>1.817883</td>\n",
       "      <td>0.058964</td>\n",
       "      <td>-1.989771</td>\n",
       "      <td>1.158538</td>\n",
       "      <td>-0.676569</td>\n",
       "      <td>-0.647796</td>\n",
       "      <td>-1.570263</td>\n",
       "      <td>-0.385619</td>\n",
       "      <td>-2.602803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.415270</td>\n",
       "      <td>-0.233568</td>\n",
       "      <td>-0.485154</td>\n",
       "      <td>0.100138</td>\n",
       "      <td>0.777029</td>\n",
       "      <td>0.415959</td>\n",
       "      <td>-0.671644</td>\n",
       "      <td>-0.936163</td>\n",
       "      <td>0.598917</td>\n",
       "      <td>-0.674595</td>\n",
       "      <td>0.593295</td>\n",
       "      <td>2.321953</td>\n",
       "      <td>-0.565495</td>\n",
       "      <td>0.145577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.056278</td>\n",
       "      <td>-0.538365</td>\n",
       "      <td>-2.035299</td>\n",
       "      <td>-0.786087</td>\n",
       "      <td>-0.196908</td>\n",
       "      <td>-0.520349</td>\n",
       "      <td>1.173315</td>\n",
       "      <td>0.244964</td>\n",
       "      <td>0.302927</td>\n",
       "      <td>0.170116</td>\n",
       "      <td>0.121828</td>\n",
       "      <td>-0.434741</td>\n",
       "      <td>-0.864817</td>\n",
       "      <td>-0.144488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.943468</td>\n",
       "      <td>-0.262951</td>\n",
       "      <td>-0.741601</td>\n",
       "      <td>0.582080</td>\n",
       "      <td>-0.472952</td>\n",
       "      <td>-0.234918</td>\n",
       "      <td>0.185775</td>\n",
       "      <td>0.460341</td>\n",
       "      <td>-0.298004</td>\n",
       "      <td>-0.089468</td>\n",
       "      <td>-0.796131</td>\n",
       "      <td>-1.114263</td>\n",
       "      <td>0.050091</td>\n",
       "      <td>-0.312687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.939779</td>\n",
       "      <td>-0.987735</td>\n",
       "      <td>-0.251745</td>\n",
       "      <td>0.133066</td>\n",
       "      <td>1.094890</td>\n",
       "      <td>-0.297343</td>\n",
       "      <td>-2.700651</td>\n",
       "      <td>0.585107</td>\n",
       "      <td>0.982556</td>\n",
       "      <td>0.520110</td>\n",
       "      <td>0.314605</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>-1.499442</td>\n",
       "      <td>-0.845574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.129639</td>\n",
       "      <td>-2.024403</td>\n",
       "      <td>0.810383</td>\n",
       "      <td>0.127532</td>\n",
       "      <td>-0.297367</td>\n",
       "      <td>-0.652425</td>\n",
       "      <td>0.504963</td>\n",
       "      <td>1.832211</td>\n",
       "      <td>0.762544</td>\n",
       "      <td>-1.250540</td>\n",
       "      <td>-0.098536</td>\n",
       "      <td>0.431886</td>\n",
       "      <td>0.465799</td>\n",
       "      <td>-2.451167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.215829</td>\n",
       "      <td>-1.654899</td>\n",
       "      <td>-0.837927</td>\n",
       "      <td>2.861428</td>\n",
       "      <td>0.384361</td>\n",
       "      <td>-0.891486</td>\n",
       "      <td>0.568802</td>\n",
       "      <td>-0.438358</td>\n",
       "      <td>1.031554</td>\n",
       "      <td>0.168371</td>\n",
       "      <td>0.361630</td>\n",
       "      <td>1.105988</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>0.480013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.136869</td>\n",
       "      <td>-1.083203</td>\n",
       "      <td>1.357632</td>\n",
       "      <td>-0.348590</td>\n",
       "      <td>0.383872</td>\n",
       "      <td>-0.341981</td>\n",
       "      <td>1.071189</td>\n",
       "      <td>-0.712925</td>\n",
       "      <td>-0.286234</td>\n",
       "      <td>0.613680</td>\n",
       "      <td>0.206219</td>\n",
       "      <td>0.074014</td>\n",
       "      <td>-1.082022</td>\n",
       "      <td>0.449712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.228964</td>\n",
       "      <td>-0.430872</td>\n",
       "      <td>-1.327280</td>\n",
       "      <td>-0.377273</td>\n",
       "      <td>1.389761</td>\n",
       "      <td>-0.265707</td>\n",
       "      <td>-0.246922</td>\n",
       "      <td>-0.330596</td>\n",
       "      <td>-0.257201</td>\n",
       "      <td>0.502421</td>\n",
       "      <td>1.269257</td>\n",
       "      <td>1.143528</td>\n",
       "      <td>-0.435369</td>\n",
       "      <td>0.711527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.695146</td>\n",
       "      <td>0.457604</td>\n",
       "      <td>-0.769346</td>\n",
       "      <td>-0.035712</td>\n",
       "      <td>0.742386</td>\n",
       "      <td>1.183854</td>\n",
       "      <td>0.108753</td>\n",
       "      <td>1.698550</td>\n",
       "      <td>0.096840</td>\n",
       "      <td>0.787736</td>\n",
       "      <td>-0.649492</td>\n",
       "      <td>-0.960009</td>\n",
       "      <td>-0.405656</td>\n",
       "      <td>0.798463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.673463</td>\n",
       "      <td>-1.902440</td>\n",
       "      <td>-0.240004</td>\n",
       "      <td>2.327057</td>\n",
       "      <td>1.017179</td>\n",
       "      <td>0.351166</td>\n",
       "      <td>0.430511</td>\n",
       "      <td>1.312027</td>\n",
       "      <td>1.153205</td>\n",
       "      <td>-0.048670</td>\n",
       "      <td>-0.281967</td>\n",
       "      <td>0.628821</td>\n",
       "      <td>0.073823</td>\n",
       "      <td>0.143403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.722166</td>\n",
       "      <td>0.021358</td>\n",
       "      <td>1.621775</td>\n",
       "      <td>-0.098504</td>\n",
       "      <td>-0.920280</td>\n",
       "      <td>1.593947</td>\n",
       "      <td>-0.308180</td>\n",
       "      <td>-1.133219</td>\n",
       "      <td>-0.669685</td>\n",
       "      <td>0.995385</td>\n",
       "      <td>-0.709320</td>\n",
       "      <td>1.033608</td>\n",
       "      <td>0.178217</td>\n",
       "      <td>-1.897041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.757093</td>\n",
       "      <td>-1.539339</td>\n",
       "      <td>-0.013525</td>\n",
       "      <td>-0.352009</td>\n",
       "      <td>-0.388951</td>\n",
       "      <td>0.801254</td>\n",
       "      <td>0.037179</td>\n",
       "      <td>0.030929</td>\n",
       "      <td>-2.609712</td>\n",
       "      <td>1.223902</td>\n",
       "      <td>-0.715013</td>\n",
       "      <td>-1.418049</td>\n",
       "      <td>0.882566</td>\n",
       "      <td>0.255738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.628678</td>\n",
       "      <td>-2.632518</td>\n",
       "      <td>1.082110</td>\n",
       "      <td>1.458691</td>\n",
       "      <td>0.686207</td>\n",
       "      <td>0.054818</td>\n",
       "      <td>-2.489097</td>\n",
       "      <td>-0.555055</td>\n",
       "      <td>2.109427</td>\n",
       "      <td>1.538382</td>\n",
       "      <td>1.652889</td>\n",
       "      <td>-0.838494</td>\n",
       "      <td>-1.643691</td>\n",
       "      <td>-1.101963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.057391</td>\n",
       "      <td>-0.654887</td>\n",
       "      <td>0.317156</td>\n",
       "      <td>-0.452213</td>\n",
       "      <td>-0.178781</td>\n",
       "      <td>1.489966</td>\n",
       "      <td>-0.517980</td>\n",
       "      <td>-1.838721</td>\n",
       "      <td>1.322395</td>\n",
       "      <td>1.060383</td>\n",
       "      <td>-0.084937</td>\n",
       "      <td>-0.858862</td>\n",
       "      <td>0.231761</td>\n",
       "      <td>0.675018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.423524</td>\n",
       "      <td>-0.176870</td>\n",
       "      <td>0.318705</td>\n",
       "      <td>0.217995</td>\n",
       "      <td>1.029183</td>\n",
       "      <td>0.429475</td>\n",
       "      <td>0.965592</td>\n",
       "      <td>0.191336</td>\n",
       "      <td>0.499996</td>\n",
       "      <td>0.118169</td>\n",
       "      <td>0.131777</td>\n",
       "      <td>-1.090417</td>\n",
       "      <td>-0.921065</td>\n",
       "      <td>0.914476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.226127</td>\n",
       "      <td>0.813840</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>-0.462188</td>\n",
       "      <td>0.492496</td>\n",
       "      <td>-0.940211</td>\n",
       "      <td>2.060731</td>\n",
       "      <td>0.120592</td>\n",
       "      <td>-0.252443</td>\n",
       "      <td>-0.670390</td>\n",
       "      <td>0.224753</td>\n",
       "      <td>-0.015617</td>\n",
       "      <td>0.105302</td>\n",
       "      <td>-0.876590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.911455</td>\n",
       "      <td>0.199485</td>\n",
       "      <td>-0.638089</td>\n",
       "      <td>-0.416854</td>\n",
       "      <td>2.351695</td>\n",
       "      <td>0.430302</td>\n",
       "      <td>-0.370356</td>\n",
       "      <td>-0.617115</td>\n",
       "      <td>2.006875</td>\n",
       "      <td>0.078716</td>\n",
       "      <td>-0.220873</td>\n",
       "      <td>0.944354</td>\n",
       "      <td>-0.862587</td>\n",
       "      <td>-1.201874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.290633</td>\n",
       "      <td>-0.433765</td>\n",
       "      <td>-0.443114</td>\n",
       "      <td>0.432201</td>\n",
       "      <td>0.221660</td>\n",
       "      <td>0.040998</td>\n",
       "      <td>-0.145046</td>\n",
       "      <td>0.358298</td>\n",
       "      <td>0.991688</td>\n",
       "      <td>0.998200</td>\n",
       "      <td>-1.618268</td>\n",
       "      <td>0.481274</td>\n",
       "      <td>-1.426305</td>\n",
       "      <td>2.043953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.312291</td>\n",
       "      <td>-0.735255</td>\n",
       "      <td>-0.366935</td>\n",
       "      <td>0.451333</td>\n",
       "      <td>-0.023566</td>\n",
       "      <td>-1.705366</td>\n",
       "      <td>-0.872043</td>\n",
       "      <td>-0.054721</td>\n",
       "      <td>-1.663138</td>\n",
       "      <td>-0.530753</td>\n",
       "      <td>-1.629585</td>\n",
       "      <td>-0.295695</td>\n",
       "      <td>0.862091</td>\n",
       "      <td>1.189902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.104877</td>\n",
       "      <td>1.272477</td>\n",
       "      <td>-0.808313</td>\n",
       "      <td>0.934850</td>\n",
       "      <td>-0.294110</td>\n",
       "      <td>-0.907465</td>\n",
       "      <td>1.420540</td>\n",
       "      <td>0.549287</td>\n",
       "      <td>-0.155727</td>\n",
       "      <td>-0.228040</td>\n",
       "      <td>-0.128765</td>\n",
       "      <td>0.021928</td>\n",
       "      <td>-0.704515</td>\n",
       "      <td>0.109991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.440780</td>\n",
       "      <td>-2.958484</td>\n",
       "      <td>-0.038005</td>\n",
       "      <td>-0.155518</td>\n",
       "      <td>-2.008415</td>\n",
       "      <td>1.928285</td>\n",
       "      <td>0.822374</td>\n",
       "      <td>-0.511711</td>\n",
       "      <td>-0.488496</td>\n",
       "      <td>1.044512</td>\n",
       "      <td>0.989719</td>\n",
       "      <td>-0.124980</td>\n",
       "      <td>-0.966005</td>\n",
       "      <td>-0.285723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.073053</td>\n",
       "      <td>1.296603</td>\n",
       "      <td>1.389923</td>\n",
       "      <td>-0.504007</td>\n",
       "      <td>1.258409</td>\n",
       "      <td>0.214323</td>\n",
       "      <td>-1.921479</td>\n",
       "      <td>-1.389260</td>\n",
       "      <td>0.298724</td>\n",
       "      <td>-0.958645</td>\n",
       "      <td>1.123535</td>\n",
       "      <td>1.784467</td>\n",
       "      <td>0.428863</td>\n",
       "      <td>-1.133294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.590173</td>\n",
       "      <td>-0.048059</td>\n",
       "      <td>0.878789</td>\n",
       "      <td>-0.290391</td>\n",
       "      <td>0.171735</td>\n",
       "      <td>-1.404475</td>\n",
       "      <td>-2.391928</td>\n",
       "      <td>0.418166</td>\n",
       "      <td>-0.918952</td>\n",
       "      <td>-0.533108</td>\n",
       "      <td>0.265932</td>\n",
       "      <td>-0.412331</td>\n",
       "      <td>-0.317886</td>\n",
       "      <td>-0.848116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.399167</td>\n",
       "      <td>0.440825</td>\n",
       "      <td>-1.283827</td>\n",
       "      <td>2.227096</td>\n",
       "      <td>-1.177989</td>\n",
       "      <td>-1.156166</td>\n",
       "      <td>1.639631</td>\n",
       "      <td>0.632697</td>\n",
       "      <td>-0.216161</td>\n",
       "      <td>0.669059</td>\n",
       "      <td>-0.438771</td>\n",
       "      <td>-0.507973</td>\n",
       "      <td>0.306449</td>\n",
       "      <td>-0.184829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.339862</td>\n",
       "      <td>-1.439921</td>\n",
       "      <td>0.217743</td>\n",
       "      <td>0.732350</td>\n",
       "      <td>0.989012</td>\n",
       "      <td>-0.622258</td>\n",
       "      <td>-0.484682</td>\n",
       "      <td>0.608675</td>\n",
       "      <td>-0.490622</td>\n",
       "      <td>-1.060110</td>\n",
       "      <td>-0.184689</td>\n",
       "      <td>-1.656397</td>\n",
       "      <td>-0.055470</td>\n",
       "      <td>-1.360479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.658979</td>\n",
       "      <td>-0.270936</td>\n",
       "      <td>-0.033896</td>\n",
       "      <td>0.496706</td>\n",
       "      <td>-0.278124</td>\n",
       "      <td>0.762373</td>\n",
       "      <td>-1.579996</td>\n",
       "      <td>0.876143</td>\n",
       "      <td>1.411948</td>\n",
       "      <td>0.770937</td>\n",
       "      <td>-0.842644</td>\n",
       "      <td>-0.121870</td>\n",
       "      <td>0.738804</td>\n",
       "      <td>-0.520021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.680889</td>\n",
       "      <td>-2.080416</td>\n",
       "      <td>0.582247</td>\n",
       "      <td>0.224400</td>\n",
       "      <td>-0.407728</td>\n",
       "      <td>0.644810</td>\n",
       "      <td>2.126441</td>\n",
       "      <td>1.596192</td>\n",
       "      <td>1.302601</td>\n",
       "      <td>-0.625267</td>\n",
       "      <td>0.728203</td>\n",
       "      <td>0.181350</td>\n",
       "      <td>0.539067</td>\n",
       "      <td>-1.590740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.013833</td>\n",
       "      <td>-0.324368</td>\n",
       "      <td>-1.163318</td>\n",
       "      <td>-0.827445</td>\n",
       "      <td>1.198508</td>\n",
       "      <td>0.916244</td>\n",
       "      <td>-0.793833</td>\n",
       "      <td>1.762520</td>\n",
       "      <td>0.912498</td>\n",
       "      <td>1.182887</td>\n",
       "      <td>-0.581198</td>\n",
       "      <td>-0.121828</td>\n",
       "      <td>-0.891564</td>\n",
       "      <td>0.650155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.395118</td>\n",
       "      <td>-0.404135</td>\n",
       "      <td>0.796659</td>\n",
       "      <td>-1.231262</td>\n",
       "      <td>-0.455600</td>\n",
       "      <td>-0.847728</td>\n",
       "      <td>0.106599</td>\n",
       "      <td>-0.966910</td>\n",
       "      <td>-0.576079</td>\n",
       "      <td>-1.678606</td>\n",
       "      <td>-0.366704</td>\n",
       "      <td>-0.755679</td>\n",
       "      <td>-2.214221</td>\n",
       "      <td>1.023971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.037205</td>\n",
       "      <td>-2.196969</td>\n",
       "      <td>-0.052552</td>\n",
       "      <td>0.242629</td>\n",
       "      <td>0.827549</td>\n",
       "      <td>0.490336</td>\n",
       "      <td>0.088045</td>\n",
       "      <td>-1.556536</td>\n",
       "      <td>-1.535694</td>\n",
       "      <td>0.786301</td>\n",
       "      <td>-0.114562</td>\n",
       "      <td>1.583300</td>\n",
       "      <td>-0.090033</td>\n",
       "      <td>1.568755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962337</td>\n",
       "      <td>-1.483698</td>\n",
       "      <td>0.473625</td>\n",
       "      <td>-1.269572</td>\n",
       "      <td>0.718894</td>\n",
       "      <td>-1.375272</td>\n",
       "      <td>0.308170</td>\n",
       "      <td>-2.695540</td>\n",
       "      <td>0.240131</td>\n",
       "      <td>0.908059</td>\n",
       "      <td>-0.443687</td>\n",
       "      <td>0.743719</td>\n",
       "      <td>0.436824</td>\n",
       "      <td>-0.018611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.133296</td>\n",
       "      <td>-1.313003</td>\n",
       "      <td>-0.344768</td>\n",
       "      <td>0.835567</td>\n",
       "      <td>1.303471</td>\n",
       "      <td>-1.383373</td>\n",
       "      <td>-1.023127</td>\n",
       "      <td>1.049151</td>\n",
       "      <td>-0.617696</td>\n",
       "      <td>-1.974539</td>\n",
       "      <td>0.847049</td>\n",
       "      <td>0.580214</td>\n",
       "      <td>-0.783629</td>\n",
       "      <td>-1.576371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.781138</td>\n",
       "      <td>0.307556</td>\n",
       "      <td>-0.663133</td>\n",
       "      <td>-0.263828</td>\n",
       "      <td>0.786838</td>\n",
       "      <td>-0.133194</td>\n",
       "      <td>-1.838633</td>\n",
       "      <td>-0.079153</td>\n",
       "      <td>-0.677238</td>\n",
       "      <td>-1.100844</td>\n",
       "      <td>-1.377004</td>\n",
       "      <td>-0.521673</td>\n",
       "      <td>-0.213441</td>\n",
       "      <td>0.097560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.284012</td>\n",
       "      <td>-0.185986</td>\n",
       "      <td>0.460376</td>\n",
       "      <td>-0.155886</td>\n",
       "      <td>0.501357</td>\n",
       "      <td>0.615855</td>\n",
       "      <td>-0.496964</td>\n",
       "      <td>-1.228920</td>\n",
       "      <td>-0.043244</td>\n",
       "      <td>0.254221</td>\n",
       "      <td>-0.082780</td>\n",
       "      <td>1.184575</td>\n",
       "      <td>0.475846</td>\n",
       "      <td>-0.593911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.494802</td>\n",
       "      <td>0.172424</td>\n",
       "      <td>0.445790</td>\n",
       "      <td>1.080547</td>\n",
       "      <td>0.966801</td>\n",
       "      <td>2.392395</td>\n",
       "      <td>-1.806336</td>\n",
       "      <td>1.643205</td>\n",
       "      <td>-1.633132</td>\n",
       "      <td>0.496792</td>\n",
       "      <td>-0.742969</td>\n",
       "      <td>-0.489626</td>\n",
       "      <td>0.027503</td>\n",
       "      <td>0.600103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.452051</td>\n",
       "      <td>-0.904870</td>\n",
       "      <td>0.772610</td>\n",
       "      <td>0.927047</td>\n",
       "      <td>-0.058490</td>\n",
       "      <td>1.123663</td>\n",
       "      <td>-0.610201</td>\n",
       "      <td>1.766335</td>\n",
       "      <td>0.915223</td>\n",
       "      <td>-2.323803</td>\n",
       "      <td>1.551051</td>\n",
       "      <td>1.316492</td>\n",
       "      <td>-1.429129</td>\n",
       "      <td>-1.152140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.340752</td>\n",
       "      <td>0.826739</td>\n",
       "      <td>0.147502</td>\n",
       "      <td>0.228408</td>\n",
       "      <td>-1.268531</td>\n",
       "      <td>-1.339232</td>\n",
       "      <td>1.032885</td>\n",
       "      <td>0.064164</td>\n",
       "      <td>-0.769165</td>\n",
       "      <td>0.078407</td>\n",
       "      <td>-0.909857</td>\n",
       "      <td>-0.635943</td>\n",
       "      <td>0.573373</td>\n",
       "      <td>-0.222647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.156193</td>\n",
       "      <td>-1.333003</td>\n",
       "      <td>0.138932</td>\n",
       "      <td>-0.533458</td>\n",
       "      <td>-0.892250</td>\n",
       "      <td>0.372546</td>\n",
       "      <td>-0.433719</td>\n",
       "      <td>1.087625</td>\n",
       "      <td>-0.359697</td>\n",
       "      <td>-0.121089</td>\n",
       "      <td>-1.684378</td>\n",
       "      <td>-0.296507</td>\n",
       "      <td>-0.035591</td>\n",
       "      <td>0.845945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.566892</td>\n",
       "      <td>-0.843184</td>\n",
       "      <td>0.967841</td>\n",
       "      <td>-0.865240</td>\n",
       "      <td>0.541239</td>\n",
       "      <td>0.316399</td>\n",
       "      <td>1.567672</td>\n",
       "      <td>0.185875</td>\n",
       "      <td>-0.811463</td>\n",
       "      <td>1.102997</td>\n",
       "      <td>-0.229851</td>\n",
       "      <td>-0.266047</td>\n",
       "      <td>2.367364</td>\n",
       "      <td>-0.610867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.890357</td>\n",
       "      <td>0.478736</td>\n",
       "      <td>-0.661198</td>\n",
       "      <td>-1.149995</td>\n",
       "      <td>-1.081964</td>\n",
       "      <td>-1.286690</td>\n",
       "      <td>-0.978894</td>\n",
       "      <td>0.691046</td>\n",
       "      <td>0.271246</td>\n",
       "      <td>0.844581</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>0.808210</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.882274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.234831</td>\n",
       "      <td>1.344638</td>\n",
       "      <td>0.543106</td>\n",
       "      <td>-1.535960</td>\n",
       "      <td>0.747636</td>\n",
       "      <td>-0.632584</td>\n",
       "      <td>-0.266455</td>\n",
       "      <td>0.463741</td>\n",
       "      <td>1.341787</td>\n",
       "      <td>-1.005388</td>\n",
       "      <td>-0.526252</td>\n",
       "      <td>-0.099187</td>\n",
       "      <td>0.944251</td>\n",
       "      <td>0.476258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.399513</td>\n",
       "      <td>-0.500048</td>\n",
       "      <td>-0.092196</td>\n",
       "      <td>-0.216849</td>\n",
       "      <td>1.167259</td>\n",
       "      <td>-0.098799</td>\n",
       "      <td>-1.439551</td>\n",
       "      <td>1.233285</td>\n",
       "      <td>2.879099</td>\n",
       "      <td>-0.476272</td>\n",
       "      <td>0.422538</td>\n",
       "      <td>-0.259557</td>\n",
       "      <td>-0.554024</td>\n",
       "      <td>-0.359096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.379593</td>\n",
       "      <td>1.374420</td>\n",
       "      <td>0.137976</td>\n",
       "      <td>-0.705007</td>\n",
       "      <td>0.311377</td>\n",
       "      <td>-0.933001</td>\n",
       "      <td>-2.452414</td>\n",
       "      <td>-2.291273</td>\n",
       "      <td>-1.149213</td>\n",
       "      <td>-1.033316</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.556236</td>\n",
       "      <td>0.145493</td>\n",
       "      <td>-1.853645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.982943</td>\n",
       "      <td>-0.132700</td>\n",
       "      <td>0.669776</td>\n",
       "      <td>-0.535850</td>\n",
       "      <td>0.458167</td>\n",
       "      <td>-0.046628</td>\n",
       "      <td>-0.727509</td>\n",
       "      <td>1.598275</td>\n",
       "      <td>0.174711</td>\n",
       "      <td>-0.127460</td>\n",
       "      <td>-1.648830</td>\n",
       "      <td>-0.906735</td>\n",
       "      <td>1.318613</td>\n",
       "      <td>0.039692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.324451</td>\n",
       "      <td>-0.966093</td>\n",
       "      <td>1.537763</td>\n",
       "      <td>0.768892</td>\n",
       "      <td>0.834857</td>\n",
       "      <td>-0.245928</td>\n",
       "      <td>-0.630132</td>\n",
       "      <td>0.337157</td>\n",
       "      <td>-1.620996</td>\n",
       "      <td>-1.340387</td>\n",
       "      <td>0.350453</td>\n",
       "      <td>-1.799553</td>\n",
       "      <td>-1.164455</td>\n",
       "      <td>0.596010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.257375</td>\n",
       "      <td>0.346621</td>\n",
       "      <td>-0.483461</td>\n",
       "      <td>-1.039528</td>\n",
       "      <td>0.093341</td>\n",
       "      <td>-1.874148</td>\n",
       "      <td>-0.580189</td>\n",
       "      <td>-0.102385</td>\n",
       "      <td>0.115753</td>\n",
       "      <td>-0.978703</td>\n",
       "      <td>-0.282042</td>\n",
       "      <td>-0.068302</td>\n",
       "      <td>-0.817777</td>\n",
       "      <td>1.165414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.301735</td>\n",
       "      <td>-0.266397</td>\n",
       "      <td>-0.227812</td>\n",
       "      <td>0.572541</td>\n",
       "      <td>0.942559</td>\n",
       "      <td>-0.445910</td>\n",
       "      <td>0.765039</td>\n",
       "      <td>-0.110520</td>\n",
       "      <td>0.921936</td>\n",
       "      <td>-1.180993</td>\n",
       "      <td>0.909055</td>\n",
       "      <td>-0.339337</td>\n",
       "      <td>-0.969875</td>\n",
       "      <td>0.285163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016591</td>\n",
       "      <td>-0.155176</td>\n",
       "      <td>1.458151</td>\n",
       "      <td>-0.781127</td>\n",
       "      <td>-0.752588</td>\n",
       "      <td>2.092981</td>\n",
       "      <td>0.231840</td>\n",
       "      <td>0.836649</td>\n",
       "      <td>-3.242005</td>\n",
       "      <td>2.554540</td>\n",
       "      <td>0.251309</td>\n",
       "      <td>0.945601</td>\n",
       "      <td>-0.239059</td>\n",
       "      <td>-1.729303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.236326</td>\n",
       "      <td>0.238113</td>\n",
       "      <td>-0.816594</td>\n",
       "      <td>-0.543328</td>\n",
       "      <td>-1.189269</td>\n",
       "      <td>-0.291341</td>\n",
       "      <td>1.123865</td>\n",
       "      <td>-2.151016</td>\n",
       "      <td>-0.896312</td>\n",
       "      <td>-2.013886</td>\n",
       "      <td>-0.458833</td>\n",
       "      <td>1.053839</td>\n",
       "      <td>-0.003071</td>\n",
       "      <td>0.233951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.898821</td>\n",
       "      <td>-0.515543</td>\n",
       "      <td>0.786032</td>\n",
       "      <td>0.249295</td>\n",
       "      <td>0.557928</td>\n",
       "      <td>0.183303</td>\n",
       "      <td>1.370312</td>\n",
       "      <td>2.106222</td>\n",
       "      <td>0.201604</td>\n",
       "      <td>1.026928</td>\n",
       "      <td>0.369033</td>\n",
       "      <td>-1.596023</td>\n",
       "      <td>0.327179</td>\n",
       "      <td>-0.631026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.638414</td>\n",
       "      <td>0.990666</td>\n",
       "      <td>1.031411</td>\n",
       "      <td>2.031722</td>\n",
       "      <td>0.910487</td>\n",
       "      <td>0.038755</td>\n",
       "      <td>-1.664829</td>\n",
       "      <td>-1.154991</td>\n",
       "      <td>0.876789</td>\n",
       "      <td>0.437520</td>\n",
       "      <td>-1.914616</td>\n",
       "      <td>1.265751</td>\n",
       "      <td>0.650888</td>\n",
       "      <td>-1.572120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>1.992691</td>\n",
       "      <td>-0.304611</td>\n",
       "      <td>1.722787</td>\n",
       "      <td>1.086006</td>\n",
       "      <td>0.842089</td>\n",
       "      <td>1.008068</td>\n",
       "      <td>1.303940</td>\n",
       "      <td>0.015752</td>\n",
       "      <td>0.759068</td>\n",
       "      <td>0.688316</td>\n",
       "      <td>0.249673</td>\n",
       "      <td>-0.436027</td>\n",
       "      <td>-1.259726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.354690</td>\n",
       "      <td>0.725696</td>\n",
       "      <td>0.243493</td>\n",
       "      <td>0.829609</td>\n",
       "      <td>0.815333</td>\n",
       "      <td>0.619227</td>\n",
       "      <td>0.269490</td>\n",
       "      <td>0.793482</td>\n",
       "      <td>-0.862932</td>\n",
       "      <td>-1.850052</td>\n",
       "      <td>0.314250</td>\n",
       "      <td>0.010627</td>\n",
       "      <td>-1.400407</td>\n",
       "      <td>-1.662923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.680959</td>\n",
       "      <td>1.593497</td>\n",
       "      <td>-0.945763</td>\n",
       "      <td>-0.103994</td>\n",
       "      <td>1.319037</td>\n",
       "      <td>1.877450</td>\n",
       "      <td>0.049087</td>\n",
       "      <td>-0.555811</td>\n",
       "      <td>0.443928</td>\n",
       "      <td>-0.320651</td>\n",
       "      <td>-0.350842</td>\n",
       "      <td>0.309964</td>\n",
       "      <td>0.335206</td>\n",
       "      <td>-0.798133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.095581</td>\n",
       "      <td>-0.545238</td>\n",
       "      <td>-0.792189</td>\n",
       "      <td>-1.397642</td>\n",
       "      <td>0.267877</td>\n",
       "      <td>1.850542</td>\n",
       "      <td>-2.150406</td>\n",
       "      <td>1.238607</td>\n",
       "      <td>0.153936</td>\n",
       "      <td>-0.783455</td>\n",
       "      <td>2.296221</td>\n",
       "      <td>0.868531</td>\n",
       "      <td>-0.011128</td>\n",
       "      <td>1.783243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.314175</td>\n",
       "      <td>0.753043</td>\n",
       "      <td>-0.341610</td>\n",
       "      <td>-0.459202</td>\n",
       "      <td>-0.405305</td>\n",
       "      <td>1.118053</td>\n",
       "      <td>0.439409</td>\n",
       "      <td>-0.114551</td>\n",
       "      <td>0.395257</td>\n",
       "      <td>-0.685737</td>\n",
       "      <td>1.974084</td>\n",
       "      <td>0.619691</td>\n",
       "      <td>1.859962</td>\n",
       "      <td>1.896146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.065394</td>\n",
       "      <td>0.958108</td>\n",
       "      <td>-0.939434</td>\n",
       "      <td>-0.047829</td>\n",
       "      <td>1.746429</td>\n",
       "      <td>-0.559338</td>\n",
       "      <td>1.032144</td>\n",
       "      <td>-0.682549</td>\n",
       "      <td>0.575425</td>\n",
       "      <td>0.562728</td>\n",
       "      <td>-0.599750</td>\n",
       "      <td>0.751304</td>\n",
       "      <td>-0.318354</td>\n",
       "      <td>0.804555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6   \\\n",
       "0    1.0 -1.324155  0.280023  0.906174 -0.546467  0.450499  1.544092   \n",
       "1    1.0  0.296955 -0.014474 -0.373151 -0.983438  1.030189  0.302328   \n",
       "2    1.0  0.765843 -0.230580 -0.567077  0.141241 -0.407424  1.817883   \n",
       "3    1.0  1.415270 -0.233568 -0.485154  0.100138  0.777029  0.415959   \n",
       "4    1.0  0.056278 -0.538365 -2.035299 -0.786087 -0.196908 -0.520349   \n",
       "5    1.0  0.943468 -0.262951 -0.741601  0.582080 -0.472952 -0.234918   \n",
       "6    1.0  0.939779 -0.987735 -0.251745  0.133066  1.094890 -0.297343   \n",
       "7    1.0 -1.129639 -2.024403  0.810383  0.127532 -0.297367 -0.652425   \n",
       "8    1.0 -0.215829 -1.654899 -0.837927  2.861428  0.384361 -0.891486   \n",
       "9    1.0  2.136869 -1.083203  1.357632 -0.348590  0.383872 -0.341981   \n",
       "10   1.0  0.228964 -0.430872 -1.327280 -0.377273  1.389761 -0.265707   \n",
       "11   1.0 -0.695146  0.457604 -0.769346 -0.035712  0.742386  1.183854   \n",
       "12   1.0  0.673463 -1.902440 -0.240004  2.327057  1.017179  0.351166   \n",
       "13   1.0  0.722166  0.021358  1.621775 -0.098504 -0.920280  1.593947   \n",
       "14   1.0 -0.757093 -1.539339 -0.013525 -0.352009 -0.388951  0.801254   \n",
       "15   1.0  0.628678 -2.632518  1.082110  1.458691  0.686207  0.054818   \n",
       "16   1.0  1.057391 -0.654887  0.317156 -0.452213 -0.178781  1.489966   \n",
       "17   1.0  0.423524 -0.176870  0.318705  0.217995  1.029183  0.429475   \n",
       "18   1.0  0.226127  0.813840  0.004886 -0.462188  0.492496 -0.940211   \n",
       "19   1.0 -0.911455  0.199485 -0.638089 -0.416854  2.351695  0.430302   \n",
       "20   1.0 -0.290633 -0.433765 -0.443114  0.432201  0.221660  0.040998   \n",
       "21   1.0  0.312291 -0.735255 -0.366935  0.451333 -0.023566 -1.705366   \n",
       "22   1.0 -0.104877  1.272477 -0.808313  0.934850 -0.294110 -0.907465   \n",
       "23   1.0 -0.440780 -2.958484 -0.038005 -0.155518 -2.008415  1.928285   \n",
       "24   1.0 -0.073053  1.296603  1.389923 -0.504007  1.258409  0.214323   \n",
       "25   1.0  0.590173 -0.048059  0.878789 -0.290391  0.171735 -1.404475   \n",
       "26   1.0 -0.399167  0.440825 -1.283827  2.227096 -1.177989 -1.156166   \n",
       "27   1.0 -0.339862 -1.439921  0.217743  0.732350  0.989012 -0.622258   \n",
       "28   1.0  1.658979 -0.270936 -0.033896  0.496706 -0.278124  0.762373   \n",
       "29   1.0 -0.680889 -2.080416  0.582247  0.224400 -0.407728  0.644810   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "90   1.0 -0.013833 -0.324368 -1.163318 -0.827445  1.198508  0.916244   \n",
       "91   1.0  0.395118 -0.404135  0.796659 -1.231262 -0.455600 -0.847728   \n",
       "92   1.0 -1.037205 -2.196969 -0.052552  0.242629  0.827549  0.490336   \n",
       "93   1.0  0.962337 -1.483698  0.473625 -1.269572  0.718894 -1.375272   \n",
       "94   1.0 -0.133296 -1.313003 -0.344768  0.835567  1.303471 -1.383373   \n",
       "95   1.0 -0.781138  0.307556 -0.663133 -0.263828  0.786838 -0.133194   \n",
       "96   1.0 -0.284012 -0.185986  0.460376 -0.155886  0.501357  0.615855   \n",
       "97   1.0  0.494802  0.172424  0.445790  1.080547  0.966801  2.392395   \n",
       "98   1.0  1.452051 -0.904870  0.772610  0.927047 -0.058490  1.123663   \n",
       "99   1.0  0.340752  0.826739  0.147502  0.228408 -1.268531 -1.339232   \n",
       "100  1.0 -1.156193 -1.333003  0.138932 -0.533458 -0.892250  0.372546   \n",
       "101  1.0  2.566892 -0.843184  0.967841 -0.865240  0.541239  0.316399   \n",
       "102  1.0 -0.890357  0.478736 -0.661198 -1.149995 -1.081964 -1.286690   \n",
       "103  1.0  0.234831  1.344638  0.543106 -1.535960  0.747636 -0.632584   \n",
       "104  1.0  0.399513 -0.500048 -0.092196 -0.216849  1.167259 -0.098799   \n",
       "105  1.0 -0.379593  1.374420  0.137976 -0.705007  0.311377 -0.933001   \n",
       "106  1.0 -0.982943 -0.132700  0.669776 -0.535850  0.458167 -0.046628   \n",
       "107  1.0  1.324451 -0.966093  1.537763  0.768892  0.834857 -0.245928   \n",
       "108  1.0 -2.257375  0.346621 -0.483461 -1.039528  0.093341 -1.874148   \n",
       "109  1.0  0.301735 -0.266397 -0.227812  0.572541  0.942559 -0.445910   \n",
       "110  1.0  0.016591 -0.155176  1.458151 -0.781127 -0.752588  2.092981   \n",
       "111  1.0  0.236326  0.238113 -0.816594 -0.543328 -1.189269 -0.291341   \n",
       "112  1.0 -0.898821 -0.515543  0.786032  0.249295  0.557928  0.183303   \n",
       "113  1.0  0.638414  0.990666  1.031411  2.031722  0.910487  0.038755   \n",
       "114  1.0  0.027211  1.992691 -0.304611  1.722787  1.086006  0.842089   \n",
       "115  1.0  0.354690  0.725696  0.243493  0.829609  0.815333  0.619227   \n",
       "116  1.0 -1.680959  1.593497 -0.945763 -0.103994  1.319037  1.877450   \n",
       "117  1.0  1.095581 -0.545238 -0.792189 -1.397642  0.267877  1.850542   \n",
       "118  1.0 -0.314175  0.753043 -0.341610 -0.459202 -0.405305  1.118053   \n",
       "119  1.0 -1.065394  0.958108 -0.939434 -0.047829  1.746429 -0.559338   \n",
       "\n",
       "           7         8         9         10        11        12        13  \\\n",
       "0   -1.187309 -1.809241  0.921990 -0.048623  1.400153  0.295839 -0.649771   \n",
       "1   -1.094334  0.981874  0.326451  0.370504 -0.752608  1.196286  0.608868   \n",
       "2    0.058964 -1.989771  1.158538 -0.676569 -0.647796 -1.570263 -0.385619   \n",
       "3   -0.671644 -0.936163  0.598917 -0.674595  0.593295  2.321953 -0.565495   \n",
       "4    1.173315  0.244964  0.302927  0.170116  0.121828 -0.434741 -0.864817   \n",
       "5    0.185775  0.460341 -0.298004 -0.089468 -0.796131 -1.114263  0.050091   \n",
       "6   -2.700651  0.585107  0.982556  0.520110  0.314605  0.003622 -1.499442   \n",
       "7    0.504963  1.832211  0.762544 -1.250540 -0.098536  0.431886  0.465799   \n",
       "8    0.568802 -0.438358  1.031554  0.168371  0.361630  1.105988  0.555555   \n",
       "9    1.071189 -0.712925 -0.286234  0.613680  0.206219  0.074014 -1.082022   \n",
       "10  -0.246922 -0.330596 -0.257201  0.502421  1.269257  1.143528 -0.435369   \n",
       "11   0.108753  1.698550  0.096840  0.787736 -0.649492 -0.960009 -0.405656   \n",
       "12   0.430511  1.312027  1.153205 -0.048670 -0.281967  0.628821  0.073823   \n",
       "13  -0.308180 -1.133219 -0.669685  0.995385 -0.709320  1.033608  0.178217   \n",
       "14   0.037179  0.030929 -2.609712  1.223902 -0.715013 -1.418049  0.882566   \n",
       "15  -2.489097 -0.555055  2.109427  1.538382  1.652889 -0.838494 -1.643691   \n",
       "16  -0.517980 -1.838721  1.322395  1.060383 -0.084937 -0.858862  0.231761   \n",
       "17   0.965592  0.191336  0.499996  0.118169  0.131777 -1.090417 -0.921065   \n",
       "18   2.060731  0.120592 -0.252443 -0.670390  0.224753 -0.015617  0.105302   \n",
       "19  -0.370356 -0.617115  2.006875  0.078716 -0.220873  0.944354 -0.862587   \n",
       "20  -0.145046  0.358298  0.991688  0.998200 -1.618268  0.481274 -1.426305   \n",
       "21  -0.872043 -0.054721 -1.663138 -0.530753 -1.629585 -0.295695  0.862091   \n",
       "22   1.420540  0.549287 -0.155727 -0.228040 -0.128765  0.021928 -0.704515   \n",
       "23   0.822374 -0.511711 -0.488496  1.044512  0.989719 -0.124980 -0.966005   \n",
       "24  -1.921479 -1.389260  0.298724 -0.958645  1.123535  1.784467  0.428863   \n",
       "25  -2.391928  0.418166 -0.918952 -0.533108  0.265932 -0.412331 -0.317886   \n",
       "26   1.639631  0.632697 -0.216161  0.669059 -0.438771 -0.507973  0.306449   \n",
       "27  -0.484682  0.608675 -0.490622 -1.060110 -0.184689 -1.656397 -0.055470   \n",
       "28  -1.579996  0.876143  1.411948  0.770937 -0.842644 -0.121870  0.738804   \n",
       "29   2.126441  1.596192  1.302601 -0.625267  0.728203  0.181350  0.539067   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "90  -0.793833  1.762520  0.912498  1.182887 -0.581198 -0.121828 -0.891564   \n",
       "91   0.106599 -0.966910 -0.576079 -1.678606 -0.366704 -0.755679 -2.214221   \n",
       "92   0.088045 -1.556536 -1.535694  0.786301 -0.114562  1.583300 -0.090033   \n",
       "93   0.308170 -2.695540  0.240131  0.908059 -0.443687  0.743719  0.436824   \n",
       "94  -1.023127  1.049151 -0.617696 -1.974539  0.847049  0.580214 -0.783629   \n",
       "95  -1.838633 -0.079153 -0.677238 -1.100844 -1.377004 -0.521673 -0.213441   \n",
       "96  -0.496964 -1.228920 -0.043244  0.254221 -0.082780  1.184575  0.475846   \n",
       "97  -1.806336  1.643205 -1.633132  0.496792 -0.742969 -0.489626  0.027503   \n",
       "98  -0.610201  1.766335  0.915223 -2.323803  1.551051  1.316492 -1.429129   \n",
       "99   1.032885  0.064164 -0.769165  0.078407 -0.909857 -0.635943  0.573373   \n",
       "100 -0.433719  1.087625 -0.359697 -0.121089 -1.684378 -0.296507 -0.035591   \n",
       "101  1.567672  0.185875 -0.811463  1.102997 -0.229851 -0.266047  2.367364   \n",
       "102 -0.978894  0.691046  0.271246  0.844581  0.003153  0.808210 -0.071268   \n",
       "103 -0.266455  0.463741  1.341787 -1.005388 -0.526252 -0.099187  0.944251   \n",
       "104 -1.439551  1.233285  2.879099 -0.476272  0.422538 -0.259557 -0.554024   \n",
       "105 -2.452414 -2.291273 -1.149213 -1.033316  0.002143  0.556236  0.145493   \n",
       "106 -0.727509  1.598275  0.174711 -0.127460 -1.648830 -0.906735  1.318613   \n",
       "107 -0.630132  0.337157 -1.620996 -1.340387  0.350453 -1.799553 -1.164455   \n",
       "108 -0.580189 -0.102385  0.115753 -0.978703 -0.282042 -0.068302 -0.817777   \n",
       "109  0.765039 -0.110520  0.921936 -1.180993  0.909055 -0.339337 -0.969875   \n",
       "110  0.231840  0.836649 -3.242005  2.554540  0.251309  0.945601 -0.239059   \n",
       "111  1.123865 -2.151016 -0.896312 -2.013886 -0.458833  1.053839 -0.003071   \n",
       "112  1.370312  2.106222  0.201604  1.026928  0.369033 -1.596023  0.327179   \n",
       "113 -1.664829 -1.154991  0.876789  0.437520 -1.914616  1.265751  0.650888   \n",
       "114  1.008068  1.303940  0.015752  0.759068  0.688316  0.249673 -0.436027   \n",
       "115  0.269490  0.793482 -0.862932 -1.850052  0.314250  0.010627 -1.400407   \n",
       "116  0.049087 -0.555811  0.443928 -0.320651 -0.350842  0.309964  0.335206   \n",
       "117 -2.150406  1.238607  0.153936 -0.783455  2.296221  0.868531 -0.011128   \n",
       "118  0.439409 -0.114551  0.395257 -0.685737  1.974084  0.619691  1.859962   \n",
       "119  1.032144 -0.682549  0.575425  0.562728 -0.599750  0.751304 -0.318354   \n",
       "\n",
       "           14  \n",
       "0    0.018622  \n",
       "1    0.964627  \n",
       "2   -2.602803  \n",
       "3    0.145577  \n",
       "4   -0.144488  \n",
       "5   -0.312687  \n",
       "6   -0.845574  \n",
       "7   -2.451167  \n",
       "8    0.480013  \n",
       "9    0.449712  \n",
       "10   0.711527  \n",
       "11   0.798463  \n",
       "12   0.143403  \n",
       "13  -1.897041  \n",
       "14   0.255738  \n",
       "15  -1.101963  \n",
       "16   0.675018  \n",
       "17   0.914476  \n",
       "18  -0.876590  \n",
       "19  -1.201874  \n",
       "20   2.043953  \n",
       "21   1.189902  \n",
       "22   0.109991  \n",
       "23  -0.285723  \n",
       "24  -1.133294  \n",
       "25  -0.848116  \n",
       "26  -0.184829  \n",
       "27  -1.360479  \n",
       "28  -0.520021  \n",
       "29  -1.590740  \n",
       "..        ...  \n",
       "90   0.650155  \n",
       "91   1.023971  \n",
       "92   1.568755  \n",
       "93  -0.018611  \n",
       "94  -1.576371  \n",
       "95   0.097560  \n",
       "96  -0.593911  \n",
       "97   0.600103  \n",
       "98  -1.152140  \n",
       "99  -0.222647  \n",
       "100  0.845945  \n",
       "101 -0.610867  \n",
       "102 -0.882274  \n",
       "103  0.476258  \n",
       "104 -0.359096  \n",
       "105 -1.853645  \n",
       "106  0.039692  \n",
       "107  0.596010  \n",
       "108  1.165414  \n",
       "109  0.285163  \n",
       "110 -1.729303  \n",
       "111  0.233951  \n",
       "112 -0.631026  \n",
       "113 -1.572120  \n",
       "114 -1.259726  \n",
       "115 -1.662923  \n",
       "116 -0.798133  \n",
       "117  1.783243  \n",
       "118  1.896146  \n",
       "119  0.804555  \n",
       "\n",
       "[120 rows x 15 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense, LSTM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "%matplotlib inline\n",
    "\n",
    "# Model.add(LSTM(32, input_shape=(5,2), return_sequences=True, activation='tanh'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5000)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx = np.random.randn(5, 5000)\n",
    "trainx.shape[:-1]\n",
    "\n",
    "input_layer = Input(shape = (5, 1))\n",
    "l1 = LSTM(32, activation='tanh')(input_layer)\n",
    "y1 = Dense(1, activation='softmax')(l1)\n",
    "\n",
    "model = Model(input_layer, y1)\n",
    "trainx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_55 (InputLayer)        (None, 5, 1)              0         \n",
      "_________________________________________________________________\n",
      "lstm_55 (LSTM)               (None, 32)                4352      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=\"mean_squared_error\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_55 to have 3 dimensions, but got array with shape (5, 5000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-03e5ecd85064>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTrainmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1635\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1636\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1637\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1638\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1481\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1483\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1484\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1485\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    111\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    114\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_55 to have 3 dimensions, but got array with shape (5, 5000)"
     ]
    }
   ],
   "source": [
    "Trainmod = model.fit(x=trainx, y=trainx, epochs=2, batch_size=2**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_57 (InputLayer)        (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "lstm_57 (LSTM)               (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 601\n",
      "Trainable params: 601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "visible = Input(shape=(100,1))\n",
    "hidden1 = LSTM(10)(visible)\n",
    "hidden2 = Dense(10, activation='relu')(hidden1)\n",
    "output = Dense(1, activation='sigmoid')(hidden2)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_60 (InputLayer)        (None, 23, 1)             0         \n",
      "_________________________________________________________________\n",
      "lstm_60 (LSTM)               (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 601\n",
      "Trainable params: 601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 1s 213ms/step - loss: 0.9757\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.9738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21ec14862b0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# refref here\n",
    "trainx = np.random.randn(5, 23, 1)\n",
    "trainy = np.random.randn(5, 1, )\n",
    "\n",
    "visible = Input(shape=(23,1))\n",
    "hidden1 = LSTM(10)(visible)\n",
    "hidden2 = Dense(10, activation='relu')(hidden1)\n",
    "output = Dense(1, activation='sigmoid')(hidden2)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "print(model.summary())\n",
    "model.compile(optimizer='adam', loss=\"mean_squared_error\")\n",
    "\n",
    "model.fit(x=trainx, y=trainy, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Snowfall Accumulation (in.)')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"GFS\": [np.nan, np.nan, 1, 2, np.nan, \n",
    "                          2, 3, np.nan, np.nan, 4], \n",
    "                    \"RAP\": [-2.45832646,  0.56266567, -0.4453474 , \n",
    "                            -0.85447845, -1.34830127,\n",
    "                            -0.38113925, -0.41400397,  \n",
    "                            np.nan, -0.78764545, -0.02807674]})\n",
    "\n",
    "fh = np.array([\"Fri 4 am\", \"Fri 6 am\",\"Fri 8 am\",\"Fri 10 am\",\n",
    "                \"Fri 6 pm\",\"Fri 10 pm\",\"Sat 4 am\",\"Sat 6 am\",\n",
    "                \"Sat 8 am\",\"100az 10 am\"\n",
    "                ])\n",
    "\n",
    "gfs = df['GFS']\n",
    "rap = df['RAP']\n",
    "\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(10,8))\n",
    "\n",
    "# workaround to set the order of xlabels\n",
    "ax2.plot(fh, [np.nan]*len(fh)) \n",
    "\n",
    "# remove nan's  so that the points are connected\n",
    "ax2.plot(fh[~np.isnan(gfs)], gfs[~np.isnan(gfs)], \"ob-\") \n",
    "ax2.plot(fh[~np.isnan(rap)],rap[~np.isnan(rap)],marker='x')\n",
    "\n",
    "ax2.tick_params(which='major',labelsize='12')\n",
    "ax2.grid(which='major', color='#CCCCCC', linestyle='-')\n",
    "plt.xticks(rotation='90')\n",
    "plt.xlabel('Forecast Run')\n",
    "plt.ylabel('Snowfall Accumulation (in.)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2     True\n",
       "3     True\n",
       "4    False\n",
       "5     True\n",
       "6     True\n",
       "7    False\n",
       "8    False\n",
       "9     True\n",
       "Name: GFS, dtype: bool"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~np.isnan(gfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 2001)              0         \n",
      "_________________________________________________________________\n",
      "enc0 (Dense)                 (None, 1000)              2002000   \n",
      "_________________________________________________________________\n",
      "enc1 (Dense)                 (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "enc2 (Dense)                 (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "enc3 (Dense)                 (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dec2 (Dense)                 (None, 200)               10200     \n",
      "_________________________________________________________________\n",
      "dec1 (Dense)                 (None, 500)               100500    \n",
      "_________________________________________________________________\n",
      "dec0 (Dense)                 (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2001)              2003001   \n",
      "=================================================================\n",
      "Total params: 5,227,451\n",
      "Trainable params: 5,227,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "# data\n",
    "data = np.random.randn(20000, 2001) + 100# read my input samples. They are 1d functions of time and I have 2000 of them.\n",
    "# Each function has 2001 time components\n",
    "\n",
    "# shuffling data before training\n",
    "import random\n",
    "random.seed(5)\n",
    "random.shuffle(data)\n",
    "\n",
    "# split training (1500 samples) and testing (500 samples) dataset\n",
    "X_train = data[:1500]\n",
    "X_test = data[1500:]\n",
    "\n",
    "# normalize training and testing set using mean and std deviation of training set\n",
    "X_mean = X_train.mean()\n",
    "X_train -= X_mean\n",
    "X_std = X_train.std()\n",
    "X_train /= X_std\n",
    "\n",
    "X_test -= X_mean\n",
    "X_test /= X_std\n",
    "\n",
    "\n",
    "### MODEL ###\n",
    "\n",
    "# Architecture\n",
    "\n",
    "# input layer\n",
    "input_shape = [X_train.shape[1]]\n",
    "X_input = Input(input_shape)\n",
    "\n",
    "# hidden layers\n",
    "\n",
    "x = Dense(1000, activation='tanh', name='enc0')(X_input)\n",
    "encoded = Dense(500, activation='tanh', name='enc1')(x)\n",
    "encoded_2 = Dense(200, activation='tanh', name='enc2')(encoded)\n",
    "encoded_3 = Dense(50, activation='tanh', name='enc3')(encoded_2)\n",
    "decoded_2 = Dense(200, activation='tanh', name='dec2')(encoded_3)\n",
    "decoded_1 = Dense(500, activation='tanh', name='dec1')(decoded_2)\n",
    "x2 = Dense(1000, activation='tanh', name='dec0')(decoded_1)\n",
    "\n",
    "# output layer\n",
    "decoded = Dense(input_shape[0], name='out')(x2)\n",
    "\n",
    "# the Model\n",
    "model = Model(inputs=X_input, outputs=decoded, name='autoencoder')\n",
    "\n",
    "# optimizer\n",
    "opt = optimizers.Adamax()\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['acc'])\n",
    "print(model.summary())\n",
    "\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/200\n",
      "1200/1200 [==============================] - 1s 729us/step - loss: 1.0182 - acc: 0.0000e+00 - val_loss: 1.0009 - val_acc: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.9901 - acc: 0.0058 - val_loss: 0.9921 - val_acc: 0.0000e+00\n",
      "Epoch 3/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.9703 - acc: 0.0000e+00 - val_loss: 0.9858 - val_acc: 0.0000e+00\n",
      "Epoch 4/200\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 0.9516 - acc: 0.0017 - val_loss: 0.9810 - val_acc: 0.0000e+00\n",
      "Epoch 5/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.9331 - acc: 0.0042 - val_loss: 0.9781 - val_acc: 0.0033\n",
      "Epoch 6/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.9151 - acc: 0.0108 - val_loss: 0.9771 - val_acc: 0.0033\n",
      "Epoch 7/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.8981 - acc: 0.0158 - val_loss: 0.9781 - val_acc: 0.0067\n",
      "Epoch 8/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.8826 - acc: 0.0225 - val_loss: 0.9808 - val_acc: 0.0033\n",
      "Epoch 9/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.8689 - acc: 0.0225 - val_loss: 0.9848 - val_acc: 0.0067\n",
      "Epoch 10/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.8568 - acc: 0.0242 - val_loss: 0.9896 - val_acc: 0.0033\n",
      "Epoch 11/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.8464 - acc: 0.0267 - val_loss: 0.9946 - val_acc: 0.0000e+00\n",
      "Epoch 12/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.8376 - acc: 0.0192 - val_loss: 0.9993 - val_acc: 0.0000e+00\n",
      "Epoch 13/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.8298 - acc: 0.0267 - val_loss: 1.0033 - val_acc: 0.0033\n",
      "Epoch 14/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.8229 - acc: 0.0275 - val_loss: 1.0065 - val_acc: 0.0100\n",
      "Epoch 15/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.8167 - acc: 0.0317 - val_loss: 1.0086 - val_acc: 0.0100\n",
      "Epoch 16/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.8109 - acc: 0.0317 - val_loss: 1.0097 - val_acc: 0.0067\n",
      "Epoch 17/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.8055 - acc: 0.0367 - val_loss: 1.0100 - val_acc: 0.0067\n",
      "Epoch 18/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.8005 - acc: 0.0383 - val_loss: 1.0097 - val_acc: 0.0067\n",
      "Epoch 19/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.7960 - acc: 0.0408 - val_loss: 1.0089 - val_acc: 0.0133\n",
      "Epoch 20/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7919 - acc: 0.0517 - val_loss: 1.0077 - val_acc: 0.0167\n",
      "Epoch 21/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7881 - acc: 0.0508 - val_loss: 1.0066 - val_acc: 0.0167\n",
      "Epoch 22/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7847 - acc: 0.0617 - val_loss: 1.0057 - val_acc: 0.0167\n",
      "Epoch 23/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7815 - acc: 0.0600 - val_loss: 1.0050 - val_acc: 0.0133\n",
      "Epoch 24/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7786 - acc: 0.0658 - val_loss: 1.0044 - val_acc: 0.0100\n",
      "Epoch 25/200\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 0.7760 - acc: 0.0750 - val_loss: 1.0039 - val_acc: 0.0100\n",
      "Epoch 26/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7735 - acc: 0.0700 - val_loss: 1.0037 - val_acc: 0.0100\n",
      "Epoch 27/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7712 - acc: 0.0767 - val_loss: 1.0038 - val_acc: 0.0100\n",
      "Epoch 28/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7690 - acc: 0.0833 - val_loss: 1.0041 - val_acc: 0.0100\n",
      "Epoch 29/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7669 - acc: 0.0875 - val_loss: 1.0045 - val_acc: 0.0100\n",
      "Epoch 30/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7649 - acc: 0.0925 - val_loss: 1.0050 - val_acc: 0.0100\n",
      "Epoch 31/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.7630 - acc: 0.0850 - val_loss: 1.0056 - val_acc: 0.0100\n",
      "Epoch 32/200\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 0.7611 - acc: 0.0858 - val_loss: 1.0063 - val_acc: 0.0100\n",
      "Epoch 33/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.7593 - acc: 0.0942 - val_loss: 1.0070 - val_acc: 0.0100\n",
      "Epoch 34/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7576 - acc: 0.0892 - val_loss: 1.0077 - val_acc: 0.0100\n",
      "Epoch 35/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.7559 - acc: 0.0917 - val_loss: 1.0082 - val_acc: 0.0133\n",
      "Epoch 36/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.7542 - acc: 0.0942 - val_loss: 1.0087 - val_acc: 0.0133\n",
      "Epoch 37/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.7526 - acc: 0.1000 - val_loss: 1.0088 - val_acc: 0.0133\n",
      "Epoch 38/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7509 - acc: 0.1000 - val_loss: 1.0093 - val_acc: 0.0100\n",
      "Epoch 39/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7494 - acc: 0.0992 - val_loss: 1.0092 - val_acc: 0.0100\n",
      "Epoch 40/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7477 - acc: 0.0992 - val_loss: 1.0095 - val_acc: 0.0100\n",
      "Epoch 41/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.7460 - acc: 0.0983 - val_loss: 1.0095 - val_acc: 0.0100\n",
      "Epoch 42/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7442 - acc: 0.0942 - val_loss: 1.0095 - val_acc: 0.0100\n",
      "Epoch 43/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.7425 - acc: 0.0942 - val_loss: 1.0098 - val_acc: 0.0100\n",
      "Epoch 44/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7407 - acc: 0.0925 - val_loss: 1.0097 - val_acc: 0.0067\n",
      "Epoch 45/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7389 - acc: 0.0892 - val_loss: 1.0100 - val_acc: 0.0067\n",
      "Epoch 46/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7369 - acc: 0.0850 - val_loss: 1.0099 - val_acc: 0.0100\n",
      "Epoch 47/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7348 - acc: 0.0858 - val_loss: 1.0100 - val_acc: 0.0067\n",
      "Epoch 48/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7326 - acc: 0.0850 - val_loss: 1.0100 - val_acc: 0.0067\n",
      "Epoch 49/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7303 - acc: 0.0850 - val_loss: 1.0099 - val_acc: 0.0067\n",
      "Epoch 50/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7278 - acc: 0.0850 - val_loss: 1.0099 - val_acc: 0.0067\n",
      "Epoch 51/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7252 - acc: 0.0875 - val_loss: 1.0097 - val_acc: 0.0067\n",
      "Epoch 52/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.7223 - acc: 0.0875 - val_loss: 1.0096 - val_acc: 0.0067\n",
      "Epoch 53/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.7193 - acc: 0.0908 - val_loss: 1.0094 - val_acc: 0.0033\n",
      "Epoch 54/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7160 - acc: 0.0942 - val_loss: 1.0092 - val_acc: 0.0033\n",
      "Epoch 55/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7126 - acc: 0.0942 - val_loss: 1.0089 - val_acc: 0.0033\n",
      "Epoch 56/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7089 - acc: 0.0942 - val_loss: 1.0086 - val_acc: 0.0033\n",
      "Epoch 57/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.7049 - acc: 0.0983 - val_loss: 1.0084 - val_acc: 0.0033\n",
      "Epoch 58/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.7007 - acc: 0.1033 - val_loss: 1.0079 - val_acc: 0.0033\n",
      "Epoch 59/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.6962 - acc: 0.1092 - val_loss: 1.0076 - val_acc: 0.0033\n",
      "Epoch 60/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.6916 - acc: 0.1133 - val_loss: 1.0070 - val_acc: 0.0033\n",
      "Epoch 61/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.6867 - acc: 0.1133 - val_loss: 1.0067 - val_acc: 0.0100\n",
      "Epoch 62/200\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 0.6817 - acc: 0.1242 - val_loss: 1.0060 - val_acc: 0.0100\n",
      "Epoch 63/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.6765 - acc: 0.1267 - val_loss: 1.0056 - val_acc: 0.0100\n",
      "Epoch 64/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.6712 - acc: 0.1325 - val_loss: 1.0049 - val_acc: 0.0133\n",
      "Epoch 65/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.6657 - acc: 0.1333 - val_loss: 1.0045 - val_acc: 0.0167\n",
      "Epoch 66/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.6601 - acc: 0.1417 - val_loss: 1.0039 - val_acc: 0.0200\n",
      "Epoch 67/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.6544 - acc: 0.1492 - val_loss: 1.0034 - val_acc: 0.0233\n",
      "Epoch 68/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.6487 - acc: 0.1558 - val_loss: 1.0027 - val_acc: 0.0233\n",
      "Epoch 69/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.6428 - acc: 0.1492 - val_loss: 1.0021 - val_acc: 0.0233\n",
      "Epoch 70/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.6368 - acc: 0.1375 - val_loss: 1.0015 - val_acc: 0.0200\n",
      "Epoch 71/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.6307 - acc: 0.1358 - val_loss: 1.0008 - val_acc: 0.0200\n",
      "Epoch 72/200\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 0.6244 - acc: 0.1400 - val_loss: 1.0003 - val_acc: 0.0200\n",
      "Epoch 73/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.6181 - acc: 0.1567 - val_loss: 0.9995 - val_acc: 0.0200\n",
      "Epoch 74/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.6117 - acc: 0.1558 - val_loss: 0.9990 - val_acc: 0.0200\n",
      "Epoch 75/200\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 0.6052 - acc: 0.1567 - val_loss: 0.9981 - val_acc: 0.0200\n",
      "Epoch 76/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.5985 - acc: 0.1633 - val_loss: 0.9974 - val_acc: 0.0200\n",
      "Epoch 77/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.5918 - acc: 0.1675 - val_loss: 0.9966 - val_acc: 0.0233\n",
      "Epoch 78/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.5850 - acc: 0.1792 - val_loss: 0.9957 - val_acc: 0.0233\n",
      "Epoch 79/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.5781 - acc: 0.1750 - val_loss: 0.9949 - val_acc: 0.0200\n",
      "Epoch 80/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.5711 - acc: 0.1808 - val_loss: 0.9940 - val_acc: 0.0233\n",
      "Epoch 81/200\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 0.5640 - acc: 0.1867 - val_loss: 0.9932 - val_acc: 0.0267\n",
      "Epoch 82/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.5568 - acc: 0.1975 - val_loss: 0.9922 - val_acc: 0.0267\n",
      "Epoch 83/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.5495 - acc: 0.2058 - val_loss: 0.9912 - val_acc: 0.0267\n",
      "Epoch 84/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.5421 - acc: 0.2192 - val_loss: 0.9902 - val_acc: 0.0267\n",
      "Epoch 85/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.5346 - acc: 0.2258 - val_loss: 0.9892 - val_acc: 0.0300\n",
      "Epoch 86/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.5271 - acc: 0.2342 - val_loss: 0.9882 - val_acc: 0.0400\n",
      "Epoch 87/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.5195 - acc: 0.2467 - val_loss: 0.9872 - val_acc: 0.0433\n",
      "Epoch 88/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.5119 - acc: 0.2567 - val_loss: 0.9862 - val_acc: 0.0400\n",
      "Epoch 89/200\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 0.5042 - acc: 0.2592 - val_loss: 0.9852 - val_acc: 0.0400\n",
      "Epoch 90/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.4965 - acc: 0.2642 - val_loss: 0.9841 - val_acc: 0.0433\n",
      "Epoch 91/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.4887 - acc: 0.2708 - val_loss: 0.9831 - val_acc: 0.0467\n",
      "Epoch 92/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.4809 - acc: 0.2767 - val_loss: 0.9821 - val_acc: 0.0467\n",
      "Epoch 93/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.4730 - acc: 0.2825 - val_loss: 0.9811 - val_acc: 0.0533\n",
      "Epoch 94/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.4651 - acc: 0.2892 - val_loss: 0.9801 - val_acc: 0.0533\n",
      "Epoch 95/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.4573 - acc: 0.2917 - val_loss: 0.9791 - val_acc: 0.0567\n",
      "Epoch 96/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.4494 - acc: 0.2967 - val_loss: 0.9782 - val_acc: 0.0633\n",
      "Epoch 97/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.4415 - acc: 0.2967 - val_loss: 0.9772 - val_acc: 0.0633\n",
      "Epoch 98/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.4336 - acc: 0.3033 - val_loss: 0.9763 - val_acc: 0.0667\n",
      "Epoch 99/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.4257 - acc: 0.3067 - val_loss: 0.9753 - val_acc: 0.0700\n",
      "Epoch 100/200\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 0.4179 - acc: 0.3075 - val_loss: 0.9745 - val_acc: 0.0700\n",
      "Epoch 101/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.4101 - acc: 0.3017 - val_loss: 0.9736 - val_acc: 0.0767\n",
      "Epoch 102/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.4023 - acc: 0.3108 - val_loss: 0.9728 - val_acc: 0.0833\n",
      "Epoch 103/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.3946 - acc: 0.3225 - val_loss: 0.9719 - val_acc: 0.0867\n",
      "Epoch 104/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.3869 - acc: 0.3350 - val_loss: 0.9713 - val_acc: 0.0900\n",
      "Epoch 105/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.3794 - acc: 0.3400 - val_loss: 0.9703 - val_acc: 0.0900\n",
      "Epoch 106/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.3721 - acc: 0.3525 - val_loss: 0.9701 - val_acc: 0.0967\n",
      "Epoch 107/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.3647 - acc: 0.3642 - val_loss: 0.9691 - val_acc: 0.0967\n",
      "Epoch 108/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.3575 - acc: 0.3617 - val_loss: 0.9687 - val_acc: 0.0933\n",
      "Epoch 109/200\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 0.3502 - acc: 0.3642 - val_loss: 0.9680 - val_acc: 0.0933\n",
      "Epoch 110/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.3431 - acc: 0.3742 - val_loss: 0.9675 - val_acc: 0.0900\n",
      "Epoch 111/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.3361 - acc: 0.3825 - val_loss: 0.9671 - val_acc: 0.1000\n",
      "Epoch 112/200\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 0.3292 - acc: 0.3967 - val_loss: 0.9665 - val_acc: 0.0967\n",
      "Epoch 113/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.3224 - acc: 0.3983 - val_loss: 0.9659 - val_acc: 0.0933\n",
      "Epoch 114/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.3157 - acc: 0.4000 - val_loss: 0.9656 - val_acc: 0.1000\n",
      "Epoch 115/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.3092 - acc: 0.4100 - val_loss: 0.9651 - val_acc: 0.0967\n",
      "Epoch 116/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.3027 - acc: 0.4167 - val_loss: 0.9646 - val_acc: 0.1000\n",
      "Epoch 117/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.2963 - acc: 0.4217 - val_loss: 0.9641 - val_acc: 0.1067\n",
      "Epoch 118/200\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 0.2901 - acc: 0.4275 - val_loss: 0.9638 - val_acc: 0.1033\n",
      "Epoch 119/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.2840 - acc: 0.4383 - val_loss: 0.9632 - val_acc: 0.1033\n",
      "Epoch 120/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.2779 - acc: 0.4492 - val_loss: 0.9628 - val_acc: 0.1067\n",
      "Epoch 121/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.2720 - acc: 0.4500 - val_loss: 0.9625 - val_acc: 0.1100\n",
      "Epoch 122/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.2661 - acc: 0.4600 - val_loss: 0.9621 - val_acc: 0.1133\n",
      "Epoch 123/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.2604 - acc: 0.4658 - val_loss: 0.9618 - val_acc: 0.1100\n",
      "Epoch 124/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.2549 - acc: 0.4658 - val_loss: 0.9616 - val_acc: 0.1100\n",
      "Epoch 125/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.2494 - acc: 0.4767 - val_loss: 0.9612 - val_acc: 0.1133\n",
      "Epoch 126/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.2440 - acc: 0.4758 - val_loss: 0.9609 - val_acc: 0.1200\n",
      "Epoch 127/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.2386 - acc: 0.4783 - val_loss: 0.9605 - val_acc: 0.1133\n",
      "Epoch 128/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.2334 - acc: 0.4833 - val_loss: 0.9603 - val_acc: 0.1200\n",
      "Epoch 129/200\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 0.2284 - acc: 0.4858 - val_loss: 0.9601 - val_acc: 0.1300\n",
      "Epoch 130/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.2234 - acc: 0.5008 - val_loss: 0.9597 - val_acc: 0.1333\n",
      "Epoch 131/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.2185 - acc: 0.5100 - val_loss: 0.9596 - val_acc: 0.1367\n",
      "Epoch 132/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.2136 - acc: 0.5183 - val_loss: 0.9593 - val_acc: 0.1433\n",
      "Epoch 133/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.2089 - acc: 0.5242 - val_loss: 0.9593 - val_acc: 0.1500\n",
      "Epoch 134/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.2043 - acc: 0.5325 - val_loss: 0.9591 - val_acc: 0.1500\n",
      "Epoch 135/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1999 - acc: 0.5375 - val_loss: 0.9590 - val_acc: 0.1500\n",
      "Epoch 136/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1955 - acc: 0.5450 - val_loss: 0.9588 - val_acc: 0.1633\n",
      "Epoch 137/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1912 - acc: 0.5567 - val_loss: 0.9590 - val_acc: 0.1567\n",
      "Epoch 138/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.1870 - acc: 0.5525 - val_loss: 0.9586 - val_acc: 0.1700\n",
      "Epoch 139/200\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 0.1829 - acc: 0.5542 - val_loss: 0.9586 - val_acc: 0.1700\n",
      "Epoch 140/200\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 0.1788 - acc: 0.5550 - val_loss: 0.9585 - val_acc: 0.1733\n",
      "Epoch 141/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1748 - acc: 0.5650 - val_loss: 0.9584 - val_acc: 0.1733\n",
      "Epoch 142/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1710 - acc: 0.5692 - val_loss: 0.9583 - val_acc: 0.1833\n",
      "Epoch 143/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1673 - acc: 0.5633 - val_loss: 0.9585 - val_acc: 0.1900\n",
      "Epoch 144/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.1636 - acc: 0.5950 - val_loss: 0.9586 - val_acc: 0.1933\n",
      "Epoch 145/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1599 - acc: 0.5850 - val_loss: 0.9585 - val_acc: 0.2000\n",
      "Epoch 146/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1564 - acc: 0.5958 - val_loss: 0.9587 - val_acc: 0.2000\n",
      "Epoch 147/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.1530 - acc: 0.5950 - val_loss: 0.9587 - val_acc: 0.2000\n",
      "Epoch 148/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1496 - acc: 0.6025 - val_loss: 0.9587 - val_acc: 0.2033\n",
      "Epoch 149/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1462 - acc: 0.6067 - val_loss: 0.9586 - val_acc: 0.2067\n",
      "Epoch 150/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1430 - acc: 0.6192 - val_loss: 0.9589 - val_acc: 0.2000\n",
      "Epoch 151/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1399 - acc: 0.6158 - val_loss: 0.9592 - val_acc: 0.2100\n",
      "Epoch 152/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1369 - acc: 0.6225 - val_loss: 0.9593 - val_acc: 0.2133\n",
      "Epoch 153/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1340 - acc: 0.6300 - val_loss: 0.9596 - val_acc: 0.2167\n",
      "Epoch 154/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1311 - acc: 0.6367 - val_loss: 0.9599 - val_acc: 0.2033\n",
      "Epoch 155/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1282 - acc: 0.6142 - val_loss: 0.9600 - val_acc: 0.2133\n",
      "Epoch 156/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1253 - acc: 0.6408 - val_loss: 0.9603 - val_acc: 0.2200\n",
      "Epoch 157/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1225 - acc: 0.6400 - val_loss: 0.9604 - val_acc: 0.2167\n",
      "Epoch 158/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1199 - acc: 0.6367 - val_loss: 0.9607 - val_acc: 0.2233\n",
      "Epoch 159/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1172 - acc: 0.6367 - val_loss: 0.9609 - val_acc: 0.2267\n",
      "Epoch 160/200\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 0.1146 - acc: 0.6508 - val_loss: 0.9614 - val_acc: 0.2333\n",
      "Epoch 161/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.1122 - acc: 0.6625 - val_loss: 0.9614 - val_acc: 0.2233\n",
      "Epoch 162/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1097 - acc: 0.6500 - val_loss: 0.9621 - val_acc: 0.2367\n",
      "Epoch 163/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1072 - acc: 0.6617 - val_loss: 0.9621 - val_acc: 0.2300\n",
      "Epoch 164/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1048 - acc: 0.6583 - val_loss: 0.9625 - val_acc: 0.2233\n",
      "Epoch 165/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1025 - acc: 0.6550 - val_loss: 0.9629 - val_acc: 0.2367\n",
      "Epoch 166/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.1002 - acc: 0.6683 - val_loss: 0.9631 - val_acc: 0.2400\n",
      "Epoch 167/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0979 - acc: 0.6675 - val_loss: 0.9636 - val_acc: 0.2300\n",
      "Epoch 168/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0957 - acc: 0.6658 - val_loss: 0.9641 - val_acc: 0.2367\n",
      "Epoch 169/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0936 - acc: 0.6700 - val_loss: 0.9645 - val_acc: 0.2367\n",
      "Epoch 170/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0915 - acc: 0.6792 - val_loss: 0.9650 - val_acc: 0.2467\n",
      "Epoch 171/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0895 - acc: 0.6775 - val_loss: 0.9654 - val_acc: 0.2400\n",
      "Epoch 172/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0874 - acc: 0.6817 - val_loss: 0.9660 - val_acc: 0.2467\n",
      "Epoch 173/200\n",
      "1200/1200 [==============================] - 0s 50us/step - loss: 0.0855 - acc: 0.6942 - val_loss: 0.9665 - val_acc: 0.2600\n",
      "Epoch 174/200\n",
      "1200/1200 [==============================] - 0s 50us/step - loss: 0.0837 - acc: 0.7050 - val_loss: 0.9670 - val_acc: 0.2467\n",
      "Epoch 175/200\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 0.0818 - acc: 0.6850 - val_loss: 0.9676 - val_acc: 0.2533\n",
      "Epoch 176/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.0799 - acc: 0.7000 - val_loss: 0.9683 - val_acc: 0.2467\n",
      "Epoch 177/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0783 - acc: 0.7025 - val_loss: 0.9688 - val_acc: 0.2533\n",
      "Epoch 178/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0766 - acc: 0.6983 - val_loss: 0.9693 - val_acc: 0.2500\n",
      "Epoch 179/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.0748 - acc: 0.6992 - val_loss: 0.9700 - val_acc: 0.2467\n",
      "Epoch 180/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0730 - acc: 0.7100 - val_loss: 0.9705 - val_acc: 0.2567\n",
      "Epoch 181/200\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.0713 - acc: 0.7217 - val_loss: 0.9712 - val_acc: 0.2533\n",
      "Epoch 182/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0697 - acc: 0.7175 - val_loss: 0.9717 - val_acc: 0.2567\n",
      "Epoch 183/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0680 - acc: 0.7208 - val_loss: 0.9723 - val_acc: 0.2567\n",
      "Epoch 184/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0664 - acc: 0.7200 - val_loss: 0.9730 - val_acc: 0.2600\n",
      "Epoch 185/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0649 - acc: 0.7350 - val_loss: 0.9737 - val_acc: 0.2500\n",
      "Epoch 186/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0634 - acc: 0.7258 - val_loss: 0.9744 - val_acc: 0.2600\n",
      "Epoch 187/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0620 - acc: 0.7350 - val_loss: 0.9751 - val_acc: 0.2567\n",
      "Epoch 188/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0606 - acc: 0.7383 - val_loss: 0.9759 - val_acc: 0.2567\n",
      "Epoch 189/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0591 - acc: 0.7425 - val_loss: 0.9765 - val_acc: 0.2633\n",
      "Epoch 190/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0578 - acc: 0.7525 - val_loss: 0.9771 - val_acc: 0.2600\n",
      "Epoch 191/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0564 - acc: 0.7575 - val_loss: 0.9780 - val_acc: 0.2633\n",
      "Epoch 192/200\n",
      "1200/1200 [==============================] - 0s 50us/step - loss: 0.0551 - acc: 0.7433 - val_loss: 0.9788 - val_acc: 0.2667\n",
      "Epoch 193/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0539 - acc: 0.7508 - val_loss: 0.9797 - val_acc: 0.2600\n",
      "Epoch 194/200\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 0.0527 - acc: 0.7567 - val_loss: 0.9804 - val_acc: 0.2733\n",
      "Epoch 195/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0515 - acc: 0.7550 - val_loss: 0.9810 - val_acc: 0.2733\n",
      "Epoch 196/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0502 - acc: 0.7533 - val_loss: 0.9817 - val_acc: 0.2633\n",
      "Epoch 197/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0490 - acc: 0.7483 - val_loss: 0.9824 - val_acc: 0.2733\n",
      "Epoch 198/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0477 - acc: 0.7742 - val_loss: 0.9832 - val_acc: 0.2833\n",
      "Epoch 199/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0466 - acc: 0.7642 - val_loss: 0.9838 - val_acc: 0.2700\n",
      "Epoch 200/200\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 0.0455 - acc: 0.7658 - val_loss: 0.9847 - val_acc: 0.2900\n"
     ]
    }
   ],
   "source": [
    "### TRAINING ###\n",
    "\n",
    "epochs = 200\n",
    "# train the model\n",
    "history = model.fit(x = X_train, y = X_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=2000,\n",
    "                    validation_split=0.2)  # using 20% of training samples for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing \n",
    "prediction = model.predict(X_test)\n",
    "for i in range(len(prediction)):\n",
    "    prediction[i] = np.multiply(prediction[i], X_std) + X_mean\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_ = np.arange(epochs)\n",
    "plt.figure()\n",
    "plt.plot(epochs_, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs_, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.062056916092937\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import losses\n",
    "prediction = model.predict(X_test)\n",
    "print(np.mean(tf.Session().run(losses.mean_squared_error(X_test, prediction))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0620569160929365"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((prediction - X_test)**2).mean(axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000.629506378205\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "prediction = model.predict(X_test)\n",
    "for i in range(len(prediction)):\n",
    "    prediction[i] = np.multiply(prediction[i], X_std) + X_mean\n",
    "print(np.mean(tf.Session().run(losses.mean_squared_error(X_test, prediction))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.168099032710294\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_train)\n",
    "print(np.mean(tf.Session().run(losses.mean_squared_error(X_train, prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.arange(0, 6000)\n",
    "yy = np.linspace(-0.2,1, num = len(xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24f38519f98>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = [6, 5])\n",
    "plt.plot(xx, yy, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeseries_learn",
   "language": "python",
   "name": "timeseries_learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
