{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame( (np.random.randn(1000,2)), columns=[\"A\", \"B\"] )\n",
    "X[\"B\"] = (X[\"A\"]) *6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X.A, X.B, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return(x * (x > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. Keras initializes some negative and some positive weights, randomly by default -- you can also specify how the weights are initialized [link] (https://keras.io/initializers/). I think that you're right that some nodes could be dead at the start, if the incoming weights sum up to a value less than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# CUSTOM LOSS FUNCTION ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from keras.backend import clear_session\n",
    "clear_session()\n",
    "\n",
    "from keras import losses\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    final_loss = (losses.binary_crossentropy(y_true[:, 0], y_pred[:, 0]) + \n",
    "                    y_true[:, 0] * \n",
    "                  losses.categorical_crossentropy(y_true[:, 1:], y_pred[:,1:]))\n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate\n",
    "XX = pd.DataFrame( (np.random.randn(1000,2)), columns=[\"A\", \"B\"] )\n",
    "XX[\"B\"] = (XX[\"A\"]) *6 + XX[\"B\"]\n",
    "yy = np.random.randn(1000, 2)\n",
    "\n",
    "n = 2\n",
    "\n",
    "input_layer = Input(shape=(n, ))\n",
    "shared = Dense(32)(input_layer)\n",
    "sub1 = Dense(16)(shared)\n",
    "sub2 = Dense(16)(shared)\n",
    "y1 = Dense(1, activation='sigmoid')(sub1)\n",
    "y2 = Dense(4, activation='softmax')(sub2)\n",
    "mergedOutput = Concatenate()([y1, y2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_layer, mergedOutput)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss=my_loss)\n",
    "yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainmod = model.fit(x=XX, y=yy, epochs=10, batch_size=2**6)\n",
    "# plt.plot(XX.A, X.B, \"o\", label = \"actual\")\n",
    "# plt.plot(AE.predict(X)[:,0], AE.predict(X)[:,1], \"o\", label = \"preds\")\n",
    "\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.array([[13, .12]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def disemvowel(string):\n",
    "    message = []\n",
    "    vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "    for letter in string:\n",
    "         if letter not in vowels:\n",
    "                message.append(letter)\n",
    "    return (message)\n",
    "\n",
    "disemvowel(\"Leol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeInput = Input((2,))\n",
    "encode = Dense(2, activation='relu')(aeInput)\n",
    "aeOutput = Dense(2, activation='relu')(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "# examples of custom loss functions\n",
    "def my_loss(y_true, y_pred):\n",
    "    # this example is mean squared error\n",
    "    # works if if y_pred and y_true are greater than 1D\n",
    "    return K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    # calculate mean(abs(y_pred1*y_pred2 - y_true1*ytrue2)) \n",
    "    return K.mean(K.abs(K.prod(y_pred, axis = 1) - K.prod(y_true, axis = 1)))\n",
    "\n",
    "AE = Model(aeInput, aeOutput, name=\"autoencoder\")\n",
    "AE.compile(optimizer='adam', loss=my_loss, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y_true = np.random.rand(100, 2) + 1\n",
    "y_pred = np.random.rand(100, 2) + 1\n",
    "\n",
    "my_loss(y_true, y_pred) # tensor\n",
    "print(tf.Session().run(K.mean(my_loss(y_true, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = [np.array([[4,  5 ],\n",
    "        [2, 6]], dtype=\"float32\"),\n",
    " np.array([ 0, 0], dtype=\"float32\"),\n",
    " np.array([[1, 1],\n",
    "        [1, 1 ]], dtype=\"float32\"),\n",
    " np.array([0, 0  ], dtype=\"float32\")]\n",
    "\n",
    "AE.set_weights(wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AE.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainAE = AE.fit( x=X, y=X, epochs=10, batch_size=2**6)\n",
    "plt.plot(X.A, X.B, \"o\", label = \"actual\")\n",
    "plt.plot(AE.predict(X)[:,0], AE.predict(X)[:,1], \"o\", label = \"preds\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.evaluate(X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y_true = X.astype(\"float32\")\n",
    "y_pred = AE.predict(X).astype(\"float32\")\n",
    "\n",
    "print(tf.Session().run((my_loss(y_true, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = X\n",
    "y_pred = AE.predict(X)\n",
    "\n",
    "my_loss(y_true, y_pred) # tensor\n",
    "print(tf.Session().run(K.mean(my_loss(y_true, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.predict(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = AE.get_weights()\n",
    "wts = [np.array([[-0,  -0 ],\n",
    "        [ -0, -0]], dtype=\"float32\"),\n",
    " np.array([ 0, 0], dtype=\"float32\"),\n",
    " np.array([[ 0,  0 ],\n",
    "        [ 0 , 0 ]], dtype=\"float32\"),\n",
    " np.array([0, 0  ], dtype=\"float32\")]\n",
    "\n",
    "AE.set_weights(wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtsConcatenated = [np.vstack([wts[ii + 1], wts[ii]]) for ii in np.arange(0, len(wts), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.predict(np.array([[-0.9, -0.33]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.predict(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LayerValues = []\n",
    "    \n",
    "inputData= np.array([[1, -0.9, -0.33]])\n",
    "LayerValues.append(inputData)\n",
    "\n",
    "jj = 0\n",
    "\n",
    "wtsConcatenated = [np.vstack([wts[ii + 1], wts[ii]]) for ii in np.arange(0, len(wts), 2)]\n",
    "\n",
    "nextLayer = np.dot(LayerValues[jj], wtsConcatenated[jj]).astype(\"float32\")\n",
    "nextLayer = np.hstack([np.array([1], dtype = \"float32\").reshape(-1,1), nextLayer] )\n",
    "nextLayer = relu(nextLayer) # apply relu\n",
    "print(nextLayer)\n",
    "jj = 1\n",
    "nextLayer = np.dot(np.array([nextLayer]), wtsConcatenated[jj]).astype(\"float32\")\n",
    "relu(nextLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X.A, X.B, \"o\", label = \"actual\")\n",
    "plt.plot(AE.predict(X)[:,0], AE.predict(X)[:,1], \"o\", label = \"preds\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.kdeplot( X.A, X.B, shade=False, axis=ax)\n",
    "sns.kdeplot(AE.predict(X)[:,0], AE.predict(X)[:,1], shade=False, axis=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(AE.predict(X)[:,0], AE.predict(X)[:,1], \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pylab import imshow, show, get_cmap\n",
    "from numpy import random\n",
    "\n",
    "Z = random.random((50,50))   # Test data\n",
    "\n",
    "imshow(Z, cmap=get_cmap(\"Spectral\"), interpolation='nearest')\n",
    "plt.savefig('your_file.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "BLACK_MIN = np.array([0, 20, 20], np.uint8)\n",
    "BLACK_MAX = np.array([120, 255, 255], np.uint8)\n",
    "imgg = cv2.imread('your_file.tif', 1)\n",
    "dst = cv2.inRange(imgg, BLACK_MIN, BLACK_MAX)\n",
    "\n",
    "no_black = cv2.countNonZero(dst)\n",
    "\n",
    "print('The number of black pixels is: ' + str(no_black))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(imgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### More custom losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Losses:\n",
    "    def IoULoss(targets, inputs, smooth=1e-6):\n",
    "        #targets = K.flatten(targets)\n",
    "#         inputs = K.reshape(inputs, [1, -1]) # 1 row, as many columns as needed\n",
    "#         targets = K.reshape(targets, [-1, 1]) # 1 column, as many rows as needed\n",
    "        inputs = K.reshape(inputs, [1, -1]) # 1 row, as many columns as needed\n",
    "        targets = K.reshape(targets, [-1, 1]) # 1 column, as many rows as needed\n",
    "        print(inputs.shape, targets.shape)\n",
    "        print(K.sum(K.dot(targets, inputs)).shape)\n",
    "        \n",
    "        intersection = K.sum(K.dot(targets, inputs))\n",
    "        total = K.sum(targets) + K.sum(inputs)\n",
    "        union = total - intersection\n",
    "\n",
    "        IoU = (intersection + smooth) / (union + smooth)\n",
    "        return 1 - IoU\n",
    "\n",
    "aeInput = Input((2,))\n",
    "encode = Dense(2, activation='relu')(aeInput)\n",
    "aeOutput = Dense(2, activation='relu')(encode)\n",
    "model = Model(aeInput, aeOutput)\n",
    "\n",
    "model.compile(loss=Losses.IoULoss, optimizer=\"adam\",\n",
    "                      metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = pd.DataFrame( (np.random.randn(1000,2)), columns=[\"A\", \"B\"] )\n",
    "XX[\"B\"] = (XX[\"A\"]) *6 + XX[\"B\"]\n",
    "yy = np.random.randn(1000, 2)\n",
    "\n",
    "model.fit(XX, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xx = np.random.randn(100)\n",
    "xx = np.asarray(xx, np.float32)\n",
    "\n",
    "xx_tf = tf.convert_to_tensor(xx, np.float32)\n",
    "xx_tf = K.reshape(xx_tf, [-1, 1])\n",
    "print(xx_tf.shape)\n",
    "\n",
    "yy = np.random.randn(100)\n",
    "yy = np.asarray(yy, np.float32)\n",
    "\n",
    "yy_tf = tf.convert_to_tensor(xx, np.float32)\n",
    "yy_tf = K.reshape(yy_tf, [1, -1])\n",
    "K.dot(xx_tf, yy_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_tf = K.reshape(xx_tf, [1, -1])\n",
    "yy_tf = K.reshape(yy_tf, [-1, 1])\n",
    "\n",
    "\n",
    "print(tf.Session().run(Losses.IoULoss(xx_tf, yy_tf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.random.rand(100), np.random.rand(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = K.random_uniform_variable(shape=(100, 1), low=0, high=1)\n",
    "y = K.random_uniform_variable(shape=(1, 100), low=0, high=1)\n",
    "xy = K.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = K.flatten(y)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X, y = make_blobs(n_samples = 100, n_features = 2, centers = 2, random_state = 123)\n",
    "\n",
    "# fit supervised KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X, y) \n",
    "\n",
    "# create 50 new data points\n",
    "# with the same number of features as the training set\n",
    "new_data = np.random.randn(50, 2)\n",
    "\n",
    "# predict new labels\n",
    "new_labels = knn.predict(new_data)\n",
    "\n",
    "# plot training clusters\n",
    "plt.plot(X[y== 1, 0], \n",
    "         X[y==1,1], \n",
    "         \"C1o\", label = \"training cluster 1\")\n",
    "plt.plot(X[y== 0, 0], \n",
    "         X[y==0,1], \n",
    "         \"C0o\", label = \"training custer 2\")\n",
    "\n",
    "# plot predictions on new data\n",
    "plt.plot(new_data[new_labels== 1, 0], \n",
    "         new_data[new_labels==1,1], \n",
    "         \"ro\", label = \"new data assigned to cluster 1\")\n",
    "plt.plot(new_data[new_labels== 0, 0], \n",
    "         new_data[new_labels==0,1], \n",
    "         \"bo\", label = \"new data assigned to cluster 2\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_X_train = iris_X[indices[:-10]]\n",
    "iris_y_train = iris_y[indices[:-10]]\n",
    "iris_X_test = iris_X[indices[-10:]]\n",
    "iris_y_test = iris_y[indices[-10:]]\n",
    "# Create and fit a nearest-neighbor classifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(iris_X_train, iris_y_train) \n",
    "\n",
    "\n",
    "\n",
    "knn.predict(iris_X_test)\n",
    "\n",
    "iris_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import time\n",
    "\n",
    "players = input(\"Let's play Five's! How many are you?:\" )\n",
    "#print(\"you are\", players, \"players?\") #test number of players\n",
    "players = int(players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your choices are [0, 5, 10, 15, 20]\n",
      "Computer has chosen 10\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Guess: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're Out.\n",
      "Your choices are [0, 5, 10, 15]\n",
      "Computer has chosen 15\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Guess: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong\n",
      "Your choices are [0, 5, 10]\n",
      "Computer has chosen 5\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Guess: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong\n",
      "Your choices are [0, 5]\n",
      "Computer has chosen 5\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Guess: 1\n",
      "Not allowed, choose again: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your choices are [0]\n",
      "Computer has chosen 0\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Guess: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're Out.\n"
     ]
    }
   ],
   "source": [
    "if players <=1:\n",
    "    print(\"Game Over\")\n",
    "\n",
    "else:\n",
    "    while players >= 1:\n",
    "        #players = players+1\n",
    "\n",
    "#Decide possible values than can be chosen\n",
    "        options = [] #Possible options\n",
    "        for i in range(0,players):\n",
    "            x = i * 5\n",
    "            options.append(x)\n",
    "        print(\"Your choices are\", options)\n",
    "\n",
    "#Playing the game\n",
    "#Each turn\n",
    "        guess = random.choice(options)\n",
    "        print(\"Computer has chosen\", int(guess))\n",
    "        count_down = 3\n",
    "        while (count_down):\n",
    "            print(count_down)\n",
    "            time.sleep(1)\n",
    "            count_down -=  1\n",
    "        choice = input(\"Guess:\")\n",
    "        choice = int(choice)\n",
    "        if choice not in options: #If choice isn't a multiple of 5\n",
    "            input(\"Not allowed, choose again:\")\n",
    "        elif choice in options and choice != guess: #Valid choice but wrong\n",
    "                print(\"Wrong\")                      #so player is still in the\n",
    "        else:                                       #game\n",
    "            choice = int(choice)\n",
    "            if choice == guess: #Correct choice so player leaves game\n",
    "                print(\"You're Out.\") # this should reduce the player count\n",
    "        players -=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def next_batch(X,y,batchsize):\n",
    "    for i in np.arange(0,X.shape[0],batchsize):\n",
    "        yield(X[i:i+batchsize],y[i:i+batchsize])\n",
    "\n",
    "def des(X,y,learning_rate,epoches, batchsize):\n",
    "    X=np.c_[np.ones((X.shape[0])),X]\n",
    "    W=np.random.uniform(size=(X.shape[1],))\n",
    "    lossHistory=[]\n",
    "    for epoch in np.arange(0,epoches):\n",
    "        epochLoss=[]\n",
    "        for (batchX,batchY) in next_batch(X,y,batchsize):\n",
    "            #batchY = batchY.reshape(-1)\n",
    "            preds=batchX.dot(W)\n",
    "            print(preds.shape, batchY.shape)\n",
    "            error=preds-batchY\n",
    "            loss=np.sum(error**2)\n",
    "            epochLoss.append(loss)\n",
    "            gradient=batchX.T.dot(error)/batchX.shape[0]\n",
    "            W+=-learning_rate*gradient\n",
    "    lossHistory.append(np.average(epochLoss))\n",
    "    return W,lossHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 14), (120, 1))"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = np.random.randn(150,13)\n",
    "target = np.random.randn(150)\n",
    "\n",
    "train_data_intercept = np.insert(data, 0, 1, axis=1) \n",
    "train_data,test_data,train_target,test_target = train_test_split(train_data_intercept,(target[:, np.newaxis]), test_size=0.2, random_state=42)\n",
    "#train_data,test_data,train_target,test_target = train_test_split(data,target, test_size=0.2, random_state=42)\n",
    "train_data.shape, train_target.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,) (32, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (15,) (15,32) (15,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-260-3fe6d125dbf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-256-915778a83ade>\u001b[0m in \u001b[0;36mdes\u001b[1;34m(X, y, learning_rate, epoches, batchsize)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mepochLoss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mgradient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatchX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatchX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mW\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mlossHistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochLoss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlossHistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (15,) (15,32) (15,) "
     ]
    }
   ],
   "source": [
    "w, loss = des(train_data,train_target,0.01,10,32)\n",
    "w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.324155</td>\n",
       "      <td>0.280023</td>\n",
       "      <td>0.906174</td>\n",
       "      <td>-0.546467</td>\n",
       "      <td>0.450499</td>\n",
       "      <td>1.544092</td>\n",
       "      <td>-1.187309</td>\n",
       "      <td>-1.809241</td>\n",
       "      <td>0.921990</td>\n",
       "      <td>-0.048623</td>\n",
       "      <td>1.400153</td>\n",
       "      <td>0.295839</td>\n",
       "      <td>-0.649771</td>\n",
       "      <td>0.018622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.296955</td>\n",
       "      <td>-0.014474</td>\n",
       "      <td>-0.373151</td>\n",
       "      <td>-0.983438</td>\n",
       "      <td>1.030189</td>\n",
       "      <td>0.302328</td>\n",
       "      <td>-1.094334</td>\n",
       "      <td>0.981874</td>\n",
       "      <td>0.326451</td>\n",
       "      <td>0.370504</td>\n",
       "      <td>-0.752608</td>\n",
       "      <td>1.196286</td>\n",
       "      <td>0.608868</td>\n",
       "      <td>0.964627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.765843</td>\n",
       "      <td>-0.230580</td>\n",
       "      <td>-0.567077</td>\n",
       "      <td>0.141241</td>\n",
       "      <td>-0.407424</td>\n",
       "      <td>1.817883</td>\n",
       "      <td>0.058964</td>\n",
       "      <td>-1.989771</td>\n",
       "      <td>1.158538</td>\n",
       "      <td>-0.676569</td>\n",
       "      <td>-0.647796</td>\n",
       "      <td>-1.570263</td>\n",
       "      <td>-0.385619</td>\n",
       "      <td>-2.602803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.415270</td>\n",
       "      <td>-0.233568</td>\n",
       "      <td>-0.485154</td>\n",
       "      <td>0.100138</td>\n",
       "      <td>0.777029</td>\n",
       "      <td>0.415959</td>\n",
       "      <td>-0.671644</td>\n",
       "      <td>-0.936163</td>\n",
       "      <td>0.598917</td>\n",
       "      <td>-0.674595</td>\n",
       "      <td>0.593295</td>\n",
       "      <td>2.321953</td>\n",
       "      <td>-0.565495</td>\n",
       "      <td>0.145577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.056278</td>\n",
       "      <td>-0.538365</td>\n",
       "      <td>-2.035299</td>\n",
       "      <td>-0.786087</td>\n",
       "      <td>-0.196908</td>\n",
       "      <td>-0.520349</td>\n",
       "      <td>1.173315</td>\n",
       "      <td>0.244964</td>\n",
       "      <td>0.302927</td>\n",
       "      <td>0.170116</td>\n",
       "      <td>0.121828</td>\n",
       "      <td>-0.434741</td>\n",
       "      <td>-0.864817</td>\n",
       "      <td>-0.144488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.943468</td>\n",
       "      <td>-0.262951</td>\n",
       "      <td>-0.741601</td>\n",
       "      <td>0.582080</td>\n",
       "      <td>-0.472952</td>\n",
       "      <td>-0.234918</td>\n",
       "      <td>0.185775</td>\n",
       "      <td>0.460341</td>\n",
       "      <td>-0.298004</td>\n",
       "      <td>-0.089468</td>\n",
       "      <td>-0.796131</td>\n",
       "      <td>-1.114263</td>\n",
       "      <td>0.050091</td>\n",
       "      <td>-0.312687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.939779</td>\n",
       "      <td>-0.987735</td>\n",
       "      <td>-0.251745</td>\n",
       "      <td>0.133066</td>\n",
       "      <td>1.094890</td>\n",
       "      <td>-0.297343</td>\n",
       "      <td>-2.700651</td>\n",
       "      <td>0.585107</td>\n",
       "      <td>0.982556</td>\n",
       "      <td>0.520110</td>\n",
       "      <td>0.314605</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>-1.499442</td>\n",
       "      <td>-0.845574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.129639</td>\n",
       "      <td>-2.024403</td>\n",
       "      <td>0.810383</td>\n",
       "      <td>0.127532</td>\n",
       "      <td>-0.297367</td>\n",
       "      <td>-0.652425</td>\n",
       "      <td>0.504963</td>\n",
       "      <td>1.832211</td>\n",
       "      <td>0.762544</td>\n",
       "      <td>-1.250540</td>\n",
       "      <td>-0.098536</td>\n",
       "      <td>0.431886</td>\n",
       "      <td>0.465799</td>\n",
       "      <td>-2.451167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.215829</td>\n",
       "      <td>-1.654899</td>\n",
       "      <td>-0.837927</td>\n",
       "      <td>2.861428</td>\n",
       "      <td>0.384361</td>\n",
       "      <td>-0.891486</td>\n",
       "      <td>0.568802</td>\n",
       "      <td>-0.438358</td>\n",
       "      <td>1.031554</td>\n",
       "      <td>0.168371</td>\n",
       "      <td>0.361630</td>\n",
       "      <td>1.105988</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>0.480013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.136869</td>\n",
       "      <td>-1.083203</td>\n",
       "      <td>1.357632</td>\n",
       "      <td>-0.348590</td>\n",
       "      <td>0.383872</td>\n",
       "      <td>-0.341981</td>\n",
       "      <td>1.071189</td>\n",
       "      <td>-0.712925</td>\n",
       "      <td>-0.286234</td>\n",
       "      <td>0.613680</td>\n",
       "      <td>0.206219</td>\n",
       "      <td>0.074014</td>\n",
       "      <td>-1.082022</td>\n",
       "      <td>0.449712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.228964</td>\n",
       "      <td>-0.430872</td>\n",
       "      <td>-1.327280</td>\n",
       "      <td>-0.377273</td>\n",
       "      <td>1.389761</td>\n",
       "      <td>-0.265707</td>\n",
       "      <td>-0.246922</td>\n",
       "      <td>-0.330596</td>\n",
       "      <td>-0.257201</td>\n",
       "      <td>0.502421</td>\n",
       "      <td>1.269257</td>\n",
       "      <td>1.143528</td>\n",
       "      <td>-0.435369</td>\n",
       "      <td>0.711527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.695146</td>\n",
       "      <td>0.457604</td>\n",
       "      <td>-0.769346</td>\n",
       "      <td>-0.035712</td>\n",
       "      <td>0.742386</td>\n",
       "      <td>1.183854</td>\n",
       "      <td>0.108753</td>\n",
       "      <td>1.698550</td>\n",
       "      <td>0.096840</td>\n",
       "      <td>0.787736</td>\n",
       "      <td>-0.649492</td>\n",
       "      <td>-0.960009</td>\n",
       "      <td>-0.405656</td>\n",
       "      <td>0.798463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.673463</td>\n",
       "      <td>-1.902440</td>\n",
       "      <td>-0.240004</td>\n",
       "      <td>2.327057</td>\n",
       "      <td>1.017179</td>\n",
       "      <td>0.351166</td>\n",
       "      <td>0.430511</td>\n",
       "      <td>1.312027</td>\n",
       "      <td>1.153205</td>\n",
       "      <td>-0.048670</td>\n",
       "      <td>-0.281967</td>\n",
       "      <td>0.628821</td>\n",
       "      <td>0.073823</td>\n",
       "      <td>0.143403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.722166</td>\n",
       "      <td>0.021358</td>\n",
       "      <td>1.621775</td>\n",
       "      <td>-0.098504</td>\n",
       "      <td>-0.920280</td>\n",
       "      <td>1.593947</td>\n",
       "      <td>-0.308180</td>\n",
       "      <td>-1.133219</td>\n",
       "      <td>-0.669685</td>\n",
       "      <td>0.995385</td>\n",
       "      <td>-0.709320</td>\n",
       "      <td>1.033608</td>\n",
       "      <td>0.178217</td>\n",
       "      <td>-1.897041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.757093</td>\n",
       "      <td>-1.539339</td>\n",
       "      <td>-0.013525</td>\n",
       "      <td>-0.352009</td>\n",
       "      <td>-0.388951</td>\n",
       "      <td>0.801254</td>\n",
       "      <td>0.037179</td>\n",
       "      <td>0.030929</td>\n",
       "      <td>-2.609712</td>\n",
       "      <td>1.223902</td>\n",
       "      <td>-0.715013</td>\n",
       "      <td>-1.418049</td>\n",
       "      <td>0.882566</td>\n",
       "      <td>0.255738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.628678</td>\n",
       "      <td>-2.632518</td>\n",
       "      <td>1.082110</td>\n",
       "      <td>1.458691</td>\n",
       "      <td>0.686207</td>\n",
       "      <td>0.054818</td>\n",
       "      <td>-2.489097</td>\n",
       "      <td>-0.555055</td>\n",
       "      <td>2.109427</td>\n",
       "      <td>1.538382</td>\n",
       "      <td>1.652889</td>\n",
       "      <td>-0.838494</td>\n",
       "      <td>-1.643691</td>\n",
       "      <td>-1.101963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.057391</td>\n",
       "      <td>-0.654887</td>\n",
       "      <td>0.317156</td>\n",
       "      <td>-0.452213</td>\n",
       "      <td>-0.178781</td>\n",
       "      <td>1.489966</td>\n",
       "      <td>-0.517980</td>\n",
       "      <td>-1.838721</td>\n",
       "      <td>1.322395</td>\n",
       "      <td>1.060383</td>\n",
       "      <td>-0.084937</td>\n",
       "      <td>-0.858862</td>\n",
       "      <td>0.231761</td>\n",
       "      <td>0.675018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.423524</td>\n",
       "      <td>-0.176870</td>\n",
       "      <td>0.318705</td>\n",
       "      <td>0.217995</td>\n",
       "      <td>1.029183</td>\n",
       "      <td>0.429475</td>\n",
       "      <td>0.965592</td>\n",
       "      <td>0.191336</td>\n",
       "      <td>0.499996</td>\n",
       "      <td>0.118169</td>\n",
       "      <td>0.131777</td>\n",
       "      <td>-1.090417</td>\n",
       "      <td>-0.921065</td>\n",
       "      <td>0.914476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.226127</td>\n",
       "      <td>0.813840</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>-0.462188</td>\n",
       "      <td>0.492496</td>\n",
       "      <td>-0.940211</td>\n",
       "      <td>2.060731</td>\n",
       "      <td>0.120592</td>\n",
       "      <td>-0.252443</td>\n",
       "      <td>-0.670390</td>\n",
       "      <td>0.224753</td>\n",
       "      <td>-0.015617</td>\n",
       "      <td>0.105302</td>\n",
       "      <td>-0.876590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.911455</td>\n",
       "      <td>0.199485</td>\n",
       "      <td>-0.638089</td>\n",
       "      <td>-0.416854</td>\n",
       "      <td>2.351695</td>\n",
       "      <td>0.430302</td>\n",
       "      <td>-0.370356</td>\n",
       "      <td>-0.617115</td>\n",
       "      <td>2.006875</td>\n",
       "      <td>0.078716</td>\n",
       "      <td>-0.220873</td>\n",
       "      <td>0.944354</td>\n",
       "      <td>-0.862587</td>\n",
       "      <td>-1.201874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.290633</td>\n",
       "      <td>-0.433765</td>\n",
       "      <td>-0.443114</td>\n",
       "      <td>0.432201</td>\n",
       "      <td>0.221660</td>\n",
       "      <td>0.040998</td>\n",
       "      <td>-0.145046</td>\n",
       "      <td>0.358298</td>\n",
       "      <td>0.991688</td>\n",
       "      <td>0.998200</td>\n",
       "      <td>-1.618268</td>\n",
       "      <td>0.481274</td>\n",
       "      <td>-1.426305</td>\n",
       "      <td>2.043953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.312291</td>\n",
       "      <td>-0.735255</td>\n",
       "      <td>-0.366935</td>\n",
       "      <td>0.451333</td>\n",
       "      <td>-0.023566</td>\n",
       "      <td>-1.705366</td>\n",
       "      <td>-0.872043</td>\n",
       "      <td>-0.054721</td>\n",
       "      <td>-1.663138</td>\n",
       "      <td>-0.530753</td>\n",
       "      <td>-1.629585</td>\n",
       "      <td>-0.295695</td>\n",
       "      <td>0.862091</td>\n",
       "      <td>1.189902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.104877</td>\n",
       "      <td>1.272477</td>\n",
       "      <td>-0.808313</td>\n",
       "      <td>0.934850</td>\n",
       "      <td>-0.294110</td>\n",
       "      <td>-0.907465</td>\n",
       "      <td>1.420540</td>\n",
       "      <td>0.549287</td>\n",
       "      <td>-0.155727</td>\n",
       "      <td>-0.228040</td>\n",
       "      <td>-0.128765</td>\n",
       "      <td>0.021928</td>\n",
       "      <td>-0.704515</td>\n",
       "      <td>0.109991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.440780</td>\n",
       "      <td>-2.958484</td>\n",
       "      <td>-0.038005</td>\n",
       "      <td>-0.155518</td>\n",
       "      <td>-2.008415</td>\n",
       "      <td>1.928285</td>\n",
       "      <td>0.822374</td>\n",
       "      <td>-0.511711</td>\n",
       "      <td>-0.488496</td>\n",
       "      <td>1.044512</td>\n",
       "      <td>0.989719</td>\n",
       "      <td>-0.124980</td>\n",
       "      <td>-0.966005</td>\n",
       "      <td>-0.285723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.073053</td>\n",
       "      <td>1.296603</td>\n",
       "      <td>1.389923</td>\n",
       "      <td>-0.504007</td>\n",
       "      <td>1.258409</td>\n",
       "      <td>0.214323</td>\n",
       "      <td>-1.921479</td>\n",
       "      <td>-1.389260</td>\n",
       "      <td>0.298724</td>\n",
       "      <td>-0.958645</td>\n",
       "      <td>1.123535</td>\n",
       "      <td>1.784467</td>\n",
       "      <td>0.428863</td>\n",
       "      <td>-1.133294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.590173</td>\n",
       "      <td>-0.048059</td>\n",
       "      <td>0.878789</td>\n",
       "      <td>-0.290391</td>\n",
       "      <td>0.171735</td>\n",
       "      <td>-1.404475</td>\n",
       "      <td>-2.391928</td>\n",
       "      <td>0.418166</td>\n",
       "      <td>-0.918952</td>\n",
       "      <td>-0.533108</td>\n",
       "      <td>0.265932</td>\n",
       "      <td>-0.412331</td>\n",
       "      <td>-0.317886</td>\n",
       "      <td>-0.848116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.399167</td>\n",
       "      <td>0.440825</td>\n",
       "      <td>-1.283827</td>\n",
       "      <td>2.227096</td>\n",
       "      <td>-1.177989</td>\n",
       "      <td>-1.156166</td>\n",
       "      <td>1.639631</td>\n",
       "      <td>0.632697</td>\n",
       "      <td>-0.216161</td>\n",
       "      <td>0.669059</td>\n",
       "      <td>-0.438771</td>\n",
       "      <td>-0.507973</td>\n",
       "      <td>0.306449</td>\n",
       "      <td>-0.184829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.339862</td>\n",
       "      <td>-1.439921</td>\n",
       "      <td>0.217743</td>\n",
       "      <td>0.732350</td>\n",
       "      <td>0.989012</td>\n",
       "      <td>-0.622258</td>\n",
       "      <td>-0.484682</td>\n",
       "      <td>0.608675</td>\n",
       "      <td>-0.490622</td>\n",
       "      <td>-1.060110</td>\n",
       "      <td>-0.184689</td>\n",
       "      <td>-1.656397</td>\n",
       "      <td>-0.055470</td>\n",
       "      <td>-1.360479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.658979</td>\n",
       "      <td>-0.270936</td>\n",
       "      <td>-0.033896</td>\n",
       "      <td>0.496706</td>\n",
       "      <td>-0.278124</td>\n",
       "      <td>0.762373</td>\n",
       "      <td>-1.579996</td>\n",
       "      <td>0.876143</td>\n",
       "      <td>1.411948</td>\n",
       "      <td>0.770937</td>\n",
       "      <td>-0.842644</td>\n",
       "      <td>-0.121870</td>\n",
       "      <td>0.738804</td>\n",
       "      <td>-0.520021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.680889</td>\n",
       "      <td>-2.080416</td>\n",
       "      <td>0.582247</td>\n",
       "      <td>0.224400</td>\n",
       "      <td>-0.407728</td>\n",
       "      <td>0.644810</td>\n",
       "      <td>2.126441</td>\n",
       "      <td>1.596192</td>\n",
       "      <td>1.302601</td>\n",
       "      <td>-0.625267</td>\n",
       "      <td>0.728203</td>\n",
       "      <td>0.181350</td>\n",
       "      <td>0.539067</td>\n",
       "      <td>-1.590740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.013833</td>\n",
       "      <td>-0.324368</td>\n",
       "      <td>-1.163318</td>\n",
       "      <td>-0.827445</td>\n",
       "      <td>1.198508</td>\n",
       "      <td>0.916244</td>\n",
       "      <td>-0.793833</td>\n",
       "      <td>1.762520</td>\n",
       "      <td>0.912498</td>\n",
       "      <td>1.182887</td>\n",
       "      <td>-0.581198</td>\n",
       "      <td>-0.121828</td>\n",
       "      <td>-0.891564</td>\n",
       "      <td>0.650155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.395118</td>\n",
       "      <td>-0.404135</td>\n",
       "      <td>0.796659</td>\n",
       "      <td>-1.231262</td>\n",
       "      <td>-0.455600</td>\n",
       "      <td>-0.847728</td>\n",
       "      <td>0.106599</td>\n",
       "      <td>-0.966910</td>\n",
       "      <td>-0.576079</td>\n",
       "      <td>-1.678606</td>\n",
       "      <td>-0.366704</td>\n",
       "      <td>-0.755679</td>\n",
       "      <td>-2.214221</td>\n",
       "      <td>1.023971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.037205</td>\n",
       "      <td>-2.196969</td>\n",
       "      <td>-0.052552</td>\n",
       "      <td>0.242629</td>\n",
       "      <td>0.827549</td>\n",
       "      <td>0.490336</td>\n",
       "      <td>0.088045</td>\n",
       "      <td>-1.556536</td>\n",
       "      <td>-1.535694</td>\n",
       "      <td>0.786301</td>\n",
       "      <td>-0.114562</td>\n",
       "      <td>1.583300</td>\n",
       "      <td>-0.090033</td>\n",
       "      <td>1.568755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962337</td>\n",
       "      <td>-1.483698</td>\n",
       "      <td>0.473625</td>\n",
       "      <td>-1.269572</td>\n",
       "      <td>0.718894</td>\n",
       "      <td>-1.375272</td>\n",
       "      <td>0.308170</td>\n",
       "      <td>-2.695540</td>\n",
       "      <td>0.240131</td>\n",
       "      <td>0.908059</td>\n",
       "      <td>-0.443687</td>\n",
       "      <td>0.743719</td>\n",
       "      <td>0.436824</td>\n",
       "      <td>-0.018611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.133296</td>\n",
       "      <td>-1.313003</td>\n",
       "      <td>-0.344768</td>\n",
       "      <td>0.835567</td>\n",
       "      <td>1.303471</td>\n",
       "      <td>-1.383373</td>\n",
       "      <td>-1.023127</td>\n",
       "      <td>1.049151</td>\n",
       "      <td>-0.617696</td>\n",
       "      <td>-1.974539</td>\n",
       "      <td>0.847049</td>\n",
       "      <td>0.580214</td>\n",
       "      <td>-0.783629</td>\n",
       "      <td>-1.576371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.781138</td>\n",
       "      <td>0.307556</td>\n",
       "      <td>-0.663133</td>\n",
       "      <td>-0.263828</td>\n",
       "      <td>0.786838</td>\n",
       "      <td>-0.133194</td>\n",
       "      <td>-1.838633</td>\n",
       "      <td>-0.079153</td>\n",
       "      <td>-0.677238</td>\n",
       "      <td>-1.100844</td>\n",
       "      <td>-1.377004</td>\n",
       "      <td>-0.521673</td>\n",
       "      <td>-0.213441</td>\n",
       "      <td>0.097560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.284012</td>\n",
       "      <td>-0.185986</td>\n",
       "      <td>0.460376</td>\n",
       "      <td>-0.155886</td>\n",
       "      <td>0.501357</td>\n",
       "      <td>0.615855</td>\n",
       "      <td>-0.496964</td>\n",
       "      <td>-1.228920</td>\n",
       "      <td>-0.043244</td>\n",
       "      <td>0.254221</td>\n",
       "      <td>-0.082780</td>\n",
       "      <td>1.184575</td>\n",
       "      <td>0.475846</td>\n",
       "      <td>-0.593911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.494802</td>\n",
       "      <td>0.172424</td>\n",
       "      <td>0.445790</td>\n",
       "      <td>1.080547</td>\n",
       "      <td>0.966801</td>\n",
       "      <td>2.392395</td>\n",
       "      <td>-1.806336</td>\n",
       "      <td>1.643205</td>\n",
       "      <td>-1.633132</td>\n",
       "      <td>0.496792</td>\n",
       "      <td>-0.742969</td>\n",
       "      <td>-0.489626</td>\n",
       "      <td>0.027503</td>\n",
       "      <td>0.600103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.452051</td>\n",
       "      <td>-0.904870</td>\n",
       "      <td>0.772610</td>\n",
       "      <td>0.927047</td>\n",
       "      <td>-0.058490</td>\n",
       "      <td>1.123663</td>\n",
       "      <td>-0.610201</td>\n",
       "      <td>1.766335</td>\n",
       "      <td>0.915223</td>\n",
       "      <td>-2.323803</td>\n",
       "      <td>1.551051</td>\n",
       "      <td>1.316492</td>\n",
       "      <td>-1.429129</td>\n",
       "      <td>-1.152140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.340752</td>\n",
       "      <td>0.826739</td>\n",
       "      <td>0.147502</td>\n",
       "      <td>0.228408</td>\n",
       "      <td>-1.268531</td>\n",
       "      <td>-1.339232</td>\n",
       "      <td>1.032885</td>\n",
       "      <td>0.064164</td>\n",
       "      <td>-0.769165</td>\n",
       "      <td>0.078407</td>\n",
       "      <td>-0.909857</td>\n",
       "      <td>-0.635943</td>\n",
       "      <td>0.573373</td>\n",
       "      <td>-0.222647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.156193</td>\n",
       "      <td>-1.333003</td>\n",
       "      <td>0.138932</td>\n",
       "      <td>-0.533458</td>\n",
       "      <td>-0.892250</td>\n",
       "      <td>0.372546</td>\n",
       "      <td>-0.433719</td>\n",
       "      <td>1.087625</td>\n",
       "      <td>-0.359697</td>\n",
       "      <td>-0.121089</td>\n",
       "      <td>-1.684378</td>\n",
       "      <td>-0.296507</td>\n",
       "      <td>-0.035591</td>\n",
       "      <td>0.845945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.566892</td>\n",
       "      <td>-0.843184</td>\n",
       "      <td>0.967841</td>\n",
       "      <td>-0.865240</td>\n",
       "      <td>0.541239</td>\n",
       "      <td>0.316399</td>\n",
       "      <td>1.567672</td>\n",
       "      <td>0.185875</td>\n",
       "      <td>-0.811463</td>\n",
       "      <td>1.102997</td>\n",
       "      <td>-0.229851</td>\n",
       "      <td>-0.266047</td>\n",
       "      <td>2.367364</td>\n",
       "      <td>-0.610867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.890357</td>\n",
       "      <td>0.478736</td>\n",
       "      <td>-0.661198</td>\n",
       "      <td>-1.149995</td>\n",
       "      <td>-1.081964</td>\n",
       "      <td>-1.286690</td>\n",
       "      <td>-0.978894</td>\n",
       "      <td>0.691046</td>\n",
       "      <td>0.271246</td>\n",
       "      <td>0.844581</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>0.808210</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.882274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.234831</td>\n",
       "      <td>1.344638</td>\n",
       "      <td>0.543106</td>\n",
       "      <td>-1.535960</td>\n",
       "      <td>0.747636</td>\n",
       "      <td>-0.632584</td>\n",
       "      <td>-0.266455</td>\n",
       "      <td>0.463741</td>\n",
       "      <td>1.341787</td>\n",
       "      <td>-1.005388</td>\n",
       "      <td>-0.526252</td>\n",
       "      <td>-0.099187</td>\n",
       "      <td>0.944251</td>\n",
       "      <td>0.476258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.399513</td>\n",
       "      <td>-0.500048</td>\n",
       "      <td>-0.092196</td>\n",
       "      <td>-0.216849</td>\n",
       "      <td>1.167259</td>\n",
       "      <td>-0.098799</td>\n",
       "      <td>-1.439551</td>\n",
       "      <td>1.233285</td>\n",
       "      <td>2.879099</td>\n",
       "      <td>-0.476272</td>\n",
       "      <td>0.422538</td>\n",
       "      <td>-0.259557</td>\n",
       "      <td>-0.554024</td>\n",
       "      <td>-0.359096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.379593</td>\n",
       "      <td>1.374420</td>\n",
       "      <td>0.137976</td>\n",
       "      <td>-0.705007</td>\n",
       "      <td>0.311377</td>\n",
       "      <td>-0.933001</td>\n",
       "      <td>-2.452414</td>\n",
       "      <td>-2.291273</td>\n",
       "      <td>-1.149213</td>\n",
       "      <td>-1.033316</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.556236</td>\n",
       "      <td>0.145493</td>\n",
       "      <td>-1.853645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.982943</td>\n",
       "      <td>-0.132700</td>\n",
       "      <td>0.669776</td>\n",
       "      <td>-0.535850</td>\n",
       "      <td>0.458167</td>\n",
       "      <td>-0.046628</td>\n",
       "      <td>-0.727509</td>\n",
       "      <td>1.598275</td>\n",
       "      <td>0.174711</td>\n",
       "      <td>-0.127460</td>\n",
       "      <td>-1.648830</td>\n",
       "      <td>-0.906735</td>\n",
       "      <td>1.318613</td>\n",
       "      <td>0.039692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.324451</td>\n",
       "      <td>-0.966093</td>\n",
       "      <td>1.537763</td>\n",
       "      <td>0.768892</td>\n",
       "      <td>0.834857</td>\n",
       "      <td>-0.245928</td>\n",
       "      <td>-0.630132</td>\n",
       "      <td>0.337157</td>\n",
       "      <td>-1.620996</td>\n",
       "      <td>-1.340387</td>\n",
       "      <td>0.350453</td>\n",
       "      <td>-1.799553</td>\n",
       "      <td>-1.164455</td>\n",
       "      <td>0.596010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.257375</td>\n",
       "      <td>0.346621</td>\n",
       "      <td>-0.483461</td>\n",
       "      <td>-1.039528</td>\n",
       "      <td>0.093341</td>\n",
       "      <td>-1.874148</td>\n",
       "      <td>-0.580189</td>\n",
       "      <td>-0.102385</td>\n",
       "      <td>0.115753</td>\n",
       "      <td>-0.978703</td>\n",
       "      <td>-0.282042</td>\n",
       "      <td>-0.068302</td>\n",
       "      <td>-0.817777</td>\n",
       "      <td>1.165414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.301735</td>\n",
       "      <td>-0.266397</td>\n",
       "      <td>-0.227812</td>\n",
       "      <td>0.572541</td>\n",
       "      <td>0.942559</td>\n",
       "      <td>-0.445910</td>\n",
       "      <td>0.765039</td>\n",
       "      <td>-0.110520</td>\n",
       "      <td>0.921936</td>\n",
       "      <td>-1.180993</td>\n",
       "      <td>0.909055</td>\n",
       "      <td>-0.339337</td>\n",
       "      <td>-0.969875</td>\n",
       "      <td>0.285163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016591</td>\n",
       "      <td>-0.155176</td>\n",
       "      <td>1.458151</td>\n",
       "      <td>-0.781127</td>\n",
       "      <td>-0.752588</td>\n",
       "      <td>2.092981</td>\n",
       "      <td>0.231840</td>\n",
       "      <td>0.836649</td>\n",
       "      <td>-3.242005</td>\n",
       "      <td>2.554540</td>\n",
       "      <td>0.251309</td>\n",
       "      <td>0.945601</td>\n",
       "      <td>-0.239059</td>\n",
       "      <td>-1.729303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.236326</td>\n",
       "      <td>0.238113</td>\n",
       "      <td>-0.816594</td>\n",
       "      <td>-0.543328</td>\n",
       "      <td>-1.189269</td>\n",
       "      <td>-0.291341</td>\n",
       "      <td>1.123865</td>\n",
       "      <td>-2.151016</td>\n",
       "      <td>-0.896312</td>\n",
       "      <td>-2.013886</td>\n",
       "      <td>-0.458833</td>\n",
       "      <td>1.053839</td>\n",
       "      <td>-0.003071</td>\n",
       "      <td>0.233951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.898821</td>\n",
       "      <td>-0.515543</td>\n",
       "      <td>0.786032</td>\n",
       "      <td>0.249295</td>\n",
       "      <td>0.557928</td>\n",
       "      <td>0.183303</td>\n",
       "      <td>1.370312</td>\n",
       "      <td>2.106222</td>\n",
       "      <td>0.201604</td>\n",
       "      <td>1.026928</td>\n",
       "      <td>0.369033</td>\n",
       "      <td>-1.596023</td>\n",
       "      <td>0.327179</td>\n",
       "      <td>-0.631026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.638414</td>\n",
       "      <td>0.990666</td>\n",
       "      <td>1.031411</td>\n",
       "      <td>2.031722</td>\n",
       "      <td>0.910487</td>\n",
       "      <td>0.038755</td>\n",
       "      <td>-1.664829</td>\n",
       "      <td>-1.154991</td>\n",
       "      <td>0.876789</td>\n",
       "      <td>0.437520</td>\n",
       "      <td>-1.914616</td>\n",
       "      <td>1.265751</td>\n",
       "      <td>0.650888</td>\n",
       "      <td>-1.572120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>1.992691</td>\n",
       "      <td>-0.304611</td>\n",
       "      <td>1.722787</td>\n",
       "      <td>1.086006</td>\n",
       "      <td>0.842089</td>\n",
       "      <td>1.008068</td>\n",
       "      <td>1.303940</td>\n",
       "      <td>0.015752</td>\n",
       "      <td>0.759068</td>\n",
       "      <td>0.688316</td>\n",
       "      <td>0.249673</td>\n",
       "      <td>-0.436027</td>\n",
       "      <td>-1.259726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.354690</td>\n",
       "      <td>0.725696</td>\n",
       "      <td>0.243493</td>\n",
       "      <td>0.829609</td>\n",
       "      <td>0.815333</td>\n",
       "      <td>0.619227</td>\n",
       "      <td>0.269490</td>\n",
       "      <td>0.793482</td>\n",
       "      <td>-0.862932</td>\n",
       "      <td>-1.850052</td>\n",
       "      <td>0.314250</td>\n",
       "      <td>0.010627</td>\n",
       "      <td>-1.400407</td>\n",
       "      <td>-1.662923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.680959</td>\n",
       "      <td>1.593497</td>\n",
       "      <td>-0.945763</td>\n",
       "      <td>-0.103994</td>\n",
       "      <td>1.319037</td>\n",
       "      <td>1.877450</td>\n",
       "      <td>0.049087</td>\n",
       "      <td>-0.555811</td>\n",
       "      <td>0.443928</td>\n",
       "      <td>-0.320651</td>\n",
       "      <td>-0.350842</td>\n",
       "      <td>0.309964</td>\n",
       "      <td>0.335206</td>\n",
       "      <td>-0.798133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.095581</td>\n",
       "      <td>-0.545238</td>\n",
       "      <td>-0.792189</td>\n",
       "      <td>-1.397642</td>\n",
       "      <td>0.267877</td>\n",
       "      <td>1.850542</td>\n",
       "      <td>-2.150406</td>\n",
       "      <td>1.238607</td>\n",
       "      <td>0.153936</td>\n",
       "      <td>-0.783455</td>\n",
       "      <td>2.296221</td>\n",
       "      <td>0.868531</td>\n",
       "      <td>-0.011128</td>\n",
       "      <td>1.783243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.314175</td>\n",
       "      <td>0.753043</td>\n",
       "      <td>-0.341610</td>\n",
       "      <td>-0.459202</td>\n",
       "      <td>-0.405305</td>\n",
       "      <td>1.118053</td>\n",
       "      <td>0.439409</td>\n",
       "      <td>-0.114551</td>\n",
       "      <td>0.395257</td>\n",
       "      <td>-0.685737</td>\n",
       "      <td>1.974084</td>\n",
       "      <td>0.619691</td>\n",
       "      <td>1.859962</td>\n",
       "      <td>1.896146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.065394</td>\n",
       "      <td>0.958108</td>\n",
       "      <td>-0.939434</td>\n",
       "      <td>-0.047829</td>\n",
       "      <td>1.746429</td>\n",
       "      <td>-0.559338</td>\n",
       "      <td>1.032144</td>\n",
       "      <td>-0.682549</td>\n",
       "      <td>0.575425</td>\n",
       "      <td>0.562728</td>\n",
       "      <td>-0.599750</td>\n",
       "      <td>0.751304</td>\n",
       "      <td>-0.318354</td>\n",
       "      <td>0.804555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6   \\\n",
       "0    1.0 -1.324155  0.280023  0.906174 -0.546467  0.450499  1.544092   \n",
       "1    1.0  0.296955 -0.014474 -0.373151 -0.983438  1.030189  0.302328   \n",
       "2    1.0  0.765843 -0.230580 -0.567077  0.141241 -0.407424  1.817883   \n",
       "3    1.0  1.415270 -0.233568 -0.485154  0.100138  0.777029  0.415959   \n",
       "4    1.0  0.056278 -0.538365 -2.035299 -0.786087 -0.196908 -0.520349   \n",
       "5    1.0  0.943468 -0.262951 -0.741601  0.582080 -0.472952 -0.234918   \n",
       "6    1.0  0.939779 -0.987735 -0.251745  0.133066  1.094890 -0.297343   \n",
       "7    1.0 -1.129639 -2.024403  0.810383  0.127532 -0.297367 -0.652425   \n",
       "8    1.0 -0.215829 -1.654899 -0.837927  2.861428  0.384361 -0.891486   \n",
       "9    1.0  2.136869 -1.083203  1.357632 -0.348590  0.383872 -0.341981   \n",
       "10   1.0  0.228964 -0.430872 -1.327280 -0.377273  1.389761 -0.265707   \n",
       "11   1.0 -0.695146  0.457604 -0.769346 -0.035712  0.742386  1.183854   \n",
       "12   1.0  0.673463 -1.902440 -0.240004  2.327057  1.017179  0.351166   \n",
       "13   1.0  0.722166  0.021358  1.621775 -0.098504 -0.920280  1.593947   \n",
       "14   1.0 -0.757093 -1.539339 -0.013525 -0.352009 -0.388951  0.801254   \n",
       "15   1.0  0.628678 -2.632518  1.082110  1.458691  0.686207  0.054818   \n",
       "16   1.0  1.057391 -0.654887  0.317156 -0.452213 -0.178781  1.489966   \n",
       "17   1.0  0.423524 -0.176870  0.318705  0.217995  1.029183  0.429475   \n",
       "18   1.0  0.226127  0.813840  0.004886 -0.462188  0.492496 -0.940211   \n",
       "19   1.0 -0.911455  0.199485 -0.638089 -0.416854  2.351695  0.430302   \n",
       "20   1.0 -0.290633 -0.433765 -0.443114  0.432201  0.221660  0.040998   \n",
       "21   1.0  0.312291 -0.735255 -0.366935  0.451333 -0.023566 -1.705366   \n",
       "22   1.0 -0.104877  1.272477 -0.808313  0.934850 -0.294110 -0.907465   \n",
       "23   1.0 -0.440780 -2.958484 -0.038005 -0.155518 -2.008415  1.928285   \n",
       "24   1.0 -0.073053  1.296603  1.389923 -0.504007  1.258409  0.214323   \n",
       "25   1.0  0.590173 -0.048059  0.878789 -0.290391  0.171735 -1.404475   \n",
       "26   1.0 -0.399167  0.440825 -1.283827  2.227096 -1.177989 -1.156166   \n",
       "27   1.0 -0.339862 -1.439921  0.217743  0.732350  0.989012 -0.622258   \n",
       "28   1.0  1.658979 -0.270936 -0.033896  0.496706 -0.278124  0.762373   \n",
       "29   1.0 -0.680889 -2.080416  0.582247  0.224400 -0.407728  0.644810   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "90   1.0 -0.013833 -0.324368 -1.163318 -0.827445  1.198508  0.916244   \n",
       "91   1.0  0.395118 -0.404135  0.796659 -1.231262 -0.455600 -0.847728   \n",
       "92   1.0 -1.037205 -2.196969 -0.052552  0.242629  0.827549  0.490336   \n",
       "93   1.0  0.962337 -1.483698  0.473625 -1.269572  0.718894 -1.375272   \n",
       "94   1.0 -0.133296 -1.313003 -0.344768  0.835567  1.303471 -1.383373   \n",
       "95   1.0 -0.781138  0.307556 -0.663133 -0.263828  0.786838 -0.133194   \n",
       "96   1.0 -0.284012 -0.185986  0.460376 -0.155886  0.501357  0.615855   \n",
       "97   1.0  0.494802  0.172424  0.445790  1.080547  0.966801  2.392395   \n",
       "98   1.0  1.452051 -0.904870  0.772610  0.927047 -0.058490  1.123663   \n",
       "99   1.0  0.340752  0.826739  0.147502  0.228408 -1.268531 -1.339232   \n",
       "100  1.0 -1.156193 -1.333003  0.138932 -0.533458 -0.892250  0.372546   \n",
       "101  1.0  2.566892 -0.843184  0.967841 -0.865240  0.541239  0.316399   \n",
       "102  1.0 -0.890357  0.478736 -0.661198 -1.149995 -1.081964 -1.286690   \n",
       "103  1.0  0.234831  1.344638  0.543106 -1.535960  0.747636 -0.632584   \n",
       "104  1.0  0.399513 -0.500048 -0.092196 -0.216849  1.167259 -0.098799   \n",
       "105  1.0 -0.379593  1.374420  0.137976 -0.705007  0.311377 -0.933001   \n",
       "106  1.0 -0.982943 -0.132700  0.669776 -0.535850  0.458167 -0.046628   \n",
       "107  1.0  1.324451 -0.966093  1.537763  0.768892  0.834857 -0.245928   \n",
       "108  1.0 -2.257375  0.346621 -0.483461 -1.039528  0.093341 -1.874148   \n",
       "109  1.0  0.301735 -0.266397 -0.227812  0.572541  0.942559 -0.445910   \n",
       "110  1.0  0.016591 -0.155176  1.458151 -0.781127 -0.752588  2.092981   \n",
       "111  1.0  0.236326  0.238113 -0.816594 -0.543328 -1.189269 -0.291341   \n",
       "112  1.0 -0.898821 -0.515543  0.786032  0.249295  0.557928  0.183303   \n",
       "113  1.0  0.638414  0.990666  1.031411  2.031722  0.910487  0.038755   \n",
       "114  1.0  0.027211  1.992691 -0.304611  1.722787  1.086006  0.842089   \n",
       "115  1.0  0.354690  0.725696  0.243493  0.829609  0.815333  0.619227   \n",
       "116  1.0 -1.680959  1.593497 -0.945763 -0.103994  1.319037  1.877450   \n",
       "117  1.0  1.095581 -0.545238 -0.792189 -1.397642  0.267877  1.850542   \n",
       "118  1.0 -0.314175  0.753043 -0.341610 -0.459202 -0.405305  1.118053   \n",
       "119  1.0 -1.065394  0.958108 -0.939434 -0.047829  1.746429 -0.559338   \n",
       "\n",
       "           7         8         9         10        11        12        13  \\\n",
       "0   -1.187309 -1.809241  0.921990 -0.048623  1.400153  0.295839 -0.649771   \n",
       "1   -1.094334  0.981874  0.326451  0.370504 -0.752608  1.196286  0.608868   \n",
       "2    0.058964 -1.989771  1.158538 -0.676569 -0.647796 -1.570263 -0.385619   \n",
       "3   -0.671644 -0.936163  0.598917 -0.674595  0.593295  2.321953 -0.565495   \n",
       "4    1.173315  0.244964  0.302927  0.170116  0.121828 -0.434741 -0.864817   \n",
       "5    0.185775  0.460341 -0.298004 -0.089468 -0.796131 -1.114263  0.050091   \n",
       "6   -2.700651  0.585107  0.982556  0.520110  0.314605  0.003622 -1.499442   \n",
       "7    0.504963  1.832211  0.762544 -1.250540 -0.098536  0.431886  0.465799   \n",
       "8    0.568802 -0.438358  1.031554  0.168371  0.361630  1.105988  0.555555   \n",
       "9    1.071189 -0.712925 -0.286234  0.613680  0.206219  0.074014 -1.082022   \n",
       "10  -0.246922 -0.330596 -0.257201  0.502421  1.269257  1.143528 -0.435369   \n",
       "11   0.108753  1.698550  0.096840  0.787736 -0.649492 -0.960009 -0.405656   \n",
       "12   0.430511  1.312027  1.153205 -0.048670 -0.281967  0.628821  0.073823   \n",
       "13  -0.308180 -1.133219 -0.669685  0.995385 -0.709320  1.033608  0.178217   \n",
       "14   0.037179  0.030929 -2.609712  1.223902 -0.715013 -1.418049  0.882566   \n",
       "15  -2.489097 -0.555055  2.109427  1.538382  1.652889 -0.838494 -1.643691   \n",
       "16  -0.517980 -1.838721  1.322395  1.060383 -0.084937 -0.858862  0.231761   \n",
       "17   0.965592  0.191336  0.499996  0.118169  0.131777 -1.090417 -0.921065   \n",
       "18   2.060731  0.120592 -0.252443 -0.670390  0.224753 -0.015617  0.105302   \n",
       "19  -0.370356 -0.617115  2.006875  0.078716 -0.220873  0.944354 -0.862587   \n",
       "20  -0.145046  0.358298  0.991688  0.998200 -1.618268  0.481274 -1.426305   \n",
       "21  -0.872043 -0.054721 -1.663138 -0.530753 -1.629585 -0.295695  0.862091   \n",
       "22   1.420540  0.549287 -0.155727 -0.228040 -0.128765  0.021928 -0.704515   \n",
       "23   0.822374 -0.511711 -0.488496  1.044512  0.989719 -0.124980 -0.966005   \n",
       "24  -1.921479 -1.389260  0.298724 -0.958645  1.123535  1.784467  0.428863   \n",
       "25  -2.391928  0.418166 -0.918952 -0.533108  0.265932 -0.412331 -0.317886   \n",
       "26   1.639631  0.632697 -0.216161  0.669059 -0.438771 -0.507973  0.306449   \n",
       "27  -0.484682  0.608675 -0.490622 -1.060110 -0.184689 -1.656397 -0.055470   \n",
       "28  -1.579996  0.876143  1.411948  0.770937 -0.842644 -0.121870  0.738804   \n",
       "29   2.126441  1.596192  1.302601 -0.625267  0.728203  0.181350  0.539067   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "90  -0.793833  1.762520  0.912498  1.182887 -0.581198 -0.121828 -0.891564   \n",
       "91   0.106599 -0.966910 -0.576079 -1.678606 -0.366704 -0.755679 -2.214221   \n",
       "92   0.088045 -1.556536 -1.535694  0.786301 -0.114562  1.583300 -0.090033   \n",
       "93   0.308170 -2.695540  0.240131  0.908059 -0.443687  0.743719  0.436824   \n",
       "94  -1.023127  1.049151 -0.617696 -1.974539  0.847049  0.580214 -0.783629   \n",
       "95  -1.838633 -0.079153 -0.677238 -1.100844 -1.377004 -0.521673 -0.213441   \n",
       "96  -0.496964 -1.228920 -0.043244  0.254221 -0.082780  1.184575  0.475846   \n",
       "97  -1.806336  1.643205 -1.633132  0.496792 -0.742969 -0.489626  0.027503   \n",
       "98  -0.610201  1.766335  0.915223 -2.323803  1.551051  1.316492 -1.429129   \n",
       "99   1.032885  0.064164 -0.769165  0.078407 -0.909857 -0.635943  0.573373   \n",
       "100 -0.433719  1.087625 -0.359697 -0.121089 -1.684378 -0.296507 -0.035591   \n",
       "101  1.567672  0.185875 -0.811463  1.102997 -0.229851 -0.266047  2.367364   \n",
       "102 -0.978894  0.691046  0.271246  0.844581  0.003153  0.808210 -0.071268   \n",
       "103 -0.266455  0.463741  1.341787 -1.005388 -0.526252 -0.099187  0.944251   \n",
       "104 -1.439551  1.233285  2.879099 -0.476272  0.422538 -0.259557 -0.554024   \n",
       "105 -2.452414 -2.291273 -1.149213 -1.033316  0.002143  0.556236  0.145493   \n",
       "106 -0.727509  1.598275  0.174711 -0.127460 -1.648830 -0.906735  1.318613   \n",
       "107 -0.630132  0.337157 -1.620996 -1.340387  0.350453 -1.799553 -1.164455   \n",
       "108 -0.580189 -0.102385  0.115753 -0.978703 -0.282042 -0.068302 -0.817777   \n",
       "109  0.765039 -0.110520  0.921936 -1.180993  0.909055 -0.339337 -0.969875   \n",
       "110  0.231840  0.836649 -3.242005  2.554540  0.251309  0.945601 -0.239059   \n",
       "111  1.123865 -2.151016 -0.896312 -2.013886 -0.458833  1.053839 -0.003071   \n",
       "112  1.370312  2.106222  0.201604  1.026928  0.369033 -1.596023  0.327179   \n",
       "113 -1.664829 -1.154991  0.876789  0.437520 -1.914616  1.265751  0.650888   \n",
       "114  1.008068  1.303940  0.015752  0.759068  0.688316  0.249673 -0.436027   \n",
       "115  0.269490  0.793482 -0.862932 -1.850052  0.314250  0.010627 -1.400407   \n",
       "116  0.049087 -0.555811  0.443928 -0.320651 -0.350842  0.309964  0.335206   \n",
       "117 -2.150406  1.238607  0.153936 -0.783455  2.296221  0.868531 -0.011128   \n",
       "118  0.439409 -0.114551  0.395257 -0.685737  1.974084  0.619691  1.859962   \n",
       "119  1.032144 -0.682549  0.575425  0.562728 -0.599750  0.751304 -0.318354   \n",
       "\n",
       "           14  \n",
       "0    0.018622  \n",
       "1    0.964627  \n",
       "2   -2.602803  \n",
       "3    0.145577  \n",
       "4   -0.144488  \n",
       "5   -0.312687  \n",
       "6   -0.845574  \n",
       "7   -2.451167  \n",
       "8    0.480013  \n",
       "9    0.449712  \n",
       "10   0.711527  \n",
       "11   0.798463  \n",
       "12   0.143403  \n",
       "13  -1.897041  \n",
       "14   0.255738  \n",
       "15  -1.101963  \n",
       "16   0.675018  \n",
       "17   0.914476  \n",
       "18  -0.876590  \n",
       "19  -1.201874  \n",
       "20   2.043953  \n",
       "21   1.189902  \n",
       "22   0.109991  \n",
       "23  -0.285723  \n",
       "24  -1.133294  \n",
       "25  -0.848116  \n",
       "26  -0.184829  \n",
       "27  -1.360479  \n",
       "28  -0.520021  \n",
       "29  -1.590740  \n",
       "..        ...  \n",
       "90   0.650155  \n",
       "91   1.023971  \n",
       "92   1.568755  \n",
       "93  -0.018611  \n",
       "94  -1.576371  \n",
       "95   0.097560  \n",
       "96  -0.593911  \n",
       "97   0.600103  \n",
       "98  -1.152140  \n",
       "99  -0.222647  \n",
       "100  0.845945  \n",
       "101 -0.610867  \n",
       "102 -0.882274  \n",
       "103  0.476258  \n",
       "104 -0.359096  \n",
       "105 -1.853645  \n",
       "106  0.039692  \n",
       "107  0.596010  \n",
       "108  1.165414  \n",
       "109  0.285163  \n",
       "110 -1.729303  \n",
       "111  0.233951  \n",
       "112 -0.631026  \n",
       "113 -1.572120  \n",
       "114 -1.259726  \n",
       "115 -1.662923  \n",
       "116 -0.798133  \n",
       "117  1.783243  \n",
       "118  1.896146  \n",
       "119  0.804555  \n",
       "\n",
       "[120 rows x 15 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense, LSTM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "%matplotlib inline\n",
    "\n",
    "# Model.add(LSTM(32, input_shape=(5,2), return_sequences=True, activation='tanh'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5000)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx = np.random.randn(5, 5000)\n",
    "trainx.shape[:-1]\n",
    "\n",
    "input_layer = Input(shape = (5, 1))\n",
    "l1 = LSTM(32, activation='tanh')(input_layer)\n",
    "y1 = Dense(1, activation='softmax')(l1)\n",
    "\n",
    "model = Model(input_layer, y1)\n",
    "trainx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_55 (InputLayer)        (None, 5, 1)              0         \n",
      "_________________________________________________________________\n",
      "lstm_55 (LSTM)               (None, 32)                4352      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=\"mean_squared_error\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_55 to have 3 dimensions, but got array with shape (5, 5000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-03e5ecd85064>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTrainmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1635\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1636\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1637\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1638\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1481\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1483\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1484\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1485\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    111\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    114\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_55 to have 3 dimensions, but got array with shape (5, 5000)"
     ]
    }
   ],
   "source": [
    "Trainmod = model.fit(x=trainx, y=trainx, epochs=2, batch_size=2**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_57 (InputLayer)        (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "lstm_57 (LSTM)               (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 601\n",
      "Trainable params: 601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "visible = Input(shape=(100,1))\n",
    "hidden1 = LSTM(10)(visible)\n",
    "hidden2 = Dense(10, activation='relu')(hidden1)\n",
    "output = Dense(1, activation='sigmoid')(hidden2)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_60 (InputLayer)        (None, 23, 1)             0         \n",
      "_________________________________________________________________\n",
      "lstm_60 (LSTM)               (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 601\n",
      "Trainable params: 601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 1s 213ms/step - loss: 0.9757\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.9738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21ec14862b0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# refref here\n",
    "trainx = np.random.randn(5, 23, 1)\n",
    "trainy = np.random.randn(5, 1, )\n",
    "\n",
    "visible = Input(shape=(23,1))\n",
    "hidden1 = LSTM(10)(visible)\n",
    "hidden2 = Dense(10, activation='relu')(hidden1)\n",
    "output = Dense(1, activation='sigmoid')(hidden2)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "print(model.summary())\n",
    "model.compile(optimizer='adam', loss=\"mean_squared_error\")\n",
    "\n",
    "model.fit(x=trainx, y=trainy, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Snowfall Accumulation (in.)')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"GFS\": [np.nan, np.nan, 1, 2, np.nan, \n",
    "                          2, 3, np.nan, np.nan, 4], \n",
    "                    \"RAP\": [-2.45832646,  0.56266567, -0.4453474 , \n",
    "                            -0.85447845, -1.34830127,\n",
    "                            -0.38113925, -0.41400397,  \n",
    "                            np.nan, -0.78764545, -0.02807674]})\n",
    "\n",
    "fh = np.array([\"Fri 4 am\", \"Fri 6 am\",\"Fri 8 am\",\"Fri 10 am\",\n",
    "                \"Fri 6 pm\",\"Fri 10 pm\",\"Sat 4 am\",\"Sat 6 am\",\n",
    "                \"Sat 8 am\",\"100az 10 am\"\n",
    "                ])\n",
    "\n",
    "gfs = df['GFS']\n",
    "rap = df['RAP']\n",
    "\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(10,8))\n",
    "\n",
    "# workaround to set the order of xlabels\n",
    "ax2.plot(fh, [np.nan]*len(fh)) \n",
    "\n",
    "# remove nan's  so that the points are connected\n",
    "ax2.plot(fh[~np.isnan(gfs)], gfs[~np.isnan(gfs)], \"ob-\") \n",
    "ax2.plot(fh[~np.isnan(rap)],rap[~np.isnan(rap)],marker='x')\n",
    "\n",
    "ax2.tick_params(which='major',labelsize='12')\n",
    "ax2.grid(which='major', color='#CCCCCC', linestyle='-')\n",
    "plt.xticks(rotation='90')\n",
    "plt.xlabel('Forecast Run')\n",
    "plt.ylabel('Snowfall Accumulation (in.)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2     True\n",
       "3     True\n",
       "4    False\n",
       "5     True\n",
       "6     True\n",
       "7    False\n",
       "8    False\n",
       "9     True\n",
       "Name: GFS, dtype: bool"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~np.isnan(gfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "photogate",
   "language": "python",
   "name": "photogate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
